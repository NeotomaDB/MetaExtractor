{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting training data to .spacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "import os\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"transformers-ner-0.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join(os.pardir, \"data\", \"entity-extraction\", \"processed\", \"2023-05-31_label-export_39-articles\")\n",
    "# with open(os.path.join(dir, \"data_metrics.json\"), \"r\") as f:\n",
    "#     metrics = json.load(f)\n",
    "\n",
    "# train_files = metrics['train']['gdd_ids']\n",
    "# val_files = metrics['val']['gdd_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "dataset = \"val\"\n",
    "files = os.listdir(os.path.join(dir, dataset))\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    training_object = []\n",
    "    entities = []\n",
    "    with open(f\"{os.path.join(dir, dataset, f)}\", 'r') as fin:\n",
    "        article = fin.readlines()\n",
    "        article_data = json.loads(article[0])\n",
    "        text = article_data['task']['data'][\"text\"]\n",
    "    \n",
    "    doc = nlp.make_doc(text)    \n",
    "\n",
    "    for label in article_data['result']:\n",
    "        start = label['value']['start']\n",
    "        end = label['value']['end']\n",
    "        ent = label['value']['labels'][0]\n",
    "        \n",
    "        span = doc.char_span(start, end, label=ent)\n",
    "        if span is not None:\n",
    "            entities.append(span)\n",
    "            \n",
    "    doc.ents = entities\n",
    "    doc_bin.add(doc)\n",
    "    data.append((doc, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin.to_disk(\"val.spacy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Tok2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL\n",
    "from spacy.training import Example\n",
    "\n",
    "config = {\"model\": DEFAULT_TOK2VEC_MODEL}\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2vec = nlp.add_pipe(\"tok2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [Example.from_dict(d[0], d[1]) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.initialize()\n",
    "losses = tok2vec.update(training_data, sgd=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train/val file (jsonl format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "import os\n",
    "import json\n",
    "\n",
    "files = os.listdir('../../../data/processed/')\n",
    "data_files = []\n",
    "for f in files:\n",
    "    #Read the file, get the text, create a list with element as {'text':text_from_file} and save it as jsonl file   \n",
    "    data = json.load(open(f\"../../../data/processed/{f}\", 'r'))\n",
    "    text = data['data']['text']\n",
    "    data_files.append({'text':text})\n",
    "\n",
    "srsly.write_jsonl(os.path.join(\".\",\"pretrain_data.jsonl\"), data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config {config_pretrain.cfg} --pretraining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config spacy_transformer.cfg spacy_transformer.cfg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy pretrain config_pretrain.cfg ./pretrain_output --paths.raw_text ./pretrain_data.jsonl --gpu-id 0 --paths.train train.spacy --paths.dev val.spacy --paths.vectors en_core_web_lg \n",
    "!python -m spacy train config_pretrain.cfg --paths.train train.spacy --paths.dev val.spacy --output ./output --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train spacy_transformer.cfg --paths.train train.spacy --paths.dev val.spacy --output ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy debug config spacy_transformer_config.cfg --paths.train train.spacy --paths.dev val.spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_cpu()\n",
    "nlp = spacy.load(\"./output/transformer-best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Quaternary Research 80 ( 2013 ) 482-494 Contents lists available at ScienceDirect Quaternary Research journal homepage : www.elsevier.com/locate/yqres Timing of the last deglaciation in the Sierra Nevada of the Mérida Andes, Venezuela Julien Carcaillet aIsandra Angel b, Eduardo Carrillo b, Franck A. Audemard c, Christian Beck d a ISTerre, Université de Grenoble 1, UMR CNRS 5275, F-38041 Grenoble, France b Instituto de Ciencias de la Tierra, Universidad Central de Venezuela, Apdo. 3805, Caracas 1010-A, Venezuela c Fundación Venezolana de Investigaciones Sismológicas, FUNVISIS, El Llanito, Caracas 1030, Venezuela d ISTerre, Université de Savoie, UMR CNRS 5275, F-73376 Le Bourget-du-Lac, France article info Article history : Received 10 January 2013 Available online 29 September 2013 Keywords : Terrestrial cosmogenic nuclides dating Glacial landforms Andes de Mérida Venezuela Pleistocene Holocene abstract In the tropical Mérida Andes ( northwestern Venezuela -)glacial landforms were found at altitudes between 2600 and 5000 m, corresponding to 600 km2 of ice cover during the maximum glacial extension. However, the lack of sufﬁcient absolute age data prevents detailed reconstruction of the timing of the last deglaciation. On the northwestern ﬂank of the Mucuñuque Massif, successive moraines and striated eroded basement surfaces were sampled for cosmogenic 10Be investigation. Their compilation with published data allows the establishment of a detailed chronology of the post-LGM glacier history. The oldest moraines ( 18.1 and 16.8 ka ) correspond to the Oldest Dryas. Successive moraine ridges indicate stops in the overall retreat between the LGM and the Younger Dryas. The cold and short Older Dryas stadial has been identiﬁed. Results indicate that most of the ice withdrew during the Pleistocene. The dataset supports an intensiﬁcation of the vertical retreat rate from ~ 25 m/ka during the late Pleistocene to ~ 310 m/ka during the Pleistocene/Holocene. Afterwards, the glacier was conﬁned and located in the higher altitude zones. The altitude difference of the Younger Dryas moraines in the Mucubají, La Victoria and Los Zerpa valleys indicates a strong effect of valley orientation on the altitude of moraine development.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# Access entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labeled files locally to upload to LabelStudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"../src/training/spacy_ner/output/transformer-v3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(f\"../data/{model_version}_processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    data = json.load(open(f\"../data/{model_version}_processed/{f}\", 'r'))\n",
    "    \n",
    "    doc = nlp(data['data']['text'])\n",
    "    results = []\n",
    "    for ent in doc.ents:\n",
    "        results.append({\n",
    "                \"from_name\": \"label\",\n",
    "                \"to_name\": \"text\",\n",
    "                \"type\": \"labels\",\n",
    "                \"value\": {\n",
    "                    \"start\": ent.start_char,\n",
    "                    \"end\": ent.end_char,\n",
    "                    \"text\": ent.text,\n",
    "                    \"score\": 0.5,\n",
    "                    \"labels\": [\n",
    "                        ent.label_\n",
    "                    ]   \n",
    "                }\n",
    "            })\n",
    "    data['predictions'][0]['result'] = results\n",
    "    with open(f\"../data/{model_version}_labeled/{f}\", 'w') as fout:\n",
    "        json.dump(data, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fossil_lit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
