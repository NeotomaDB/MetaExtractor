{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting training data to .spacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "import os\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"transformers-ner-0.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join(os.pardir, \"data\", \"entity-extraction\", \"processed\", \"2023-05-31_label-export_39-articles\")\n",
    "# with open(os.path.join(dir, \"data_metrics.json\"), \"r\") as f:\n",
    "#     metrics = json.load(f)\n",
    "\n",
    "# train_files = metrics['train']['gdd_ids']\n",
    "# val_files = metrics['val']['gdd_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203.txt\n",
      "200.txt\n",
      "202.txt\n",
      "197.txt\n",
      "72.txt\n",
      "262.txt\n",
      "207.txt\n",
      "172.txt\n",
      "75.txt\n",
      "80.txt\n",
      "264.txt\n",
      "268.txt\n",
      "206.txt\n",
      "209.txt\n",
      "168.txt\n",
      "74.txt\n",
      "265.txt\n",
      "77.txt\n",
      "93.txt\n",
      "78.txt\n",
      "79.txt\n",
      "171.txt\n",
      "150.txt\n",
      "208.txt\n",
      "201.txt\n",
      "266.txt\n",
      "71.txt\n",
      "73.txt\n",
      "269.txt\n",
      "199.txt\n",
      "261.txt\n",
      "94.txt\n",
      "173.txt\n",
      "394.txt\n",
      "152.txt\n",
      "169.txt\n",
      "263.txt\n",
      "210.txt\n",
      "153.txt\n",
      "154.txt\n",
      "170.txt\n",
      "198.txt\n",
      "76.txt\n",
      "271.txt\n",
      "155.txt\n",
      "270.txt\n",
      "205.txt\n",
      "123.txt\n",
      "267.txt\n",
      "149.txt\n",
      "124.txt\n",
      "147.txt\n",
      "146.txt\n",
      "148.txt\n",
      "196.txt\n",
      "151.txt\n",
      "204.txt\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "dataset = \"val\"\n",
    "files = os.listdir(os.path.join(dir, dataset))\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    training_object = []\n",
    "    entities = []\n",
    "    with open(f\"{os.path.join(dir, dataset, f)}\", 'r') as fin:\n",
    "        article = fin.readlines()\n",
    "        article_data = json.loads(article[0])\n",
    "        text = article_data['task']['data'][\"text\"]\n",
    "    \n",
    "    doc = nlp.make_doc(text)    \n",
    "\n",
    "    for label in article_data['result']:\n",
    "        start = label['value']['start']\n",
    "        end = label['value']['end']\n",
    "        ent = label['value']['labels'][0]\n",
    "        \n",
    "        span = doc.char_span(start, end, label=ent)\n",
    "        if span is not None:\n",
    "            entities.append(span)\n",
    "            \n",
    "    doc.ents = entities\n",
    "    doc_bin.add(doc)\n",
    "    data.append((doc, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin.to_disk(\"val.spacy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Tok2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL\n",
    "from spacy.training import Example\n",
    "\n",
    "config = {\"model\": DEFAULT_TOK2VEC_MODEL}\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2vec = nlp.add_pipe(\"tok2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [Example.from_dict(d[0], d[1]) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.initialize()\n",
    "losses = tok2vec.update(training_data, sgd=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tok2vec': 0.0}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train/val file (jsonl format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "import os\n",
    "import json\n",
    "\n",
    "files = os.listdir('../../../data/processed/')\n",
    "data_files = []\n",
    "for f in files:\n",
    "    #Read the file, get the text, create a list with element as {'text':text_from_file} and save it as jsonl file   \n",
    "    data = json.load(open(f\"../../../data/processed/{f}\", 'r'))\n",
    "    text = data['data']['text']\n",
    "    data_files.append({'text':text})\n",
    "\n",
    "srsly.write_jsonl(os.path.join(\".\",\"pretrain_data.jsonl\"), data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\n",
      "train = null\n",
      "dev = null\n",
      "vectors = \"en_core_web_lg\"\n",
      "init_tok2vec = \"./pretrain_output/model150.bin\"\n",
      "raw_text = null\n",
      "\n",
      "[system]\n",
      "seed = 0\n",
      "gpu_allocator = null\n",
      "\n",
      "[nlp]\n",
      "lang = \"en\"\n",
      "pipeline = [\"tok2vec\",\"ner\"]\n",
      "batch_size = 512\n",
      "disabled = []\n",
      "before_creation = null\n",
      "after_creation = null\n",
      "after_pipeline_creation = null\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
      "\n",
      "[components]\n",
      "\n",
      "[components.ner]\n",
      "source = \"./output/spacy-ner-pretrained-best\"\n",
      "component = \"ner\"\n",
      "incorrect_spans_key = null\n",
      "moves = null\n",
      "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
      "update_with_oracle_cut_size = 100\n",
      "\n",
      "[components.ner.model]\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
      "state_type = \"ner\"\n",
      "extra_state_tokens = true\n",
      "hidden_width = 256\n",
      "maxout_pieces = 2\n",
      "use_upper = true\n",
      "nO = null\n",
      "\n",
      "[components.ner.model.tok2vec]\n",
      "@architectures = \"spacy.Tok2VecListener.v1\"\n",
      "width = ${components.tok2vec.model.encode.width}\n",
      "upstream = \"*\"\n",
      "\n",
      "[components.tok2vec]\n",
      "factory = \"tok2vec\"\n",
      "\n",
      "[components.tok2vec.model]\n",
      "@architectures = \"spacy.Tok2Vec.v2\"\n",
      "\n",
      "[components.tok2vec.model.embed]\n",
      "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
      "width = ${components.tok2vec.model.encode.width}\n",
      "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
      "rows = [5000,1000,2500,2500]\n",
      "include_static_vectors = true\n",
      "\n",
      "[components.tok2vec.model.encode]\n",
      "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
      "width = 256\n",
      "depth = 4\n",
      "window_size = 1\n",
      "maxout_pieces = 2\n",
      "\n",
      "[corpora]\n",
      "\n",
      "[corpora.dev]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.dev}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[corpora.pretrain]\n",
      "@readers = \"spacy.JsonlCorpus.v1\"\n",
      "path = ${paths.raw_text}\n",
      "min_length = 5\n",
      "max_length = 4000\n",
      "limit = 0\n",
      "\n",
      "[corpora.train]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.train}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[training]\n",
      "dev_corpus = \"corpora.dev\"\n",
      "train_corpus = \"corpora.train\"\n",
      "seed = ${system.seed}\n",
      "gpu_allocator = ${system.gpu_allocator}\n",
      "dropout = 0.1\n",
      "accumulate_gradient = 1\n",
      "patience = 1600\n",
      "max_epochs = 100\n",
      "max_steps = 200000\n",
      "eval_frequency = 20\n",
      "frozen_components = []\n",
      "annotating_components = []\n",
      "before_to_disk = null\n",
      "before_update = null\n",
      "\n",
      "[training.batcher]\n",
      "@batchers = \"spacy.batch_by_words.v1\"\n",
      "discard_oversize = false\n",
      "tolerance = 0.2\n",
      "get_length = null\n",
      "\n",
      "[training.batcher.size]\n",
      "@schedules = \"compounding.v1\"\n",
      "start = 100\n",
      "stop = 100000\n",
      "compound = 1.001\n",
      "t = 0.0\n",
      "\n",
      "[training.logger]\n",
      "@loggers = \"spacy.ConsoleLogger.v1\"\n",
      "progress_bar = false\n",
      "\n",
      "[training.optimizer]\n",
      "@optimizers = \"Adam.v1\"\n",
      "beta1 = 0.9\n",
      "beta2 = 0.99\n",
      "L2_is_weight_decay = true\n",
      "L2 = 0.01\n",
      "grad_clip = 1.0\n",
      "use_averages = false\n",
      "eps = 0.00000001\n",
      "learn_rate = 0.0001\n",
      "\n",
      "[training.score_weights]\n",
      "ents_f = 0.0\n",
      "ents_p = 0.3\n",
      "ents_r = 0.7\n",
      "ents_per_type = null\n",
      "\n",
      "[pretraining]\n",
      "max_epochs = 1000\n",
      "dropout = 0.2\n",
      "n_save_every = 20\n",
      "n_save_epoch = null\n",
      "component = \"tok2vec\"\n",
      "layer = \"\"\n",
      "corpus = \"corpora.pretrain\"\n",
      "\n",
      "[pretraining.batcher]\n",
      "@batchers = \"spacy.batch_by_words.v1\"\n",
      "size = 3000\n",
      "discard_oversize = false\n",
      "tolerance = 0.2\n",
      "get_length = null\n",
      "\n",
      "[pretraining.objective]\n",
      "@architectures = \"spacy.PretrainVectors.v1\"\n",
      "maxout_pieces = 3\n",
      "hidden_size = 300\n",
      "loss = \"cosine\"\n",
      "\n",
      "[pretraining.optimizer]\n",
      "@optimizers = \"Adam.v1\"\n",
      "beta1 = 0.9\n",
      "beta2 = 0.95\n",
      "L2_is_weight_decay = true\n",
      "L2 = 0.01\n",
      "grad_clip = 1.0\n",
      "use_averages = true\n",
      "eps = 0.00000001\n",
      "learn_rate = 0.001\n",
      "\n",
      "[initialize]\n",
      "init_tok2vec = ${paths.init_tok2vec}\n",
      "vocab_data = null\n",
      "lookups = null\n",
      "before_init = null\n",
      "after_init = null\n",
      "vectors = ${paths.vectors}\n",
      "\n",
      "[initialize.components]\n",
      "\n",
      "[initialize.tokenizer]\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config {config_pretrain.cfg} --pretraining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "spacy_transformer.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train spacy_transformer.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config spacy_transformer.cfg spacy_transformer.cfg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-05-23 09:37:58,130] [INFO] Set up nlp object from config\n",
      "[2023-05-23 09:37:58,140] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2023-05-23 09:37:58,143] [INFO] Created vocabulary\n",
      "[2023-05-23 09:37:59,127] [INFO] Added vectors: en_core_web_md\n",
      "[2023-05-23 09:37:59,127] [INFO] Finished initializing nlp object\n",
      "[2023-05-23 09:38:00,553] [INFO] Loaded pretrained weights from pretrain_output/model152.bin\n",
      "[2023-05-23 09:38:00,553] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    664.57    0.00    0.00    0.00    0.00\n",
      "  0      20          4.23  14152.74    0.00    0.00    0.00    0.00\n",
      "  0      40         31.52   4672.27    0.00    0.00    0.00    0.00\n",
      "  1      60         19.23    865.82    0.00    0.00    0.00    0.00\n",
      "  1      80         36.61   1356.29    0.00    0.00    0.00    0.00\n",
      "  1     100         22.45   1075.80    6.11   88.89    3.16    0.29\n",
      "  2     120       3595.71   1174.17   14.39   80.00    7.91    0.30\n",
      "  2     140       6238.67   1353.99   25.98   55.13   17.00    0.28\n",
      "  3     160       8586.34   1514.63   25.93   59.15   16.60    0.29\n",
      "  3     180        145.64    835.29   30.43   48.70   22.13    0.30\n",
      "  3     200       2410.90    850.54   36.51   55.20   27.27    0.36\n",
      "  4     220         47.41    796.31   38.87   55.07   30.04    0.38\n",
      "  4     240        115.09    723.88   42.86   60.43   33.20    0.41\n",
      "  5     260         58.11    692.40   36.75   54.69   27.67    0.36\n",
      "  5     280         60.76    837.98   43.11   58.90   33.99    0.41\n",
      "  5     300         38.12    570.21   37.95   54.01   29.25    0.37\n",
      "  6     320         48.28    796.56   45.75   50.97   41.50    0.44\n",
      "  6     340         39.66    628.00   44.14   52.75   37.94    0.42\n",
      "  6     360         38.33    541.46   46.96   52.17   42.69    0.46\n",
      "  7     380         36.06    570.24   48.29   56.99   41.90    0.46\n",
      "  7     400         45.78    546.55   50.21   52.84   47.83    0.49\n",
      "  8     420         37.62    457.32   47.26   52.94   42.69    0.46\n",
      "  8     440         45.14    557.31   49.37   52.44   46.64    0.48\n",
      "  8     460         45.73    477.97   50.92   52.52   49.41    0.50\n",
      "  9     480         46.62    489.49   53.78   57.40   50.59    0.53\n",
      "  9     500         49.07    534.08   52.51   53.25   51.78    0.52\n",
      " 10     520         35.62    316.02   57.32   59.00   55.73    0.57\n",
      " 10     540         42.40    373.64   59.10   64.49   54.55    0.58\n",
      " 10     560         52.41    411.87   59.04   60.00   58.10    0.59\n",
      " 11     580         54.65    464.22   58.71   56.36   61.26    0.60\n",
      " 11     600         50.19    373.00   60.71   60.96   60.47    0.61\n",
      " 11     620         46.94    351.67   59.43   61.70   57.31    0.59\n",
      " 12     640         49.30    377.84   61.42   59.70   63.24    0.62\n",
      " 12     660         56.71    334.01   60.90   60.55   61.26    0.61\n",
      " 13     680         57.80    395.60   59.96   59.84   60.08    0.60\n",
      " 13     700         50.40    317.91   63.72   61.94   65.61    0.65\n",
      " 13     720        372.17    376.88   62.69   61.05   64.43    0.63\n",
      " 14     740         45.25    305.15   64.45   63.71   65.22    0.65\n",
      " 14     760         70.10    373.09   62.60   60.52   64.82    0.64\n",
      " 15     780         51.11    257.08   65.37   64.37   66.40    0.66\n",
      " 15     800        381.19    318.45   63.36   61.25   65.61    0.64\n",
      " 15     820         68.04    327.87   64.38   62.13   66.80    0.65\n",
      " 16     840         76.15    326.17   64.45   63.71   65.22    0.65\n",
      " 16     860         83.68    350.29   63.57   62.36   64.82    0.64\n",
      " 16     880         59.66    245.97   64.82   64.82   64.82    0.65\n",
      " 17     900         75.67    380.52   63.24   61.03   65.61    0.64\n",
      " 17     920         79.59    331.88   65.39   63.33   67.59    0.66\n",
      " 18     940         49.91    190.87   66.42   62.90   70.36    0.68\n",
      " 18     960         69.72    276.94   65.09   64.96   65.22    0.65\n",
      " 18     980         88.84    300.44   65.63   64.50   66.80    0.66\n",
      " 19    1000         52.97    192.43   65.23   64.84   65.61    0.65\n",
      " 19    1020         71.53    265.17   64.79   61.57   68.38    0.66\n",
      " 20    1040         75.81    247.26   64.62   62.92   66.40    0.65\n",
      " 20    1060        135.49    344.50   65.13   63.20   67.19    0.66\n",
      " 20    1080         68.32    199.05   65.31   61.25   69.96    0.67\n",
      " 21    1100         81.11    247.41   65.23   64.48   66.01    0.66\n",
      " 21    1120         72.82    176.14   65.45   60.88   70.75    0.68\n",
      " 21    1140         76.90    252.16   68.37   67.97   68.77    0.69\n",
      " 22    1160         69.63    210.15   66.29   63.97   68.77    0.67\n",
      " 22    1180         81.29    229.24   65.02   62.64   67.59    0.66\n",
      " 23    1200         67.34    205.85   64.92   61.79   68.38    0.66\n",
      " 23    1220         77.02    193.57   67.06   66.54   67.59    0.67\n",
      " 23    1240         78.82    189.91   65.07   60.82   69.96    0.67\n",
      " 24    1260         76.25    194.85   66.92   65.17   68.77    0.68\n",
      " 24    1280         64.63    116.47   65.42   62.06   69.17    0.67\n",
      " 25    1300         89.38    216.17   65.55   61.97   69.57    0.67\n",
      " 25    1320         82.29    182.43   67.18   64.94   69.57    0.68\n",
      " 25    1340         79.13    177.18   65.30   61.84   69.17    0.67\n",
      " 26    1360         69.35    145.22   63.97   59.79   68.77    0.66\n",
      " 26    1380         99.67    225.64   67.55   64.96   70.36    0.69\n",
      " 26    1400         72.39    151.39   67.30   64.84   69.96    0.68\n",
      " 27    1420         71.37    175.04   67.42   64.73   70.36    0.69\n",
      " 27    1440         88.95    162.40   66.17   62.15   70.75    0.68\n",
      " 28    1460         44.88     80.35   64.22   59.93   69.17    0.66\n",
      " 28    1480         67.06    124.66   64.33   60.42   68.77    0.66\n",
      " 28    1500         97.27    154.64   65.06   61.40   69.17    0.67\n",
      " 29    1520         46.11     92.39   65.40   63.00   67.98    0.66\n",
      " 29    1540         89.64    156.01   67.04   63.38   71.15    0.69\n",
      " 30    1560         59.15     96.56   66.67   63.67   69.96    0.68\n",
      " 30    1580         55.56     99.25   66.42   62.59   70.75    0.68\n",
      " 30    1600         96.73    124.88   65.30   61.84   69.17    0.67\n",
      " 31    1620         72.74    128.49   64.50   59.27   70.75    0.67\n",
      " 31    1640         76.42     95.54   63.62   61.40   66.01    0.65\n",
      " 31    1660        129.01    145.12   63.85   59.59   68.77    0.66\n",
      " 32    1680         71.13     89.45   64.77   62.18   67.59    0.66\n",
      " 32    1700         93.50    106.31   63.84   59.86   68.38    0.66\n",
      " 33    1720        110.89    117.08   66.29   63.97   68.77    0.67\n",
      " 33    1740         77.43    104.90   64.67   61.35   68.38    0.66\n",
      " 33    1760         81.53     92.54   65.92   62.63   69.57    0.67\n",
      " 34    1780         60.92     92.26   64.34   60.14   69.17    0.66\n",
      " 34    1800         56.78     67.11   66.54   64.10   69.17    0.68\n",
      " 35    1820         67.98     74.37   64.91   62.09   67.98    0.66\n",
      " 35    1840         81.10     94.75   66.30   62.07   71.15    0.68\n",
      " 35    1860         39.97     55.99   65.54   62.28   69.17    0.67\n",
      " 36    1880         84.81     84.40   65.07   60.82   69.96    0.67\n",
      " 36    1900         73.65     75.80   66.05   61.94   70.75    0.68\n",
      " 36    1920        109.40    114.32   65.19   61.03   69.96    0.67\n",
      " 37    1940         83.93     69.74   66.67   63.35   70.36    0.68\n",
      " 37    1960         69.87     75.86   65.20   60.75   70.36    0.67\n",
      " 38    1980         54.21     61.58   63.78   58.61   69.96    0.67\n",
      " 38    2000         79.33     75.70   65.43   61.75   69.57    0.67\n",
      " 38    2020         49.39     49.73   64.33   60.42   68.77    0.66\n",
      " 39    2040         77.48     63.37   63.88   59.06   69.57    0.66\n",
      " 39    2060         54.13     58.33   63.72   59.66   68.38    0.66\n",
      " 40    2080        131.60     98.72   63.39   58.78   68.77    0.66\n",
      " 40    2100         66.17     49.70   64.00   61.76   66.40    0.65\n",
      " 40    2120         87.96     80.37   62.36   58.48   66.80    0.64\n",
      " 41    2140         83.80     61.58   65.42   62.06   69.17    0.67\n",
      " 41    2160         60.63     56.60   65.33   60.68   70.75    0.68\n",
      " 41    2180         74.14     64.62   63.27   58.59   68.77    0.66\n",
      " 42    2200         67.23     49.77   64.79   61.57   68.38    0.66\n",
      " 42    2220         73.82     58.51   64.78   61.87   67.98    0.66\n",
      " 43    2240         66.67     58.57   65.79   62.41   69.57    0.67\n",
      " 43    2260         47.03     45.22   65.28   62.77   67.98    0.66\n",
      " 43    2280         50.03     34.72   66.67   64.00   69.57    0.68\n",
      " 44    2300        118.40     82.68   65.93   61.43   71.15    0.68\n",
      " 44    2320         74.79     51.81   64.78   61.87   67.98    0.66\n",
      " 45    2340         69.94     44.05   66.92   65.17   68.77    0.68\n",
      " 45    2360         71.61     54.23   66.42   62.90   70.36    0.68\n",
      " 45    2380         55.05     43.49   66.05   62.24   70.36    0.68\n",
      " 46    2400         46.03     25.80   64.93   61.48   68.77    0.67\n",
      " 46    2420         58.32     49.08   64.31   60.70   68.38    0.66\n",
      " 46    2440         60.53     41.20   66.42   62.59   70.75    0.68\n",
      " 47    2460         46.41     30.01   64.21   60.21   68.77    0.66\n",
      " 47    2480         83.72     45.38   65.44   61.17   70.36    0.68\n",
      " 48    2500         68.52     46.04   65.09   60.27   70.75    0.68\n",
      " 48    2520         68.41     51.46   66.29   63.97   68.77    0.67\n",
      " 48    2540         58.44     23.81   65.80   62.11   69.96    0.68\n",
      " 49    2560         60.67     43.38   64.65   61.96   67.59    0.66\n",
      " 49    2580         95.66     51.44   65.31   61.54   69.57    0.67\n",
      " 50    2600         39.21     26.26   65.81   61.51   70.75    0.68\n",
      " 50    2620         42.68     24.86   65.27   63.10   67.59    0.66\n",
      " 50    2640         71.93     36.86   65.34   60.40   71.15    0.68\n",
      " 51    2660         27.93     20.42   65.54   62.28   69.17    0.67\n",
      " 51    2680         91.77     58.31   64.07   60.28   68.38    0.66\n",
      " 51    2700         95.39     43.78   64.42   61.21   67.98    0.66\n",
      " 52    2720         75.00     33.95   64.33   60.42   68.77    0.66\n",
      " 52    2740         84.41     49.14   64.52   62.04   67.19    0.66\n",
      " 53    2760         48.12     21.92   64.92   61.79   68.38    0.66\n",
      " 53    2780         53.37     29.45   63.39   58.78   68.77    0.66\n",
      " 53    2800         72.55     41.93   63.93   60.64   67.59    0.66\n",
      " 54    2820         55.77     29.58   63.60   59.45   68.38    0.66\n",
      " 54    2840         66.24     31.80   64.10   59.73   69.17    0.66\n",
      " 55    2860         77.90     50.37   64.09   60.00   68.77    0.66\n",
      " 55    2880         35.51     17.16   63.96   60.07   68.38    0.66\n",
      " 55    2900         73.34     46.89   64.67   61.35   68.38    0.66\n",
      " 56    2920         71.69     37.59   65.07   60.82   69.96    0.67\n",
      " 56    2940         49.50     27.45   63.50   58.98   68.77    0.66\n",
      " 56    2960         77.44     37.93   64.83   60.69   69.57    0.67\n",
      " 57    2980         64.24     38.79   63.47   59.52   67.98    0.65\n",
      " 57    3000         55.02     23.84   65.06   61.40   69.17    0.67\n",
      " 58    3020         51.52     28.77   64.72   60.20   69.96    0.67\n",
      " 58    3040         98.33     49.68   64.96   60.34   70.36    0.67\n",
      " 58    3060         46.97     23.03   62.48   58.68   66.80    0.64\n",
      " 59    3080         34.05     16.49   64.22   59.93   69.17    0.66\n",
      " 59    3100         59.98     32.97   63.28   60.43   66.40    0.65\n",
      " 60    3120         47.18     23.14   63.81   60.42   67.59    0.65\n",
      " 60    3140         92.18     47.85   64.19   60.49   68.38    0.66\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy pretrain config_pretrain.cfg ./pretrain_output --paths.raw_text ./pretrain_data.jsonl --gpu-id 0 --paths.train train.spacy --paths.dev val.spacy --paths.vectors en_core_web_lg \n",
    "!python -m spacy train config_pretrain.cfg --paths.train train.spacy --paths.dev val.spacy --output ./output --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train spacy_transformer.cfg --paths.train train.spacy --paths.dev val.spacy --output ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy debug config spacy_transformer_config.cfg --paths.train train.spacy --paths.dev val.spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_cpu()\n",
    "nlp = spacy.load(\"./output/transformer-best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Quaternary Research 80 ( 2013 ) 482-494 Contents lists available at ScienceDirect Quaternary Research journal homepage : www.elsevier.com/locate/yqres Timing of the last deglaciation in the Sierra Nevada of the Mérida Andes, Venezuela Julien Carcaillet aIsandra Angel b, Eduardo Carrillo b, Franck A. Audemard c, Christian Beck d a ISTerre, Université de Grenoble 1, UMR CNRS 5275, F-38041 Grenoble, France b Instituto de Ciencias de la Tierra, Universidad Central de Venezuela, Apdo. 3805, Caracas 1010-A, Venezuela c Fundación Venezolana de Investigaciones Sismológicas, FUNVISIS, El Llanito, Caracas 1030, Venezuela d ISTerre, Université de Savoie, UMR CNRS 5275, F-73376 Le Bourget-du-Lac, France article info Article history : Received 10 January 2013 Available online 29 September 2013 Keywords : Terrestrial cosmogenic nuclides dating Glacial landforms Andes de Mérida Venezuela Pleistocene Holocene abstract In the tropical Mérida Andes ( northwestern Venezuela -)glacial landforms were found at altitudes between 2600 and 5000 m, corresponding to 600 km2 of ice cover during the maximum glacial extension. However, the lack of sufﬁcient absolute age data prevents detailed reconstruction of the timing of the last deglaciation. On the northwestern ﬂank of the Mucuñuque Massif, successive moraines and striated eroded basement surfaces were sampled for cosmogenic 10Be investigation. Their compilation with published data allows the establishment of a detailed chronology of the post-LGM glacier history. The oldest moraines ( 18.1 and 16.8 ka ) correspond to the Oldest Dryas. Successive moraine ridges indicate stops in the overall retreat between the LGM and the Younger Dryas. The cold and short Older Dryas stadial has been identiﬁed. Results indicate that most of the ice withdrew during the Pleistocene. The dataset supports an intensiﬁcation of the vertical retreat rate from ~ 25 m/ka during the late Pleistocene to ~ 310 m/ka during the Pleistocene/Holocene. Afterwards, the glacier was conﬁned and located in the higher altitude zones. The altitude difference of the Younger Dryas moraines in the Mucubají, La Victoria and Los Zerpa valleys indicates a strong effect of valley orientation on the altitude of moraine development.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# Access entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labeled files locally to upload to LabelStudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"../src/training/spacy_ner/output/transformer-v3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(f\"../data/{model_version}_processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    data = json.load(open(f\"../data/{model_version}_processed/{f}\", 'r'))\n",
    "    \n",
    "    doc = nlp(data['data']['text'])\n",
    "    results = []\n",
    "    for ent in doc.ents:\n",
    "        results.append({\n",
    "                \"from_name\": \"label\",\n",
    "                \"to_name\": \"text\",\n",
    "                \"type\": \"labels\",\n",
    "                \"value\": {\n",
    "                    \"start\": ent.start_char,\n",
    "                    \"end\": ent.end_char,\n",
    "                    \"text\": ent.text,\n",
    "                    \"score\": 0.5,\n",
    "                    \"labels\": [\n",
    "                        ent.label_\n",
    "                    ]   \n",
    "                }\n",
    "            })\n",
    "    data['predictions'][0]['result'] = results\n",
    "    with open(f\"../data/{model_version}_labeled/{f}\", 'w') as fout:\n",
    "        json.dump(data, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fossil_lit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
