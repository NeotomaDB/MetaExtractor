{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenit/yes/envs/fossil_lit/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import json\n",
    "from baseline_entity_extraction import baseline_extract_all, load_taxa_data\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing data from sentences_nlp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(words):\n",
    "    '''\n",
    "    Perform basic preprocessing on individual words\n",
    "    '''\n",
    "    clean_words = []\n",
    "    for i, w in enumerate(words):\n",
    "        if w == '.':\n",
    "            if len(clean_words)>0:\n",
    "                clean_words[-1] += '.'\n",
    "        else:\n",
    "            clean_words.append(w.strip())\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_journal_articles(sentences_path):\n",
    "    \"\"\"\n",
    "    Loads and formats sentences_nlp352 json file and converts to a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Path where the individual sentences are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    journal_articles: pd.DataFrame\n",
    "        pd.DataFrame with cleaned individual sentences for all articles \n",
    "    \"\"\"\n",
    "    journal_articles = pd.read_csv(sentences_path, \n",
    "                                sep='\\t',\n",
    "                                names = ['gddid', \n",
    "                                        'sentid',\n",
    "                                        'wordidx',\n",
    "                                        'words',\n",
    "                                        'part_of_speech',\n",
    "                                        'special_class',\n",
    "                                        'lemmas',\n",
    "                                        'word_type',\n",
    "                                        'word_modified'], \n",
    "                                usecols = ['gddid', 'sentid', 'words'])\n",
    "\n",
    "    journal_articles = journal_articles.replace('\"', '', regex = True)\\\n",
    "                                .replace(',--,', '-', regex = True)\\\n",
    "                                .replace('.,/,', '. / ', regex = True)\\\n",
    "                                .replace('\\{', '', regex = True)\\\n",
    "                                .replace('}', '', regex = True)\\\n",
    "                                .replace(r'\\W{4,}', '', regex=True)\\\n",
    "                                .replace(',,,', 'comma_sym', regex=True)\\\n",
    "                                .replace(',', ' ', regex=True)\\\n",
    "                                .replace('comma_sym', ', ', regex=True)\\\n",
    "                                .replace('-LRB-', '(', regex=True)\\\n",
    "                                .replace('-LSB-', '[', regex=True)\\\n",
    "                                .replace('LRB', '(', regex=True)\\\n",
    "                                .replace('LSB', '[', regex=True)\\\n",
    "                                .replace('-RRB-', ')', regex=True)\\\n",
    "                                .replace('-RSB-', ']', regex=True)\\\n",
    "                                .replace('RRB', ')', regex=True)\\\n",
    "                                .replace('RSB', ']', regex=True)\\\n",
    "                                .replace('-RRB', ')', regex=True)\\\n",
    "                                .replace('-RSB', ']', regex=True)\n",
    "    \n",
    "    journal_articles['words']= journal_articles['words'].str.split(\" \")\n",
    "    journal_articles['words'] = journal_articles['words'].apply(clean_words)\n",
    "    journal_articles['sentence'] = journal_articles['words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    \n",
    "    return journal_articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing data from bibliography file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_bibliography(path):\n",
    "    \"\"\"\n",
    "    Loads and formats bibliography json file and converts to a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Path where the bibliography database is stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bibliography: pd.DataFrame\n",
    "        pd.DataFrame with GDD ID and the Digital Object Identifier.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        bib_dict = json.load(f)\n",
    "    \n",
    "    gdd = []\n",
    "    doi = []\n",
    "    \n",
    "    for article in bib_dict:\n",
    "        gdd.append(article['_gddid'])\n",
    "        if \"identifier\" not in article:\n",
    "            doi.append(\"\")\n",
    "        else:\n",
    "            for iden in article['identifier']:\n",
    "                if iden['type'] == \"doi\":\n",
    "                    doi.append(iden['id'])\n",
    "    \n",
    "    return pd.DataFrame({\"doi\": doi,\n",
    "                         \"gddid\": gdd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_articles = get_journal_articles('../data/sentences_nlp352')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gddid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>1</td>\n",
       "      <td>[LES, DISTORSIONS, DE, L'ENREGISTREMENT, POLLI...</td>\n",
       "      <td>LES DISTORSIONS DE L'ENREGISTREMENT POLLINIQUE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>2</td>\n",
       "      <td>[(, de, -)PONEL, PhREILLE, M., 1997.]</td>\n",
       "      <td>( de -)PONEL PhREILLE M. 1997.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>3</td>\n",
       "      <td>[Les, distorsions, de, l'enregistrement, polli...</td>\n",
       "      <td>Les distorsions de l'enregistrement pollinique...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>4</td>\n",
       "      <td>[[, Distorsions, in, the, pollen, record, of, ...</td>\n",
       "      <td>[ Distorsions in the pollen record of vegetati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>5</td>\n",
       "      <td>[GEOBIOS,, M.S., n, 째, 21, :, 195-202.]</td>\n",
       "      <td>GEOBIOS, M.S. n 째 21 : 195-202.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      gddid  sentid  \\\n",
       "0  54b4325de138239d8684d7e0       1   \n",
       "1  54b4325de138239d8684d7e0       2   \n",
       "2  54b4325de138239d8684d7e0       3   \n",
       "3  54b4325de138239d8684d7e0       4   \n",
       "4  54b4325de138239d8684d7e0       5   \n",
       "\n",
       "                                               words  \\\n",
       "0  [LES, DISTORSIONS, DE, L'ENREGISTREMENT, POLLI...   \n",
       "1              [(, de, -)PONEL, PhREILLE, M., 1997.]   \n",
       "2  [Les, distorsions, de, l'enregistrement, polli...   \n",
       "3  [[, Distorsions, in, the, pollen, record, of, ...   \n",
       "4            [GEOBIOS,, M.S., n, 째, 21, :, 195-202.]   \n",
       "\n",
       "                                            sentence  \n",
       "0  LES DISTORSIONS DE L'ENREGISTREMENT POLLINIQUE...  \n",
       "1                     ( de -)PONEL PhREILLE M. 1997.  \n",
       "2  Les distorsions de l'enregistrement pollinique...  \n",
       "3  [ Distorsions in the pollen record of vegetati...  \n",
       "4                    GEOBIOS, M.S. n 째 21 : 195-202.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bibliography to get the DOIs\n",
    "bib_df = preprocessed_bibliography(\"../data/bibjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>gddid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016/j.palaeo.2004.07.027</td>\n",
       "      <td>54b4324ae138239d8684a3c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/0277-3791(92)90024-3</td>\n",
       "      <td>5504acd5e1382326932d887d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.epsl.2010.01.007</td>\n",
       "      <td>54fccea7e138239936c6de88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.2307/3515451</td>\n",
       "      <td>571062c9cf58f1419caa214d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1111/j.1502-3885.1992.tb00030.x</td>\n",
       "      <td>56c16f4acf58f15c72c8ff9a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  doi                     gddid\n",
       "0        10.1016/j.palaeo.2004.07.027  54b4324ae138239d8684a3c1\n",
       "1        10.1016/0277-3791(92)90024-3  5504acd5e1382326932d887d\n",
       "2          10.1016/j.epsl.2010.01.007  54fccea7e138239936c6de88\n",
       "3                     10.2307/3515451  571062c9cf58f1419caa214d\n",
       "4  10.1111/j.1502-3885.1992.tb00030.x  56c16f4acf58f15c72c8ff9a"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gddid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54b43244e138239d8684933b</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54b43245e138239d8684949c</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54b43245e138239d86849568</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54b43246e138239d868497cd</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54b43246e138239d86849975</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      gddid                                           sentence\n",
       "0  54b43244e138239d8684933b  Palaeogeography, Palaeoclimatology, Palaeoecol...\n",
       "1  54b43245e138239d8684949c  Palaeogeography, Palaeoclimatology, Palaeoecol...\n",
       "2  54b43245e138239d86849568  Palaeogeography, Palaeoclimatology, Palaeoecol...\n",
       "3  54b43246e138239d868497cd  Palaeogeography, Palaeoclimatology, Palaeoecol...\n",
       "4  54b43246e138239d86849975  Palaeogeography, Palaeoclimatology, Palaeoecol..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = journal_articles.groupby(\"gddid\")['sentence'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "full_text.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging 2 dataframe to get DOI-GDD relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gddid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54b43244e138239d8684933b</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(77)90040-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54b43245e138239d8684949c</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(84)90010-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54b43245e138239d86849568</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(83)90024-X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54b43246e138239d868497cd</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(89)90008-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54b43246e138239d86849975</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(92)90137-T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      gddid  \\\n",
       "0  54b43244e138239d8684933b   \n",
       "1  54b43245e138239d8684949c   \n",
       "2  54b43245e138239d86849568   \n",
       "3  54b43246e138239d868497cd   \n",
       "4  54b43246e138239d86849975   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "1  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "2  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "3  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "4  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "\n",
       "                            doi  \n",
       "0  10.1016/0031-0182(77)90040-2  \n",
       "1  10.1016/0031-0182(84)90010-5  \n",
       "2  10.1016/0031-0182(83)90024-X  \n",
       "3  10.1016/0031-0182(89)90008-4  \n",
       "4  10.1016/0031-0182(92)90137-T  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = full_text.merge(bib_df, on ='gddid')\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the RAW text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in full_text.iterrows():\n",
    "    with open(f\"../data/raw/\" + row[1]['gddid'] + '.txt', 'w') as f:\n",
    "        f.write(row[1]['sentence'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting training files by sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# section_pattern = r\"[1-9]\\.[1-9][0-9]\\.? [A-Z][a-zA-Z]{3,}\"\n",
    "patterns = [\"Introduction\", \"Abstract\", \"Material And Method\", \"Site Description\", \"Interpretation\", \"Results\", \"Background\", \"Discussion\", \"Objectives\", \"Conclusion\"]\n",
    "endwords = [\"Acknowledgement\", \"Reference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")    \n",
    "taxa, all_taxa_words = load_taxa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_json(chunk,\n",
    "                chunk_local,\n",
    "                chunk_global,\n",
    "                chunk_subsection,\n",
    "                gdd,\n",
    "                doi):\n",
    "    # Return the JSON for 1 training file\n",
    "    # Get all the labels\n",
    "    training_json = {\n",
    "        \"data\": {\n",
    "            \"text\": chunk,\n",
    "            \"subsection\": chunk_subsection,\n",
    "            \"global_index\": chunk_global,\n",
    "            \"local_index\": chunk_local,\n",
    "            \"gdd_id\": gdd,\n",
    "            \"doi\": doi\n",
    "        },\n",
    "        \"annotations\": [{\n",
    "            \"model_version\": \"pre-labeling\",\n",
    "            \"result\": []\n",
    "        }]\n",
    "    }\n",
    "    labels = baseline_extract_all(chunk, taxa, all_taxa_words, nlp)\n",
    "    entities = []\n",
    "    for label in labels:\n",
    "        # print(label)\n",
    "        entities.append({\n",
    "            \"value\": {\n",
    "                \"start\": label['start'],\n",
    "                \"end\": label['start'],\n",
    "                \"text\": label['text'],\n",
    "                \"labels\": label['labels']\n",
    "            }}\n",
    "        )\n",
    "    training_json['annotations'][0]['result'] = entities\n",
    "\n",
    "    return training_json\n",
    "\n",
    "def get_hash(text):\n",
    "    # Returns a hash key that is used to name the file\n",
    "    return hashlib.shake_128(text.encode('utf-8')).hexdigest(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55c7e851cf58f1a8110ba2e3.txt\n",
      "5697ec53cf58f1143ae00811.txt\n",
      "54b43269e138239d8684f8b5.txt\n",
      "573bbd6acf58f151fc3e56b7.txt\n",
      "557af350e1382390b43c7494.txt\n",
      "54b43248e138239d86849e09.txt\n",
      "5506a7cde1382326932d9244.txt\n",
      "5501e1b5e1382326932d7436.txt\n",
      "54b43265e138239d8684ee5f.txt\n",
      "54b4325ce138239d8684d5ae.txt\n",
      "54b4326de138239d8685034c.txt\n",
      "56818c02cf58f1ba274d4652.txt\n",
      "5724521fcf58f1bc023df2d2.txt\n",
      "54b43283e138239d86854158.txt\n",
      "54e86a77e138237cc9150d9a.txt\n",
      "54b4325de138239d8684d700.txt\n",
      "54b4326be138239d86850036.txt\n",
      "55070990e1382326932d93c8.txt\n",
      "5507ac25e1382326932d9671.txt\n",
      "54b43267e138239d8684f39e.txt\n",
      "562cbce7cf58f10e5cb76e65.txt\n",
      "570fc4aacf58f109a30a7509.txt\n",
      "55044f61e1382326932d85d6.txt\n",
      "54b43269e138239d8684f895.txt\n",
      "54b43269e138239d8684f9c0.txt\n",
      "5504ed13e1382326932d8a51.txt\n",
      "5746564ccf58f1698be7f7c1.txt\n",
      "54b4326ee138239d8685063a.txt\n",
      "54b4324ee138239d8684ae59.txt\n",
      "557c8867e1382390b43c7ba9.txt\n",
      "56c160f5cf58f15c72c8fe73.txt\n",
      "56f90dd1cf58f179466c7159.txt\n",
      "54b43250e138239d8684b23a.txt\n",
      "570fbe4acf58f10725bd2dc7.txt\n",
      "54b4326de138239d8685047c.txt\n",
      "570e712acf58f14fa89185fe.txt\n",
      "57245dd1cf58f1bff6145370.txt\n",
      "54b4324ae138239d8684a3c1.txt\n",
      "57611ef7cf58f130993e9b1c.txt\n",
      "54b4326be138239d86850035.txt\n",
      "5681ac4bcf58f1ba274d47ff.txt\n",
      "56f908e6cf58f177290a15fb.txt\n",
      "550514c5e1382326932d8b77.txt\n",
      "54b43265e138239d8684ef25.txt\n",
      "54b4326ce138239d868501be.txt\n",
      "5518b061e1382394b500dcaf.txt\n",
      "56819e7bcf58f1ba274d4759.txt\n",
      "54f02009e138237cc9152dd4.txt\n",
      "54b4326ae138239d8684fbe7.txt\n",
      "572f8f0fcf58f1373c134aef.txt\n",
      "5719e6f6cf58f1736fb34b34.txt\n",
      "5681a805cf58f1ba274d47cb.txt\n",
      "54ea53dde138237cc91515a2.txt\n",
      "54b4326be138239d8684ffec.txt\n",
      "56b4b6d0cf58f14c238d2f50.txt\n",
      "56f8f2a1cf58f16fca2ddb4c.txt\n",
      "54b43269e138239d8684fa06.txt\n",
      "54b4324fe138239d8684b03c.txt\n",
      "5720a39fcf58f1ad80d81545.txt\n",
      "54b43266e138239d8684f1e1.txt\n",
      "56786b44cf58f1b1878a9eda.txt\n",
      "570f8a0dcf58f1b3c104569a.txt\n",
      "575f018acf58f15e86632077.txt\n",
      "55052e15e1382326932d8c1a.txt\n",
      "54b43267e138239d8684f3a3.txt\n",
      "54b43266e138239d8684f096.txt\n",
      "54b4328fe138239d86856370.txt\n",
      "54f01fe7e138237cc9152dd3.txt\n",
      "577ba721cf58f14ac2c6ff41.txt\n",
      "56f9a26ecf58f1b385af5822.txt\n",
      "550420b9e1382326932d8486.txt\n",
      "54b43269e138239d8684f9d2.txt\n",
      "56f8fbfccf58f17338c435c3.txt\n",
      "54f4b9b8e13823054496a090.txt\n",
      "54b43266e138239d8684f1cf.txt\n",
      "571062c9cf58f1419caa214d.txt\n",
      "56f8f871cf58f171fb30ac77.txt\n",
      "54b43249e138239d8684a061.txt\n",
      "54b4326ae138239d8684fc19.txt\n",
      "54b4324be138239d8684a61b.txt\n",
      "55c91402cf58f1a8110ba7b4.txt\n",
      "56c160d7cf58f15c72c8fe6c.txt\n",
      "56819830cf58f1ba274d4701.txt\n",
      "573a64fdcf58f19ca1af79dd.txt\n",
      "55069f4ce1382326932d920f.txt\n",
      "55052458e1382326932d8be9.txt\n",
      "5506d1b9e1382326932d92e1.txt\n",
      "54b4324ae138239d8684a229.txt\n",
      "54b4324de138239d8684aa1f.txt\n",
      "55b8de13e13823bd29ba8a8e.txt\n",
      "574ec740cf58f1af828bed7e.txt\n",
      "56c16f36cf58f15c72c8ff95.txt\n",
      "54b43248e138239d86849f49.txt\n",
      "54b4326ee138239d8685085e.txt\n",
      "54b4328de138239d86855e40.txt\n",
      "54c8fe15e13823b607fd3faf.txt\n",
      "5504243ee1382326932d849f.txt\n",
      "54b4326ee138239d868506dd.txt\n",
      "55f5aa17cf58f16256a22cdf.txt\n",
      "5723e078cf58f190a565c37e.txt\n",
      "5748472ecf58f13e0ea71e50.txt\n",
      "574fd26ecf58f14fae74a3d9.txt\n",
      "57125af1cf58f13bbac98daf.txt\n",
      "54b4324be138239d8684a606.txt\n",
      "5505ed43e1382326932d8f1e.txt\n",
      "54b4326de138239d868504ad.txt\n",
      "56f8deaacf58f168c9ca87e5.txt\n",
      "54b43266e138239d8684f227.txt\n",
      "54b4326ce138239d86850196.txt\n",
      "55056a6ee1382326932d8d11.txt\n",
      "54b4326ae138239d8684fbf3.txt\n",
      "54b4326be138239d8684fe83.txt\n",
      "54fccea7e138239936c6de88.txt\n",
      "55079d93e1382326932d9636.txt\n",
      "570e06ffcf58f12ab5e2dc2f.txt\n",
      "570e951fcf58f15c40c19eea.txt\n",
      "55061225e1382326932d8fca.txt\n",
      "56f9bd50cf58f1bd3219a74a.txt\n",
      "57391810cf58f148a0747de2.txt\n",
      "54b4326ce138239d868501a3.txt\n",
      "54b43269e138239d8684f980.txt\n",
      "570e6668cf58f14c05e5d5fb.txt\n",
      "56c16731cf58f15c72c8fef5.txt\n",
      "54b43268e138239d8684f714.txt\n",
      "570f623ccf58f1a5a2801171.txt\n",
      "54b4326ae138239d8684fc08.txt\n",
      "55c96523cf58f1a8110ba917.txt\n",
      "54b43265e138239d8684edc5.txt\n",
      "54b43269e138239d8684fa4c.txt\n",
      "54b43268e138239d8684f7e3.txt\n",
      "54b4326de138239d86850584.txt\n",
      "56c16904cf58f15c72c8ff12.txt\n",
      "5504858fe1382326932d875d.txt\n",
      "5507b995e1382326932d96c0.txt\n",
      "54b4327ce138239d86852ee1.txt\n",
      "54b4324ae138239d8684a37b.txt\n",
      "568ebbd6cf58f11663cb48bf.txt\n",
      "576161c4cf58f141254fd03f.txt\n",
      "56c16048cf58f15c72c8fe66.txt\n",
      "54b4326ee138239d868508b9.txt\n",
      "56c170aacf58f15c72c8ffb0.txt\n",
      "56817e34cf58f1ba274d458e.txt\n",
      "54b43267e138239d8684f29a.txt\n",
      "570ea339cf58f1611b8690e8.txt\n",
      "5681cd30cf58f1ba274d49ad.txt\n",
      "5504f015e1382326932d8a67.txt\n",
      "55061c81e1382326932d8fe8.txt\n",
      "550474b7e1382326932d86e3.txt\n",
      "54b43247e138239d86849a72.txt\n",
      "54b43246e138239d86849975.txt\n",
      "54b43263e138239d8684e9b5.txt\n",
      "55054b49e1382326932d8c77.txt\n",
      "56c16759cf58f15c72c8feff.txt\n",
      "55f4a042cf58f16256a225e3.txt\n",
      "550420dbe1382326932d8487.txt\n",
      "56f8db34cf58f167a9344674.txt\n",
      "54b4326ae138239d8684fb23.txt\n",
      "54b43269e138239d8684f936.txt\n",
      "54b4325de138239d8684d7e0.txt\n",
      "5723ec39cf58f195a0f35262.txt\n",
      "574f9db1cf58f13d146b8d3a.txt\n",
      "54b43268e138239d8684f717.txt\n",
      "550506c8e1382326932d8b0e.txt\n",
      "55f2d065cf58f16256a2197e.txt\n",
      "5681abe7cf58f1ba274d47fb.txt\n",
      "54b4326ae138239d8684fc51.txt\n",
      "56b4d1c2cf58f14c238d2ffb.txt\n",
      "57240846cf58f1a1568ecdc8.txt\n",
      "54b43246e138239d868497cd.txt\n",
      "54b43280e138239d86853a58.txt\n",
      "570f005fcf58f183fa76849d.txt\n",
      "56f90b47cf58f1784d2280fc.txt\n",
      "557bfd7de1382390b43c7963.txt\n",
      "5570f3ede138233f91911860.txt\n",
      "5504fc37e1382326932d8ac0.txt\n",
      "55f5bd33cf58f16256a22d5b.txt\n",
      "5579f3e9e138231c7c52c582.txt\n",
      "54ea3e0ae138237cc9151556.txt\n",
      "54b43268e138239d8684f80a.txt\n",
      "56f79575cf58f1b46ff85e3f.txt\n",
      "55062994e1382326932d901c.txt\n",
      "54b43248e138239d86849d0c.txt\n",
      "559d00dbe138230816dad632.txt\n",
      "571b5bf3cf58f1382fed3dd1.txt\n",
      "550439e8e1382326932d853d.txt\n",
      "54f4bdd7e13823054496a0a8.txt\n",
      "54b4328be138239d8685592f.txt\n",
      "5504445fe1382326932d8588.txt\n",
      "55051b9fe1382326932d8baa.txt\n",
      "54b4327ee138239d868532ef.txt\n",
      "5720bdeccf58f1b6ee558947.txt\n",
      "5506c16be1382326932d929b.txt\n",
      "55c8e6f9cf58f1a8110ba6f9.txt\n",
      "5720e9a7cf58f10c293b26f8.txt\n",
      "54b4326be138239d8684fdc6.txt\n",
      "573b4f41cf58f1330944f5f6.txt\n",
      "5745ad8fcf58f135b271221f.txt\n",
      "55c4889bcf58f1a8110b94c6.txt\n",
      "550453fde1382326932d85f7.txt\n",
      "575b6296cf58f130621e74f6.txt\n",
      "55044e4be1382326932d85ce.txt\n",
      "550391f7e1382326932d80ba.txt\n",
      "56c16f4acf58f15c72c8ff9a.txt\n",
      "54b4326be138239d8684fdc4.txt\n",
      "54b43266e138239d8684f27a.txt\n",
      "5505a223e1382326932d8dfa.txt\n",
      "57524fedcf58f17ea28cf479.txt\n",
      "570e807ccf58f1549800e166.txt\n",
      "54b43249e138239d86849fd1.txt\n",
      "54b4324fe138239d8684b00f.txt\n",
      "54b4324ae138239d8684a467.txt\n",
      "55f9fa7acf58f110b882fbcc.txt\n",
      "54b43265e138239d8684eef4.txt\n",
      "551099b6e1382369a6f946de.txt\n",
      "56f8e382cf58f16a5d4d4852.txt\n",
      "56819104cf58f1ba274d469b.txt\n",
      "54b43247e138239d86849ac8.txt\n",
      "572447a1cf58f1b8727784e2.txt\n",
      "54b4324de138239d8684ab71.txt\n",
      "56f8db2ecf58f167a934466d.txt\n",
      "54b43269e138239d8684fa94.txt\n",
      "56819d56cf58f1ba274d474b.txt\n",
      "54b4326fe138239d868508df.txt\n",
      "54b4324ae138239d8684a3c3.txt\n",
      "54b43266e138239d8684f0b3.txt\n",
      "54f91b25e138239936c6c6df.txt\n",
      "571abeb9cf58f1024adebc22.txt\n",
      "54b43268e138239d8684f63b.txt\n",
      "557b039be1382390b43c74fa.txt\n",
      "57525cd5cf58f1837e881c33.txt\n",
      "5505bc3be1382326932d8e4e.txt\n",
      "54b4326ce138239d8685010b.txt\n",
      "55c8d41bcf58f1a8110ba6b6.txt\n",
      "5748f823cf58f170c57fbbd7.txt\n",
      "54b43248e138239d86849f3f.txt\n",
      "54b4324ce138239d8684a8d9.txt\n",
      "570f0ffecf58f189ae1dcd8b.txt\n",
      "54b4326be138239d86850028.txt\n",
      "54bdca94e138233c8bc4f78b.txt\n",
      "54b43268e138239d8684f75d.txt\n",
      "54b4324ce138239d8684a942.txt\n",
      "54b4324de138239d8684aa6f.txt\n",
      "54b43266e138239d8684f025.txt\n",
      "54b43248e138239d86849e56.txt\n",
      "54b4326ae138239d8684fc7f.txt\n",
      "5720bf85cf58f1b773c08a65.txt\n",
      "54b43249e138239d86849fb7.txt\n",
      "570f3310cf58f195d38c63be.txt\n",
      "54b4326be138239d8684ff22.txt\n",
      "54b43248e138239d86849d6e.txt\n",
      "575db969cf58f106ec1b36ba.txt\n",
      "54b4324fe138239d8684b011.txt\n",
      "54b43265e138239d8684eda1.txt\n",
      "54b4326ee138239d86850825.txt\n",
      "576109decf58f12b157b772e.txt\n",
      "557c9e16e1382390b43c7c0f.txt\n",
      "57241c3bcf58f1a9ae494bdd.txt\n",
      "54b43266e138239d8684f11c.txt\n",
      "55f5a3c7cf58f16256a22cae.txt\n",
      "56819513cf58f1ba274d46d4.txt\n",
      "54b43245e138239d86849568.txt\n",
      "55f580eecf58f16256a22bfd.txt\n",
      "54b4326de138239d868505cf.txt\n",
      "570e2f8bcf58f138d6e061cf.txt\n",
      "55041b45e1382326932d845e.txt\n",
      "5506d0d9e1382326932d92dc.txt\n",
      "54b4326ce138239d868502ad.txt\n",
      "557667d7e138234790a0122b.txt\n",
      "54b43245e138239d8684949c.txt\n",
      "5504acd5e1382326932d887d.txt\n",
      "56c15c35cf58f15c72c8fe21.txt\n",
      "570e6e73cf58f14ed1655956.txt\n",
      "557b3f02e1382390b43c7652.txt\n",
      "574dabb1cf58f14816e1acc3.txt\n",
      "570e207ecf58f133a5f97a0b.txt\n",
      "5510bdfde1382369a6f94754.txt\n",
      "54b43250e138239d8684b2ef.txt\n",
      "56819cd8cf58f1ba274d4743.txt\n",
      "54b4326de138239d868504bc.txt\n",
      "54b43244e138239d8684933b.txt\n",
      "562cbf7dcf58f10e5cb76e77.txt\n",
      "5745afe7cf58f136a0d4092e.txt\n",
      "557b935ae1382390b43c77b1.txt\n",
      "54b4326ce138239d86850262.txt\n",
      "574909bacf58f175a7940c7b.txt\n",
      "5518f7dee1382394b500dddb.txt\n",
      "5745b499cf58f13879897213.txt\n",
      "54b4326ce138239d868500df.txt\n",
      "55c9ad18cf58f1a8110baa51.txt\n",
      "54b4326be138239d8684fe5c.txt\n",
      "54f4b22fe13823054496a059.txt\n",
      "570fc230cf58f108dc81e7c0.txt\n",
      "5507530be1382326932d9514.txt\n",
      "54c8f4f6e13823b607fd3f7c.txt\n",
      "55c95698cf58f1a8110ba8c7.txt\n",
      "54e6064ce138237cc91503ae.txt\n"
     ]
    }
   ],
   "source": [
    "prefix = \"../data/raw\"\n",
    "char_len = 4500    # If a section is very long, each chunk will be approximately char_len in length\n",
    "min_len = 1500      # If a section is very small (smaller than min_len), then it will be combined with the next section\n",
    "files = os.listdir(prefix)\n",
    "tot = 0\n",
    "\n",
    "for fin in files:\n",
    "    chunks = []\n",
    "    chunk_subsection = []\n",
    "    chunk_local = []\n",
    "    section_names = []\n",
    "    indices = []\n",
    "    local = 0\n",
    "    subsection = None\n",
    "        \n",
    "    with open(f\"{prefix}/{fin}\", 'r') as f:\n",
    "        print(fin)\n",
    "        article = f.readlines()[0]\n",
    "        sections = []\n",
    "        # # Look for a sections\n",
    "        # matches = re.finditer(section_pattern, article)\n",
    "        # for match in matches:\n",
    "        #     indices.append(match.start())\n",
    "        #     section_names.append(match.group())\n",
    "        \n",
    "        # # Divide the article into sections\n",
    "        # # The length of indices is rarely greater than 0, hence we can remove this part\n",
    "        # if len(indices) > 0:\n",
    "        #     fin_indices = [0, indices[0]]\n",
    "        #     last = 0 # Index of the last index added \n",
    "            \n",
    "        #     # Add indices if they are far away\n",
    "        #     for i in range(1, len(indices)):\n",
    "        #         if indices[i] - indices[last] < char_len:\n",
    "        #             continue\n",
    "        #         else:\n",
    "        #             fin_indices.append(indices[i])\n",
    "        #             last = i\n",
    "\n",
    "        #     for i in range(len(fin_indices)-1):\n",
    "        #         sections.append(article[fin_indices[i] : fin_indices[i+1]])\n",
    "                \n",
    "        #     sections.append(article[fin_indices[-1] : ])\n",
    "            \n",
    "        # else:\n",
    "        #     # The entire article is considered as a single section\n",
    "        #     sections.append(article)\n",
    "        #     section_names.append(\"\")\n",
    "        sections.append(article)\n",
    "        section_names.append(\"\")\n",
    "        \n",
    "        '''\n",
    "        The above section is useful only to split up extremely long articles. \n",
    "        It usually does not find a section in smaller articles and can hence be removed.\n",
    "        '''\n",
    "        \n",
    "        for num, sec in enumerate(sections):\n",
    "            local = 0\n",
    "            # If the section length is small, add the entire section as a chunk\n",
    "            if len(sec) < char_len:\n",
    "                chunks.append(sec)\n",
    "                chunk_subsection.append(sec.split(\" \")[0])\n",
    "                chunk_local.append(local)\n",
    "            else:\n",
    "                # If it is very long, then split the section based on headers\n",
    "                cur_para = \"\"        \n",
    "                sentences = sec.split('. ')\n",
    "                \n",
    "                for si, sent in enumerate(sentences):\n",
    "                    \n",
    "                    if subsection == None:\n",
    "                        subsection = sent.split(\" \")[0]\n",
    "                        \n",
    "                    # If the paragraph is long enough, add it as a chunk\n",
    "                    # If the next sentence is very long, add the current paragraph as a chunk and reset cur_para\n",
    "                    if len(cur_para) > char_len or len(sent) > char_len:\n",
    "                        chunks.append(cur_para.strip())\n",
    "                        chunk_subsection.append(subsection)\n",
    "                        chunk_local.append(local)\n",
    "                        cur_para = \"\"\n",
    "                        local += 1\n",
    "                    \n",
    "                    check=True\n",
    "                    for pat in patterns:\n",
    "                        # If there is a pattern present:\n",
    "                        # then everything before the pattern goes in the current para\n",
    "                        # Text starting from the pattern goes in the next para after\n",
    "                        if pat in sent or pat.upper() in sent:\n",
    "                            try:\n",
    "                                index = sent.index(pat)\n",
    "                            except ValueError:\n",
    "                                index = sent.index(pat.upper())\n",
    "            \n",
    "                            cur_para += sent[:index]\n",
    "                            \n",
    "                            if len(cur_para) > min_len:\n",
    "                                chunks.append(cur_para.strip())\n",
    "                                chunk_subsection.append(subsection)\n",
    "                                chunk_local.append(local)\n",
    "                                subsection = pat\n",
    "                                local = 0\n",
    "                                cur_para = sent[index: ] + '. '\n",
    "                            else:\n",
    "                                cur_para += sent[index: ] + '. '\n",
    "                                \n",
    "                            check = False\n",
    "                            break\n",
    "                            \n",
    "                    end=False\n",
    "                    # Check if there is an endwords\n",
    "                    for pat in endwords:\n",
    "                        if pat in sent or pat.upper() in sent:\n",
    "                            end=True\n",
    "                            break\n",
    "                    if end:\n",
    "                        break\n",
    "                    \n",
    "                    # If no pattern or ending condition is present in the current sentence, then add it to the current para\n",
    "                    if check:\n",
    "                        cur_para += sent + \". \"\n",
    "                if len(cur_para) > 0:\n",
    "                    chunks.append(cur_para.strip())\n",
    "                    chunk_subsection.append(subsection)\n",
    "                    chunk_local.append(local)\n",
    "    \n",
    "    # Writing files\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        gdd = fin.split('.')[0]\n",
    "        doi = data[data['gddid'] == gdd].iloc[0]['doi']\n",
    "        filename = get_hash(chunk)\n",
    "        \n",
    "        with open(f\"../data/train_files_json/{filename}.json\",'w') as fout:\n",
    "            json_chunk = return_json(chunk, \n",
    "                                    chunk_local[i],\n",
    "                                    i,\n",
    "                                    chunk_subsection[i],\n",
    "                                    gdd,\n",
    "                                    doi)\n",
    "            json.dump(json_chunk, fout)\n",
    "        # with open(f\"../data/train_files_txt/{gdd}_{i}_{chunk_subsection[i]}_{chunk_local[i]}.txt\",'w') as fout:\n",
    "        #     fout.write(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fossil_lit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
