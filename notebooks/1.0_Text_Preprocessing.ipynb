{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import json\n",
    "from baseline_entity_extraction import baseline_extract_all, load_taxa_data\n",
    "import spacy\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing data from sentences_nlp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(words):\n",
    "    '''\n",
    "    Perform basic preprocessing on individual words\n",
    "    '''\n",
    "    clean_words = []\n",
    "    for i, w in enumerate(words):\n",
    "        if w == '.':\n",
    "            if len(clean_words)>0:\n",
    "                clean_words[-1] += '.'\n",
    "        else:\n",
    "            clean_words.append(w.strip())\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_journal_articles(sentences_path):\n",
    "    \"\"\"\n",
    "    Loads and formats sentences_nlp352 json file and converts to a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Path where the individual sentences are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    journal_articles: pd.DataFrame\n",
    "        pd.DataFrame with cleaned individual sentences for all articles \n",
    "    \"\"\"\n",
    "    journal_articles = pd.read_csv(sentences_path, \n",
    "                                sep='\\t',\n",
    "                                names = ['gddid', \n",
    "                                        'sentid',\n",
    "                                        'wordidx',\n",
    "                                        'words',\n",
    "                                        'part_of_speech',\n",
    "                                        'special_class',\n",
    "                                        'lemmas',\n",
    "                                        'word_type',\n",
    "                                        'word_modified'], \n",
    "                                usecols = ['gddid', 'sentid', 'words'])\n",
    "\n",
    "    journal_articles = journal_articles.replace('\"', '', regex = True)\\\n",
    "                                .replace(',--,', '-', regex = True)\\\n",
    "                                .replace('.,/,', '. / ', regex = True)\\\n",
    "                                .replace('\\{', '', regex = True)\\\n",
    "                                .replace('}', '', regex = True)\\\n",
    "                                .replace(r'\\W{4,}', '', regex=True)\\\n",
    "                                .replace(',,,', 'comma_sym', regex=True)\\\n",
    "                                .replace(',', ' ', regex=True)\\\n",
    "                                .replace('comma_sym', ', ', regex=True)\\\n",
    "                                .replace('-LRB-', '(', regex=True)\\\n",
    "                                .replace('-LSB-', '[', regex=True)\\\n",
    "                                .replace('LRB', '(', regex=True)\\\n",
    "                                .replace('LSB', '[', regex=True)\\\n",
    "                                .replace('-RRB-', ')', regex=True)\\\n",
    "                                .replace('-RSB-', ']', regex=True)\\\n",
    "                                .replace('RRB', ')', regex=True)\\\n",
    "                                .replace('RSB', ']', regex=True)\\\n",
    "                                .replace('-RRB', ')', regex=True)\\\n",
    "                                .replace('-RSB', ']', regex=True)\n",
    "    \n",
    "    journal_articles['words']= journal_articles['words'].str.split(\" \")\n",
    "    journal_articles['words'] = journal_articles['words'].apply(clean_words)\n",
    "    journal_articles['sentence'] = journal_articles['words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    \n",
    "    return journal_articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing data from bibliography file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_bibliography(path):\n",
    "    \"\"\"\n",
    "    Loads and formats bibliography json file and converts to a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Path where the bibliography database is stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bibliography: pd.DataFrame\n",
    "        pd.DataFrame with GDD ID and the Digital Object Identifier.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        bib_dict = json.load(f)\n",
    "    \n",
    "    gdd = []\n",
    "    doi = []\n",
    "    \n",
    "    for article in bib_dict:\n",
    "        gdd.append(article['_gddid'])\n",
    "        if \"identifier\" not in article:\n",
    "            doi.append(\"\")\n",
    "        else:\n",
    "            for iden in article['identifier']:\n",
    "                if iden['type'] == \"doi\":\n",
    "                    doi.append(iden['id'])\n",
    "    \n",
    "    return pd.DataFrame({\"doi\": doi,\n",
    "                         \"gddid\": gdd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_articles = get_journal_articles('../data/sentences_nlp352')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gddid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>1</td>\n",
       "      <td>[LES, DISTORSIONS, DE, L'ENREGISTREMENT, POLLI...</td>\n",
       "      <td>LES DISTORSIONS DE L'ENREGISTREMENT POLLINIQUE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>2</td>\n",
       "      <td>[(, de, -)PONEL, PhREILLE, M., 1997.]</td>\n",
       "      <td>( de -)PONEL PhREILLE M. 1997.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>3</td>\n",
       "      <td>[Les, distorsions, de, l'enregistrement, polli...</td>\n",
       "      <td>Les distorsions de l'enregistrement pollinique...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>4</td>\n",
       "      <td>[[, Distorsions, in, the, pollen, record, of, ...</td>\n",
       "      <td>[ Distorsions in the pollen record of vegetati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54b4325de138239d8684d7e0</td>\n",
       "      <td>5</td>\n",
       "      <td>[GEOBIOS,, M.S., n, 째, 21, :, 195-202.]</td>\n",
       "      <td>GEOBIOS, M.S. n 째 21 : 195-202.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      gddid  sentid  \\\n",
       "0  54b4325de138239d8684d7e0       1   \n",
       "1  54b4325de138239d8684d7e0       2   \n",
       "2  54b4325de138239d8684d7e0       3   \n",
       "3  54b4325de138239d8684d7e0       4   \n",
       "4  54b4325de138239d8684d7e0       5   \n",
       "\n",
       "                                               words  \\\n",
       "0  [LES, DISTORSIONS, DE, L'ENREGISTREMENT, POLLI...   \n",
       "1              [(, de, -)PONEL, PhREILLE, M., 1997.]   \n",
       "2  [Les, distorsions, de, l'enregistrement, polli...   \n",
       "3  [[, Distorsions, in, the, pollen, record, of, ...   \n",
       "4            [GEOBIOS,, M.S., n, 째, 21, :, 195-202.]   \n",
       "\n",
       "                                            sentence  \n",
       "0  LES DISTORSIONS DE L'ENREGISTREMENT POLLINIQUE...  \n",
       "1                     ( de -)PONEL PhREILLE M. 1997.  \n",
       "2  Les distorsions de l'enregistrement pollinique...  \n",
       "3  [ Distorsions in the pollen record of vegetati...  \n",
       "4                    GEOBIOS, M.S. n 째 21 : 195-202.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bibliography to get the DOIs\n",
    "bib_df = preprocessed_bibliography(\"../data/bibjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>gddid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016/j.palaeo.2004.07.027</td>\n",
       "      <td>54b4324ae138239d8684a3c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/0277-3791(92)90024-3</td>\n",
       "      <td>5504acd5e1382326932d887d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.epsl.2010.01.007</td>\n",
       "      <td>54fccea7e138239936c6de88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.2307/3515451</td>\n",
       "      <td>571062c9cf58f1419caa214d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1111/j.1502-3885.1992.tb00030.x</td>\n",
       "      <td>56c16f4acf58f15c72c8ff9a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  doi                     gddid\n",
       "0        10.1016/j.palaeo.2004.07.027  54b4324ae138239d8684a3c1\n",
       "1        10.1016/0277-3791(92)90024-3  5504acd5e1382326932d887d\n",
       "2          10.1016/j.epsl.2010.01.007  54fccea7e138239936c6de88\n",
       "3                     10.2307/3515451  571062c9cf58f1419caa214d\n",
       "4  10.1111/j.1502-3885.1992.tb00030.x  56c16f4acf58f15c72c8ff9a"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gddid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54b43244e138239d8684933b</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54b43245e138239d8684949c</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54b43245e138239d86849568</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54b43246e138239d868497cd</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54b43246e138239d86849975</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      gddid                                           sentence\n",
       "0  54b43244e138239d8684933b  Palaeogeography, Palaeoclimatology, Palaeoecol...\n",
       "1  54b43245e138239d8684949c  Palaeogeography, Palaeoclimatology, Palaeoecol...\n",
       "2  54b43245e138239d86849568  Palaeogeography, Palaeoclimatology, Palaeoecol...\n",
       "3  54b43246e138239d868497cd  Palaeogeography, Palaeoclimatology, Palaeoecol...\n",
       "4  54b43246e138239d86849975  Palaeogeography, Palaeoclimatology, Palaeoecol..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = journal_articles.groupby(\"gddid\")['sentence'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "full_text.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging 2 dataframe to get DOI-GDD relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gddid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54b43244e138239d8684933b</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(77)90040-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54b43245e138239d8684949c</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(84)90010-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54b43245e138239d86849568</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(83)90024-X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54b43246e138239d868497cd</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(89)90008-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54b43246e138239d86849975</td>\n",
       "      <td>Palaeogeography, Palaeoclimatology, Palaeoecol...</td>\n",
       "      <td>10.1016/0031-0182(92)90137-T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      gddid  \\\n",
       "0  54b43244e138239d8684933b   \n",
       "1  54b43245e138239d8684949c   \n",
       "2  54b43245e138239d86849568   \n",
       "3  54b43246e138239d868497cd   \n",
       "4  54b43246e138239d86849975   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "1  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "2  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "3  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "4  Palaeogeography, Palaeoclimatology, Palaeoecol...   \n",
       "\n",
       "                            doi  \n",
       "0  10.1016/0031-0182(77)90040-2  \n",
       "1  10.1016/0031-0182(84)90010-5  \n",
       "2  10.1016/0031-0182(83)90024-X  \n",
       "3  10.1016/0031-0182(89)90008-4  \n",
       "4  10.1016/0031-0182(92)90137-T  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = full_text.merge(bib_df, on ='gddid')\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the RAW text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in full_text.iterrows():\n",
    "    with open(f\"../data/raw/\" + row[1]['gddid'] + '.txt', 'w') as f:\n",
    "        f.write(row[1]['sentence'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting training files by sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# section_pattern = r\"[1-9]\\.[1-9][0-9]\\.? [A-Z][a-zA-Z]{3,}\"\n",
    "patterns = [\"Introduction\", \"Abstract\", \"Material And Method\", \"Site Description\", \"Interpretation\", \"Results\", \"Background\", \"Discussion\", \"Objectives\", \"Conclusion\"]\n",
    "endwords = [\"Acknowledgement\", \"Reference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(text):\n",
    "    # Returns a hash key that is used to name the file\n",
    "    return hashlib.shake_128(text.encode('utf-8')).hexdigest(8)\n",
    "\n",
    "def return_json(chunk,\n",
    "                chunk_local,\n",
    "                chunk_global,\n",
    "                chunk_subsection,\n",
    "                gdd,\n",
    "                doi):\n",
    "    # Return the JSON for 1 training file\n",
    "    # Get all the labels\n",
    "    training_json = {\n",
    "        \"data\": {\n",
    "            \"text\": chunk,\n",
    "            \"subsection\": chunk_subsection,\n",
    "            \"global_index\": chunk_global,\n",
    "            \"local_index\": chunk_local,\n",
    "            \"gdd_id\": gdd,\n",
    "            \"doi\": doi,\n",
    "            \"timestamp\": str(datetime.today()),\n",
    "            \"chunk_hash\": get_hash(chunk),\n",
    "            \"article_hash\": article_hash[gdd]\n",
    "        },\n",
    "        \"predictions\": [{\n",
    "            \"model_version\": \"baseline\",\n",
    "            \"result\": []\n",
    "        }]\n",
    "    }\n",
    "    labels = baseline_extract_all(chunk)\n",
    "    entities = []\n",
    "    for label in labels:\n",
    "        # print(label)\n",
    "        entities.append({\n",
    "            \"from_name\": \"label\",\n",
    "            \"to_name\": \"text\",\n",
    "            \"type\": \"labels\", \n",
    "            \"value\": {\n",
    "                \"start\": label['start'],\n",
    "                \"end\": label['end'],\n",
    "                \"text\": label['text'],\n",
    "                \"score\": 0.5,\n",
    "                \"labels\": label['labels']\n",
    "            }}\n",
    "        )\n",
    "    training_json['predictions'][0]['result'] = entities\n",
    "\n",
    "    return training_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_hash = {}\n",
    "for i in os.listdir(\"../data/raw/\"):\n",
    "    with open(f\"../data/raw/{i}\", 'r') as fin:\n",
    "        text = fin.readlines()[0]\n",
    "        article_hash[i.split('.')[0]] = get_hash(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55c7e851cf58f1a8110ba2e3.txt\n",
      "5697ec53cf58f1143ae00811.txt\n",
      "54b43269e138239d8684f8b5.txt\n",
      "573bbd6acf58f151fc3e56b7.txt\n",
      "557af350e1382390b43c7494.txt\n",
      "54b43248e138239d86849e09.txt\n",
      "5506a7cde1382326932d9244.txt\n",
      "5501e1b5e1382326932d7436.txt\n",
      "54b43265e138239d8684ee5f.txt\n",
      "54b4325ce138239d8684d5ae.txt\n",
      "54b4326de138239d8685034c.txt\n",
      "56818c02cf58f1ba274d4652.txt\n",
      "5724521fcf58f1bc023df2d2.txt\n",
      "54b43283e138239d86854158.txt\n",
      "54e86a77e138237cc9150d9a.txt\n",
      "54b4325de138239d8684d700.txt\n",
      "54b4326be138239d86850036.txt\n",
      "55070990e1382326932d93c8.txt\n",
      "5507ac25e1382326932d9671.txt\n",
      "54b43267e138239d8684f39e.txt\n"
     ]
    }
   ],
   "source": [
    "prefix = \"../data/raw\"\n",
    "char_len = 4500    # If a section is very long, each chunk will be approximately char_len in length\n",
    "min_len = 1500      # If a section is very small (smaller than min_len), then it will be combined with the next section\n",
    "files = os.listdir(prefix)\n",
    "tot = 0\n",
    "for fin in files[:20]:\n",
    "    chunks = []\n",
    "    chunk_subsection = []\n",
    "    chunk_local = []\n",
    "    section_names = []\n",
    "    indices = []\n",
    "    local = 0\n",
    "    subsection = None\n",
    "        \n",
    "    with open(f\"{prefix}/{fin}\", 'r') as f:\n",
    "        print(fin)\n",
    "        article = f.readlines()[0]\n",
    "        sections = []\n",
    "        # # Look for a sections\n",
    "        # matches = re.finditer(section_pattern, article)\n",
    "        # for match in matches:\n",
    "        #     indices.append(match.start())\n",
    "        #     section_names.append(match.group())\n",
    "        \n",
    "        # # Divide the article into sections\n",
    "        # # The length of indices is rarely greater than 0, hence we can remove this part\n",
    "        # if len(indices) > 0:\n",
    "        #     fin_indices = [0, indices[0]]\n",
    "        #     last = 0 # Index of the last index added \n",
    "            \n",
    "        #     # Add indices if they are far away\n",
    "        #     for i in range(1, len(indices)):\n",
    "        #         if indices[i] - indices[last] < char_len:\n",
    "        #             continue\n",
    "        #         else:\n",
    "        #             fin_indices.append(indices[i])\n",
    "        #             last = i\n",
    "\n",
    "        #     for i in range(len(fin_indices)-1):\n",
    "        #         sections.append(article[fin_indices[i] : fin_indices[i+1]])\n",
    "                \n",
    "        #     sections.append(article[fin_indices[-1] : ])\n",
    "            \n",
    "        # else:\n",
    "        #     # The entire article is considered as a single section\n",
    "        #     sections.append(article)\n",
    "        #     section_names.append(\"\")\n",
    "        sections.append(article)\n",
    "        section_names.append(\"\")\n",
    "        \n",
    "        '''\n",
    "        The above section is useful only to split up extremely long articles. \n",
    "        It usually does not find a section in smaller articles and can hence be removed.\n",
    "        '''\n",
    "        \n",
    "        for num, sec in enumerate(sections):\n",
    "            local = 0\n",
    "            # If the section length is small, add the entire section as a chunk\n",
    "            if len(sec) < char_len:\n",
    "                chunks.append(sec)\n",
    "                chunk_subsection.append(sec.split(\" \")[0])\n",
    "                chunk_local.append(local)\n",
    "            else:\n",
    "                # If it is very long, then split the section based on headers\n",
    "                cur_para = \"\"        \n",
    "                sentences = sec.split('. ')\n",
    "                \n",
    "                for si, sent in enumerate(sentences):\n",
    "                    \n",
    "                    if subsection == None:\n",
    "                        subsection = sent.split(\" \")[0]\n",
    "                        \n",
    "                    # If the paragraph is long enough, add it as a chunk\n",
    "                    # If the next sentence is very long, add the current paragraph as a chunk and reset cur_para\n",
    "                    if len(cur_para) > char_len or len(sent) > char_len:\n",
    "                        chunks.append(cur_para.strip())\n",
    "                        chunk_subsection.append(subsection)\n",
    "                        chunk_local.append(local)\n",
    "                        cur_para = \"\"\n",
    "                        local += 1\n",
    "                    \n",
    "                    check=True\n",
    "                    for pat in patterns:\n",
    "                        # If there is a pattern present:\n",
    "                        # then everything before the pattern goes in the current para\n",
    "                        # Text starting from the pattern goes in the next para after\n",
    "                        if pat in sent or pat.upper() in sent:\n",
    "                            try:\n",
    "                                index = sent.index(pat)\n",
    "                            except ValueError:\n",
    "                                index = sent.index(pat.upper())\n",
    "            \n",
    "                            cur_para += sent[:index]\n",
    "                            \n",
    "                            if len(cur_para) > min_len:\n",
    "                                chunks.append(cur_para.strip())\n",
    "                                chunk_subsection.append(subsection)\n",
    "                                chunk_local.append(local)\n",
    "                                subsection = pat\n",
    "                                local = 0\n",
    "                                cur_para = sent[index: ] + '. '\n",
    "                            else:\n",
    "                                cur_para += sent[index: ] + '. '\n",
    "                                \n",
    "                            check = False\n",
    "                            break\n",
    "                            \n",
    "                    end=False\n",
    "                    # Check if there is an endwords\n",
    "                    for pat in endwords:\n",
    "                        if pat in sent or pat.upper() in sent:\n",
    "                            end=True\n",
    "                            break\n",
    "                    if end:\n",
    "                        break\n",
    "                    \n",
    "                    # If no pattern or ending condition is present in the current sentence, then add it to the current para\n",
    "                    if check:\n",
    "                        cur_para += sent + \". \"\n",
    "                if len(cur_para) > 0:\n",
    "                    chunks.append(cur_para.strip())\n",
    "                    chunk_subsection.append(subsection)\n",
    "                    chunk_local.append(local)\n",
    "    \n",
    "    # Writing files\n",
    "    gdd = fin.split('.')[0]\n",
    "    doi = data[data['gddid'] == gdd].iloc[0]['doi']\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        filename = get_hash(chunk)\n",
    "        with open(f\"../data/processed/{gdd}_{i}.json\",'w') as fout:\n",
    "            json_chunk = return_json(chunk, \n",
    "                                    chunk_local[i],\n",
    "                                    i,\n",
    "                                    chunk_subsection[i],\n",
    "                                    gdd,\n",
    "                                    doi)\n",
    "            json.dump(json_chunk, fout)\n",
    "        # with open(f\"../data/train_files_txt/{gdd}_{i}_{chunk_subsection[i]}_{chunk_local[i]}.txt\",'w') as fout:\n",
    "        #     fout.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../data/processed/\")\n",
    "for f in files:\n",
    "    with open(f\"../data/processed/{f}\", 'r') as fin:\n",
    "        data = json.load(fin)\n",
    "    for i, tag in enumerate(data['predictions'][0]['result']):\n",
    "        data['predictions'][0]['result'][i]['value']['score'] = 0.5\n",
    "    with open(f\"../data/processed/{f}\", 'w') as fout:\n",
    "        json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../data/processed/\")\n",
    "for f in files:\n",
    "    with open(f\"../data/processed/{f}\", 'r') as fin:\n",
    "        data = json.load(fin)\n",
    "    for i, tag in enumerate(data['predictions'][0]['result']):\n",
    "        if data['predictions'][0]['result'][i]['value']['labels'] ==  [\"SITE\"]:\n",
    "            data['predictions'][0]['result'][i]['value']['labels'] = [\"REGION\"]\n",
    "    with open(f\"../data/processed/{f}\", 'w') as fout:\n",
    "        json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fossil_lit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
