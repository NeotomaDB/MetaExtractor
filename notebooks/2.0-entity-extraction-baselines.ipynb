{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fossil Data Extraction Baselines\n",
    "\n",
    "This notebook sets up, runs and evaluates the baseline models for the fossil data extraction task.\n",
    "\n",
    "The data and baseline approaches are as follows:\n",
    "\n",
    "| **Entity Name**            | **Baseline Approach**                                              |\n",
    "|:---:|:---|\n",
    "| Geographic Location - GEOG | Regular Expressions (Goring et. al 2021)                                      |\n",
    "| Site Name - SITE           | spaCy Pre-Trained NER model identifying location entities |\n",
    "| Taxa - TAXA                | In-text search for existing taxa already in Neotoma                |\n",
    "| Age - AGE                  | Regular Expressions (Goring et. al 2021)                                      |\n",
    "| Altitude - ALTI            | Regular Expressions (\"above sea level\", \"a.s.l.\")                  |\n",
    "| Email Address(es) - EMAIL  | Regular Expressions                                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# ensure that the parent directory is on the path for relative imports\n",
    "sys.path.append(os.path.join(os.path.abspath(''), \"..\"))\n",
    "\n",
    "from src.entity_extraction.baseline_entity_extraction import (\n",
    "    extract_geographic_coordinates,\n",
    "    extract_site_names,\n",
    "    extract_taxa,\n",
    "    extract_age,\n",
    "    extract_altitude,\n",
    "    extract_email,\n",
    "    baseline_extract_all\n",
    ")\n",
    "\n",
    "from src.entity_extraction.entity_extraction_evaluation import (\n",
    "    get_token_labels,\n",
    "    plot_token_classification_report,\n",
    "    calculate_entity_classification_metrics,\n",
    "    visualize_mislabelled_entities\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Location - GEOG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of a site are often reported in the literature in a variety of formats. Few varying examples are:\n",
    "- 402646302N \n",
    "- 0795855903W\n",
    "- 40:26:46.302N \n",
    "- 079:58:55.903W\n",
    "- 40°26′46″N\n",
    "- 40d 26′ 46″ N\n",
    "- 40.446195N \n",
    "- -79.982195\n",
    "- 40° 26.7717\n",
    "- N40:26:46.302\n",
    "- N40°26′46″\n",
    "- N40d 26′ 46″\n",
    "- N40.446195\n",
    "- 52°05.75′N\n",
    "- 10°50'E\n",
    "\n",
    "The baseline solution based off of Goring et. al 2021, which uses regular expressions to identify coordinates. We combine mulitple regex patterns presented in the past work to come up with a pattern which is robust to different formats of the coordinates and reduces the number of false positive string matches compared to the previous work:\n",
    "\n",
    "1. Pattern for geographic coordinates - ```[-]?[NESW\\d]+\\s?[NESWd.:°o◦'`\"″]\\s?[NESW]?\\d{1,7}\\s?[NESWd.:°o◦′'`\"″]?\\s?\\d{1,6}[[NESWd.:°o◦′'`\"″]?\\s?\\d{0,3}[NESW]?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_sentences = [\n",
    "    \"40:26:46.302N\",\n",
    "    \"079:58:55.903W\",\n",
    "    \"40°26′46″N\",\n",
    "    \"40d 26′ 46″ N\",\n",
    "    \"N40:26:46.302\",\n",
    "    \"N40°26′46″\",\n",
    "    \"N40d 26′ 46″\",\n",
    "    \"52°05.75′ N\",\n",
    "    \"10°50'E\",\n",
    "    ]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 0, 'end': 13, 'labels': ['GEOG'], 'text': '40:26:46.302N'}],\n",
    "    [{'start': 0, 'end': 14, 'labels': ['GEOG'], 'text': '079:58:55.903W'}],\n",
    "    [{'start': 0, 'end': 10, 'labels': ['GEOG'], 'text': '40°26′46″N'}],\n",
    "    [{'start': 0, 'end': 13, 'labels': ['GEOG'], 'text': '40d 26′ 46″ N'}],\n",
    "    [{'start': 0, 'end': 13, 'labels': ['GEOG'], 'text': 'N40:26:46.302'}],\n",
    "    [{'start': 0, 'end': 10, 'labels': ['GEOG'], 'text': 'N40°26′46″'}],\n",
    "    [{'start': 0, 'end': 12, 'labels': ['GEOG'], 'text': 'N40d 26′ 46″'}],\n",
    "    [{'start': 0, 'end': 11, 'labels': ['GEOG'], 'text': '52°05.75′ N'}],\n",
    "    [{'start': 0, 'end': 7, 'labels': ['GEOG'], 'text': \"10°50'E\"}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_geographic_coordinates = extract_geographic_coordinates(sentence)\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Got: {extracted_geographic_coordinates}\\n\")\n",
    "    assert extracted_geographic_coordinates == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site Name - SITE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pretrained ner models to extract location names from literature. For the baseline approach, we do not try to differentiate between `site names` and `region names` for the site and consider all the entities to be of type `SITE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_sentences = [\n",
    "    \"Its relevance to northwestern Europe in the Late Quaternary Period ( H. NICHOLS -)231 Chronology of Postglacial pollen profiles in the Pacific Northwest ( U.S.A. )\",\n",
    "    \"The scenery around Garibaldi lake is pristine\",\n",
    "    \"This movie was shot in the old towns of Europe\",\n",
    "    \"Philosophical Transactions of and tbe pollen record in the British Isles, In : Birks HH, Birks HJb, Kaland PE, Moe D, eds.\",\n",
    "    \"Holocene fluctuations of cold climate in the Swiss Alps ( H. ZOLLER -)\"\n",
    "]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 30, 'end': 36, 'labels': ['SITE'], 'text': 'Europe'}, {'start': 131, 'end': 152, 'labels': ['SITE'], 'text': 'the Pacific Northwest'}],\n",
    "    [{'start': 19, 'end': 33, 'labels': ['SITE'], 'text': 'Garibaldi lake'}],\n",
    "    [{'start': 40, 'end': 46, 'labels': ['SITE'], 'text': 'Europe'}],\n",
    "    [{'start': 55, 'end': 72, 'labels': ['SITE'], 'text': 'the British Isles'}],\n",
    "    [{'start': 41, 'end': 55, 'labels': ['SITE'], 'text': 'the Swiss Alps'}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_site_names = extract_site_names(sentence)\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Got: {extracted_site_names}\\n\")\n",
    "    assert extracted_site_names == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxa - TAXA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline approach to extract taxas from full text journal articles is to perform string matching. Using an exhaustive list of all the taxas is present on the neotoma database, we search for the exact taxon names in literature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_sentences = [\n",
    "    \"Percentage calculation is based on the terrestrial pollen sum from which Betula was excluded KM/1 KM/2 KM/3 NM/1 NM/2 NM/3 NM/4 NM/5 NM/6 NM/7 NM/8\",\n",
    "    \"The palaeoecology of an Early Neolithic waterlogged site in northwestern England ( F. OLovmLo -)A pollen-analytical study of cores from the Outer Silver Pit\", #False positive\n",
    "    \"Description Salix 0.57 1.76 0.73 13.3 1.67 8.78 1.50 2.88 Solanum dulcamara 0 0 0.73 0 0 1.58 0 0 Lysimachia vulgaris 0 0 4.90 0 0.84 0.53 0 0 Mentha-type 00 0 1.04 0 0 00 Lemna 00 0 7.44 0 1.58 0 0\",\n",
    "    \"The first major impacts upon the vegetation record become eident from about 3610 BP with sharp reductions in arboreal taxa, the appearance of cerealtype pollen in L.A.BI, and marked increases in Calluna, Foaceae and Cyperaceae.\",\n",
    "    \"The overlying Sphagnum peat is devoid of clastic elements for a short period during which sediment inorganic content declines.\",\n",
    "    \"Abstract ) ( A. T. CROSS, G. G. THOMPSON and J. B. ZAITZEFF ) 3 - 1 1 Gymnospermae, general The gymnospermous affinity of Eucommiidites ERDTMAN, 1948\"\n",
    "]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 73, 'end': 79, 'labels': ['TAXA'], 'text': 'Betula'}],\n",
    "    [{'start': 146, 'end': 152, 'labels': ['TAXA'], 'text': 'Silver'}], # False positive\n",
    "    [\n",
    "        {'start': 12, 'end': 17, 'labels': ['TAXA'], 'text': 'Salix'}, \n",
    "        {'start': 58, 'end': 75, 'labels': ['TAXA'], 'text': 'Solanum dulcamara'},\n",
    "        {'start': 98, 'end': 117, 'labels': ['TAXA'], 'text': 'Lysimachia vulgaris'}, \n",
    "        {'start': 143, 'end': 154, 'labels': ['TAXA'], 'text': 'Mentha-type'},\n",
    "        {'start': 143, 'end': 149, 'labels': ['TAXA'], 'text': 'Mentha'}, \n",
    "        {'start': 172, 'end': 177, 'labels': ['TAXA'], 'text': 'Lemna'}],\n",
    "    [\n",
    "        {'start': 195, 'end': 202, 'labels': ['TAXA'], 'text': 'Calluna'}, \n",
    "        {'start': 216, 'end': 226, 'labels': ['TAXA'], 'text': 'Cyperaceae'}],\n",
    "    [{'start': 14, 'end': 22, 'labels': ['TAXA'], 'text': 'Sphagnum'}],\n",
    "    [{'start': 70, 'end': 81, 'labels': ['TAXA'], 'text': 'Gymnosperma'}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_taxas = extract_taxa(sentence)\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Got: {extracted_taxas}\\n\")\n",
    "    assert extracted_taxas == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age - AGE\n",
    "\n",
    "The age of samples is often reported in the literature in a variety of formats.  The most common formats are:\n",
    "- years BP - before present\n",
    "- kyr BP - 1000’s of years BP\n",
    "- ka BP - kilo annum BP\n",
    "- a BP - annum BP\n",
    "- Ma BP - million years BP\n",
    "- YBP - years BP\n",
    "\n",
    "In Neotoma there are three age columns, we have ageold, agetype and ageyoung.\n",
    "\n",
    "- agetype: Age type or units. Includes the following:\n",
    "  - Calendar years AD/BC\n",
    "  - Calendar years BP\n",
    "  - Calibrated radiocarbon years BP\n",
    "  - Radiocarbon years BP\n",
    "  - Varve years BP\n",
    "\n",
    "The baseline solution based off of Goring et. al 2021 uses regular expressions to:\n",
    "1. Identify the age entity in the sentence - `\" BP \"`\n",
    "2. Determine if it is a range of dates - `\"(\\\\d+(?:[.]\\\\d+)*) ((?:- {1,2})|(?:to)) (\\\\d+(?:[.]\\\\d+)*) ([a-zA-Z]+,BP\"`\n",
    "3. Extract the age entity from the sentence - `\"(\\\\d+(?:[.]\\\\d+)*),((?:- {1,2})|(?:to)),(\\\\d+(?:[.]\\\\d+)*),([a-zA-Z]+,BP),\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"1234 BP\",\n",
    "    \"1234 Ma BP\",\n",
    "    \"1234 to 1235 BP\",\n",
    "    \"1234 - 1235 BP\",\n",
    "    \"1234 -- 1235 BP\",\n",
    "    \"1234 BP and 456 to 789 BP\",\n",
    "    \"1234 BP and 456 to 789 Ma BP\",\n",
    "    \"1234 ka BP\",\n",
    "    \"1234 a BP\",\n",
    "    \"1234 Ma BP\",\n",
    "    \"1234 kyr BP\",\n",
    "    \"1234 cal yr BP\",\n",
    "    \"1234 YBP\",\n",
    "    \"1234 14C BP\",\n",
    "]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 0, 'end': 7, 'labels': ['AGE'], 'text': '1234 BP'}],\n",
    "    [{'start': 0, 'end': 10, 'labels': ['AGE'], 'text': '1234 Ma BP'}],\n",
    "    [{'start': 0, 'end': 15, 'labels': ['AGE'], 'text': '1234 to 1235 BP'}],\n",
    "    [{'start': 0, 'end': 14, 'labels': ['AGE'], 'text': '1234 - 1235 BP'}],\n",
    "    [{'start': 0, 'end': 15, 'labels': ['AGE'], 'text': '1234 -- 1235 BP'}],\n",
    "    [\n",
    "        {'start': 0, 'end': 7, 'labels': ['AGE'], 'text': '1234 BP'}, \n",
    "        {'start': 12, 'end': 25, 'labels': ['AGE'], 'text': '456 to 789 BP'}\n",
    "    ],\n",
    "    [\n",
    "        {'start': 0, 'end': 7, 'labels': ['AGE'], 'text': '1234 BP'}, \n",
    "        {'start': 12, 'end': 28, 'labels': ['AGE'], 'text': '456 to 789 Ma BP'}\n",
    "    ],\n",
    "    [{'start': 0, 'end': 10, 'labels': ['AGE'], 'text': '1234 ka BP'}],\n",
    "    [{'start': 0, 'end': 9, 'labels': ['AGE'], 'text': '1234 a BP'}],\n",
    "    [{'start': 0, 'end': 10, 'labels': ['AGE'], 'text': '1234 Ma BP'}],\n",
    "    [{'start': 0, 'end': 11, 'labels': ['AGE'], 'text': '1234 kyr BP'}],\n",
    "    [{'start': 0, 'end': 14, 'labels': ['AGE'], 'text': '1234 cal yr BP'}],\n",
    "    [{'start': 0, 'end': 8, 'labels': ['AGE'], 'text': '1234 YBP'}],\n",
    "    [{'start': 0, 'end': 11, 'labels': ['AGE'], 'text': '1234 14C BP'}],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that all the test sentences are extracted correctly\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_ages = extract_age(sentence)\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Got: {extracted_ages}\\n\")\n",
    "    assert extracted_ages == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altitude - ALTI\n",
    "\n",
    "To identify altitude descriptions the primary indicators are:\n",
    "- \"above sea level\"\n",
    "- \"a.s.l.\"\n",
    "- a single m as the last character after numbers or as a standalone word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"120m above sea level\",\n",
    "    \"120m a.s.l.\",\n",
    "    \"120 m above sea level\",\n",
    "    \"120 m a.s.l.\",\n",
    "    \"120m asl\",\n",
    "    \"120 m asl\",\n",
    "    \"The site was 120m above sea level\",\n",
    "    \"The site was 120m a.s.l.\",\n",
    "    \"The site was 120 m above sea level\",\n",
    "    \"The site was 120 m a.s.l.\",\n",
    "    \"First site was 120m asl and the second was 300 m asl\",\n",
    "]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 0, 'end': 20, 'labels': ['ALTI'], 'text': '120m above sea level'}],\n",
    "    [{'start': 0, 'end': 11, 'labels': ['ALTI'], 'text': '120m a.s.l.'}],\n",
    "    [{'start': 0, 'end': 21, 'labels': ['ALTI'], 'text': '120 m above sea level'}],\n",
    "    [{'start': 0, 'end': 12, 'labels': ['ALTI'], 'text': '120 m a.s.l.'}],\n",
    "    [{'start': 0, 'end': 8, 'labels': ['ALTI'], 'text': '120m asl'}],\n",
    "    [{'start': 0, 'end': 9, 'labels': ['ALTI'], 'text': '120 m asl'}],\n",
    "    [{'start': 13, 'end': 33, 'labels': ['ALTI'], 'text': '120m above sea level'}],\n",
    "    [{'start': 13, 'end': 24, 'labels': ['ALTI'], 'text': '120m a.s.l.'}],\n",
    "    [{'start': 13, 'end': 34, 'labels': ['ALTI'], 'text': '120 m above sea level'}],\n",
    "    [{'start': 13, 'end': 25, 'labels': ['ALTI'], 'text': '120 m a.s.l.'}],\n",
    "    [\n",
    "        {'start': 15, 'end': 23, 'labels': ['ALTI'], 'text': '120m asl'},\n",
    "        {'start': 43, 'end': 52, 'labels': ['ALTI'], 'text': '300 m asl'}\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that all the test sentences are extracted correctly\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_altitude = extract_altitude(sentence)\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Found: {extracted_altitude}\\n\")\n",
    "    assert extracted_altitude == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Addresses - EMAIL\n",
    "\n",
    "There are existing regex patterns developed to identify emails. The one used below was sourced from this StackoverFlow thread: \n",
    "- https://stackoverflow.com/questions/201323/how-can-i-validate-an-email-address-using-a-regular-expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"ty.elgin.andrews@gmail.com\",\n",
    "    \"john.smith@aol.com\",\n",
    "    \"ty.andrews@student.ubc.ca\",\n",
    "    # from GGD 54b4324ae138239d8684a37b segment 0\n",
    "    \"E-mail addresses : carina.hoorn@milne.cc (C. Hoorn -) mauro.cremaschi@libero.it\"\n",
    "]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 0, 'end': 26, 'labels': ['EMAIL'], 'text': 'ty.elgin.andrews@gmail.com'}],\n",
    "    [{'start': 0, 'end': 18, 'labels': ['EMAIL'], 'text': 'john.smith@aol.com'}],\n",
    "    [{'start': 0, 'end': 25, 'labels': ['EMAIL'], 'text': 'ty.andrews@student.ubc.ca'}],\n",
    "    [\n",
    "        {'start': 19, 'end': 40, 'labels': ['EMAIL'], 'text': 'carina.hoorn@milne.cc'},\n",
    "        {'start': 54, 'end': 79, 'labels': ['EMAIL'], 'text': 'mauro.cremaschi@libero.it'}\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_emails = extract_email(sentence)\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Found: {extracted_emails}\\n\")\n",
    "    assert extracted_emails == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Baseline Methods\n",
    "\n",
    "Multiple methods to evaluate the NER task are evaluated for a given report:\n",
    "1. Does the number instances of each category align with what is present in Neotoma?\n",
    "   1. For each article need to pull out the number of instances of each category for the given DOI/GDD id\n",
    "2. Does the number of instances of each category align with what has been labelled?\n",
    "   1. Using the team labelled reports, count the number of instances of each category for each report.\n",
    "3. Does the extracted locations align with the labelled locations?\n",
    "   1. This can be done with the Python package `seqeval` which is used to evaluate the performance of a sequence labeling tasks. ([`seqeval` Github](https://github.com/chakki-works/seqeval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Raw & Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "labelled_file_path = os.path.join(os.getcwd(), os.pardir, \"data\", \"labelled\", \"labelling-labelled\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Neotoma Data\n",
    "\n",
    "To compare the extraction rate, for each article pull the reported taxa, site names, etc. and compare the extracted values to the reported values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Entity Counts to Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_files = os.listdir(labelled_file_path)\n",
    "\n",
    "# count the number of occurences of each label\n",
    "annotated_label_counts = defaultdict(int)\n",
    "baseline_label_counts = defaultdict(int)\n",
    "\n",
    "for file in labelled_files:\n",
    "    \n",
    "    with open(os.path.join(labelled_file_path, file), \"r\") as f:\n",
    "        task = json.load(f)\n",
    "\n",
    "    raw_text = task['task']['data']['text']\n",
    "    \n",
    "    # get the baseline annotations\n",
    "    baseline_result = baseline_extract_all(raw_text)\n",
    "    for baseline in baseline_result:\n",
    "        baseline_label_counts[baseline['labels'][0]] += 1\n",
    "\n",
    "    annotation_result = task['result']\n",
    "    \n",
    "    for annotation in annotation_result:\n",
    "        annotated_label_counts[annotation['value']['labels'][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the percentage of entities extracted by baseline vs annotated\n",
    "annotated_labels = list(annotated_label_counts.keys())\n",
    "annotated_counts = list(annotated_label_counts.values())\n",
    "\n",
    "baseline_counts = [baseline_label_counts[label] for label in annotated_labels]\n",
    "\n",
    "annotated_counts = np.array(annotated_counts)\n",
    "baseline_counts = np.array(baseline_counts)\n",
    "\n",
    "# make into a tidy dataframe with columns Label, Source, Count\n",
    "annotated_df = pd.DataFrame(\n",
    "    {\n",
    "        'Label': annotated_labels + annotated_labels,\n",
    "        'Source': ['Annotated'] * len(annotated_labels) + ['Baseline'] * len(annotated_labels),\n",
    "        'Count': np.concatenate([annotated_counts, baseline_counts])\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    annotated_df,\n",
    "    x=\"Label\",\n",
    "    y=\"Count\",\n",
    "    color=\"Source\",\n",
    "    barmode='group',\n",
    "    # labels={'x': 'Labels', 'value': 'No. of Entities'},\n",
    "    title='Counts of Labels in Annotated and Baseline Results',\n",
    "    width=800,\n",
    ").update_layout(\n",
    "    xaxis={'categoryorder': 'total descending'}, \n",
    "    margin={'l': 0, 'r': 0, 't': 50, 'b': 0}, \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_extracted = baseline_counts / annotated_counts\n",
    "\n",
    "fig = px.bar(\n",
    "    x=annotated_labels, \n",
    "    y=percentage_extracted, \n",
    "    color=annotated_labels,\n",
    "    labels={'x': 'Labels', 'y': 'Percentage Extracted'},\n",
    "    title='Percentage of Entities Extracted by Baseline vs Annotated',\n",
    "    width=800,\n",
    "    # format the text to show percent\n",
    "    text=np.round(percentage_extracted*100, 1),\n",
    ").update_layout(\n",
    "    xaxis={'categoryorder': 'total descending'},\n",
    "    margin={'l': 0, 'r': 0, 't': 50, 'b': 0},\n",
    "    yaxis={'tickformat': ',.0%'},\n",
    "    showlegend=False,\n",
    "    yaxis_range=[0, 1],\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Precision, Recall and F1 Scores with Seqeval\n",
    "\n",
    "The python package `seqeval` is used to evaluate the performance of a sequence labeling tasks. ([`seqeval` Github](https://github.com/chakki-works/seqeval)).\n",
    "\n",
    "It requires the following inputs:\n",
    "- `y_true` - a list of true labels of each token\n",
    "- `y_pred` - a list of predicted labels of each token\n",
    "\n",
    "To take the labelled data from label studio and convert it into the required format, we need to:\n",
    "1. Load the labelled data\n",
    "2. Convert the labelled data into a list of lists of labels with 'O' for tokens that are not part of an entity\n",
    "3. Convert the baseline predictions into a list of lists of labels with 'O' for tokens that are not part of an entity\n",
    "4. Calculate the precision, recall and f1 scores for each entity type\n",
    "\n",
    "### Load Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_file_path = os.path.join(os.getcwd(), os.pardir, \"data\", \"labelled\", \"labelling-labelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_label_files(labelled_files: str):\n",
    "    \"\"\"\n",
    "    Load the json files containing the labelled data and combines the text\n",
    "    into a complete text string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label_files : list\n",
    "        List of json files containing the labelled data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined_text : str\n",
    "        The combined text from all the files.\n",
    "    all_labelled_entities : list\n",
    "        List of all the labelled entities re-indexed to account for the combined text.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    combined_text = \"\"\n",
    "    all_labelled_entities = []\n",
    "    for file in labelled_files:\n",
    "        \n",
    "        with open(os.path.join(labelled_file_path, file), \"r\") as f:\n",
    "            task = json.load(f)\n",
    "\n",
    "        raw_text = task['task']['data']['text']\n",
    "\n",
    "        annotation_result = task['result']\n",
    "        labelled_entities = [annotation['value'] for annotation in annotation_result]\n",
    "\n",
    "        # add the current text length to the start and end indices of labels plus one for the space\n",
    "        for entity in labelled_entities:\n",
    "            entity['start'] += len(combined_text)\n",
    "            entity['end'] += len(combined_text)\n",
    "\n",
    "        all_labelled_entities += labelled_entities\n",
    "\n",
    "        # add the current text to the combined text with space in between\n",
    "        combined_text += raw_text + \" \"\n",
    "\n",
    "    return combined_text, all_labelled_entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Test of Evaluation\n",
    "\n",
    "The following test case is used to test the evaluation of the `seqeval` package. And ensure the steps to wrangle data into the required format are correct.\n",
    "\n",
    "The expected result is precision/accuracy/recall of 1.0 for all entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"The site was 120m above sea level and 1234 BP and found Pediastrum\"\n",
    "test_labelled_entities = [\n",
    "    {'start': 13, 'end': 33, 'labels': ['ALTI'], 'text': '120m above sea level'},\n",
    "    {'start': 38, 'end': 45, 'labels': ['AGE'], 'text': '1234 BP'},\n",
    "    {'start': 56, 'end': 66, 'labels': ['TAXA'], 'text': 'Pediastrum'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the token labels and indexes match\n",
    "test_labelled_tokens = get_token_labels(test_labelled_entities, test_text)\n",
    "\n",
    "split_text = test_text.split()\n",
    "\n",
    "# check that the token labels are correct\n",
    "for i, token in enumerate(split_text):\n",
    "    print(f\"{token}: {test_labelled_tokens[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_entities = baseline_extract_all(test_text)\n",
    "test_baseline_tokens = get_token_labels(test_baseline_entities, test_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity vs. Token Level Evaluation\n",
    "\n",
    "The `seqeval` package can be used to evaluate the performance of the NER task at the entity level or the token level. The two approaches are summarized below:\n",
    "\n",
    "1. **Entity Level Evaluation**\n",
    "   - The entity level evaluation is the standard approach to evaluating the performance of a NER task. \n",
    "   - The evaluation is done at the entity level, meaning that the entire entity must be correctly identified to be considered a true positive.\n",
    "   - The entity level evaluation is the default evaluation method used by the `seqeval` package.\n",
    "\n",
    "2. **Token Level Evaluation**\n",
    "    - The token level evaluation is a more lenient evaluation of the performance of a NER task.\n",
    "    - The evaluation is done at the token level, meaning that each token must just be of the correct tag (e.g. TAXA) to be considered a true positive.\n",
    "    - The token level evaluation is not the default evaluation method used by the `seqeval` package and is calculated by turning all Inner (I) tags into Begin (B) tags for labelled data.\n",
    "\n",
    "Below shows the difference between the two evaluation methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1, recall, precision = calculate_entity_classification_metrics(\n",
    "    labelled_tokens = ['O',  'O', 'O',      'B-TAXA', 'I-TAXA', 'I-TAXA', 'O'],  \n",
    "    predicted_tokens = ['O', 'O', 'B-TAXA', 'I-TAXA', 'I-TAXA', 'I-TAXA', 'O'],\n",
    "    method = \"entities\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1: {f1:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"Precisions: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1, recall, precision = calculate_entity_classification_metrics(\n",
    "    labelled_tokens = ['O',  'O', 'O',      'B-TAXA', 'I-TAXA', 'I-TAXA', 'O'],  \n",
    "    predicted_tokens = ['O', 'O', 'B-TAXA', 'I-TAXA', 'I-TAXA', 'I-TAXA', 'O'],\n",
    "    method = \"tokens\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1: {f1:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"Precisions: {precision:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on Simplified Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics for the test case\n",
    "accuracy, f1, recall, precision = calculate_entity_classification_metrics(\n",
    "    test_labelled_tokens, test_baseline_tokens, method=\"tokens\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1: {f1:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"Precisions: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_classification_report(\n",
    "    test_labelled_tokens, \n",
    "    test_baseline_tokens, \n",
    "    \"Test of Entity Extraction Classification Report\",\n",
    "    method=\"tokens\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of All Labelled Files\n",
    "\n",
    "The following code reads in all labelled files and evaluates the performance of the baseline models across all the text at once. A future improvement will be to be able to analyze individual research papers at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_files = os.listdir(labelled_file_path)\n",
    "\n",
    "# load the labelled data\n",
    "combined_text, labelled_entities = load_json_label_files(labelled_files)\n",
    "\n",
    "# extract the baseline entities\n",
    "baseline_entities = baseline_extract_all(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_tokens = get_token_labels(labelled_entities, combined_text)\n",
    "baseline_tokens = get_token_labels(baseline_entities, combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that both lists are the same length as they should be split identically\n",
    "len(labelled_tokens), len(baseline_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Level Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1, recall, precision = calculate_entity_classification_metrics(\n",
    "    labelled_tokens, baseline_tokens, method=\"entities\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1: {f1:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"Precisions: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_classification_report(\n",
    "    labelled_tokens, \n",
    "    baseline_tokens, \n",
    "    title=\"Classification Report\\n ENTITY Based Evaluation for Baseline Entity Extraction\",\n",
    "    method=\"entities\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Level Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1, recall, precision = calculate_entity_classification_metrics(\n",
    "    labelled_tokens, baseline_tokens, method=\"tokens\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1: {f1:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"Precisions: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_classification_report(\n",
    "    labelled_tokens, \n",
    "    baseline_tokens, \n",
    "    title=\"Classification Report\\n TOKEN Based Evaluation for Baseline Entity Extraction\",\n",
    "    method=\"tokens\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Mislabelled Entities\n",
    "\n",
    "The `spacy` tool `displacy` is used to visualize the entities identified by the baseline model and the labelled entities. This is done to understand the types of entities that are being mislabelled and to identify patterns in the mislabelled entities.\n",
    "\n",
    "We distinguish between the following types of mislabelled entities:  (set the colors of Red/Orange/Green in the markdown text to match the color)\n",
    "1. **<font color='red'>False Negatives (Red)</font>** - entities labelled in the labelled data that are not identified by the baseline model\n",
    "   - these are most important as they signify missed data\n",
    "2. **<font color='orange'>False Positives (Orange)</font>** - entities identified by the baseline model that are not labelled in the labelled data\n",
    "   - these are considered less important as they can be rejected by the data steward\n",
    "3. **<font color='green'>True Positives (Green)</font>** - entities identified by the baseline model that are also labelled in the labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test age 1234 BP and missed SITE\".split(\" \")\n",
    "actual_labels       = [\"O\", \"O\", \"O\",       \"O\", \"O\", \"B-AGE\", \"I-AGE\", \"O\", \"O\", \"B-SITE\"]\n",
    "predicted_labels    = [\"O\", \"O\", \"B-TAXA\",  \"O\", \"O\", \"B-AGE\", \"I-AGE\", \"O\", \"O\", \"O\"]\n",
    "\n",
    "visualize_mislabelled_entities(actual_labels, predicted_labels, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mislabelled_entities(labelled_tokens, baseline_tokens, combined_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci531",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
