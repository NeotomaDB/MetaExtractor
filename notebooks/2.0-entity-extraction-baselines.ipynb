{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fossil Data Extraction Baselines\n",
    "\n",
    "This notebook sets up, runs and evaluates the baseline models for the fossil data extraction task.\n",
    "\n",
    "The data and baseline approaches are as follows:\n",
    "\n",
    "| **Entity Name**            | **Baseline Approach**                                              |\n",
    "|:---:|:---|\n",
    "| Geographic Location - GEOG | Regular Expressions (Goring et. al 2021)                                      |\n",
    "| Site Name - SITE           | spaCy Pre-Trained NER model identifying location entities |\n",
    "| Taxa - TAXA                | In-text search for existing taxa already in Neotoma                |\n",
    "| Age - AGE                  | Regular Expressions (Goring et. al 2021)                                      |\n",
    "| Altitude - ALTI            | Regular Expressions (\"above sea level\", \"a.s.l.\")                  |\n",
    "| Email Address(es) - EMAIL  | Regular Expressions                                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# ensure that the parent directory is on the path for relative imports\n",
    "sys.path.append(os.path.join(os.path.abspath(''), \"..\"))\n",
    "\n",
    "from src.entity_extraction.baseline_entity_extraction import (\n",
    "    extract_geographic_coordinates,\n",
    "    extract_site_names,\n",
    "    extract_taxa,\n",
    "    extract_age,\n",
    "    extract_altitude,\n",
    "    extract_email,\n",
    "    baseline_extract_all\n",
    ")\n",
    "\n",
    "from src.entity_extraction.entity_extraction_evaluation import (\n",
    "    get_token_labels,\n",
    "    plot_token_classification_report,\n",
    "    calculate_entity_classification_metrics\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Location - GEOG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of a site are often reported in the literature in a variety of formats. Few varying examples are:\n",
    "- 402646302N 0795855903W\n",
    "- 40:26:46.302N 079:58:55.903W\n",
    "- 40°26′46″N 079°58′56″W\n",
    "- 40d 26′ 46″ N 079d 58′ 56″ W\n",
    "- 40.446195N 79.982195W\n",
    "- 40.446195, -79.982195\n",
    "- 40.446195,-79.982195\n",
    "- 40° 26.7717, -79° 58.93172\n",
    "- N40:26:46.302 W079:58:55.903\n",
    "- N40°26′46″ W079°58′56″\n",
    "- N40d 26′ 46″ W079d 58′ 56″\n",
    "- N40.446195 W79.982195\n",
    "- 52°05.75′N, 131°13.25′W\n",
    "- 10°50'E, 46°51'N\n",
    "\n",
    "The baseline solution based off of Goring et. al 2021 uses regular expressions to identify coordinates. We've adpated and updated those regex patterns to reduce the number of false positive generated:\n",
    "1. Pattern for geographic coordinates - ```[-+]?[NESW\\d]+\\s?[NESWd\\.:°o◦'`\"″]\\s?[NESW]?\\d{1,7}\\s?[NESWd\\.:°o◦′'`\"″]?\\s?\\d{1,6}[[NESWd\\.:°o◦′'`\"″]?\\s?[NESW]?```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site Name - SITE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pretrained ner models to extract location names from literature, but we are not sure whether these entities will correspond to `region_name` or `site_name`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxa - TAXA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline approach to extract taxas from full text journal articles is to perform string matching. Using an exhaustive list of all the taxas is present on the neotoma database, we search for the exact taxon names in literature. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age - AGE\n",
    "\n",
    "The age of samples is often reported in the literature in a variety of formats.  The most common formats are:\n",
    "- years BP - before present\n",
    "- kyr BP - 1000’s of years BP\n",
    "- ka BP - kilo annum BP\n",
    "- a BP - annum BP\n",
    "- Ma BP - million years BP\n",
    "- YBP - years BP\n",
    "\n",
    "In Neotoma there are three age columns, we have ageold, agetype and ageyoung.\n",
    "\n",
    "- agetype: Age type or units. Includes the following:\n",
    "  - Calendar years AD/BC\n",
    "  - Calendar years BP\n",
    "  - Calibrated radiocarbon years BP\n",
    "  - Radiocarbon years BP\n",
    "  - Varve years BP\n",
    "\n",
    "The baseline solution based off of Goring et. al 2021 uses regular expressions to:\n",
    "1. Identify the age entity in the sentence - `\" BP \"`\n",
    "2. Determine if it is a range of dates - `\"(\\\\d+(?:[.]\\\\d+)*) ((?:- {1,2})|(?:to)) (\\\\d+(?:[.]\\\\d+)*) ([a-zA-Z]+,BP\"`\n",
    "3. Extract the age entity from the sentence - `\"(\\\\d+(?:[.]\\\\d+)*),((?:- {1,2})|(?:to)),(\\\\d+(?:[.]\\\\d+)*),([a-zA-Z]+,BP),\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"1234 BP\",\n",
    "    \"1234 Ma BP\",\n",
    "    \"1234 to 1235 BP\",\n",
    "    \"1234 - 1235 BP\",\n",
    "    \"1234 -- 1235 BP\",\n",
    "    \"1234 BP and 456 to 789 BP\",\n",
    "    \"1234 BP and 456 to 789 Ma BP\",\n",
    "]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 0, 'end': 7, 'label': ['AGE'], 'text': '1234 BP'}],\n",
    "    [{'start': 0, 'end': 10, 'label': ['AGE'], 'text': '1234 Ma BP'}],\n",
    "    [{'start': 0, 'end': 15, 'label': ['AGE'], 'text': '1234 to 1235 BP'}],\n",
    "    [{'start': 0, 'end': 14, 'label': ['AGE'], 'text': '1234 - 1235 BP'}],\n",
    "    [{'start': 0, 'end': 15, 'label': ['AGE'], 'text': '1234 -- 1235 BP'}],\n",
    "    [\n",
    "        {'start': 0, 'end': 7, 'label': ['AGE'], 'text': '1234 BP'}, \n",
    "        {'start': 12, 'end': 25, 'label': ['AGE'], 'text': '456 to 789 BP'}\n",
    "    ],\n",
    "    [\n",
    "        {'start': 0, 'end': 7, 'label': ['AGE'], 'text': '1234 BP'}, \n",
    "        {'start': 12, 'end': 28, 'label': ['AGE'], 'text': '456 to 789 Ma BP'}\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that all the test sentences are extracted correctly\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_ages = extract_age(sentence)\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Got: {extracted_ages}\\n\")\n",
    "    assert extracted_ages == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altitude - ALTI\n",
    "\n",
    "To identify altitude descriptions the primary indicators are:\n",
    "- \"above sea level\"\n",
    "- \"a.s.l.\"\n",
    "- a single m as the last character after numbers or as a standalone word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"120m above sea level\",\n",
    "    \"120m a.s.l.\",\n",
    "    \"120 m above sea level\",\n",
    "    \"120 m a.s.l.\",\n",
    "    \"120m asl\",\n",
    "    \"120 m asl\",\n",
    "    \"The site was 120m above sea level\",\n",
    "    \"The site was 120m a.s.l.\",\n",
    "    \"The site was 120 m above sea level\",\n",
    "    \"The site was 120 m a.s.l.\",\n",
    "    \"First site was 120m asl and the second was 300 m asl\",\n",
    "]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 0, 'end': 20, 'label': ['ALTI'], 'text': '120m above sea level'}],\n",
    "    [{'start': 0, 'end': 11, 'label': ['ALTI'], 'text': '120m a.s.l.'}],\n",
    "    [{'start': 0, 'end': 21, 'label': ['ALTI'], 'text': '120 m above sea level'}],\n",
    "    [{'start': 0, 'end': 12, 'label': ['ALTI'], 'text': '120 m a.s.l.'}],\n",
    "    [{'start': 0, 'end': 8, 'label': ['ALTI'], 'text': '120m asl'}],\n",
    "    [{'start': 0, 'end': 9, 'label': ['ALTI'], 'text': '120 m asl'}],\n",
    "    [{'start': 13, 'end': 33, 'label': ['ALTI'], 'text': '120m above sea level'}],\n",
    "    [{'start': 13, 'end': 24, 'label': ['ALTI'], 'text': '120m a.s.l.'}],\n",
    "    [{'start': 13, 'end': 34, 'label': ['ALTI'], 'text': '120 m above sea level'}],\n",
    "    [{'start': 13, 'end': 25, 'label': ['ALTI'], 'text': '120 m a.s.l.'}],\n",
    "    [\n",
    "        {'start': 15, 'end': 23, 'label': ['ALTI'], 'text': '120m asl'},\n",
    "        {'start': 43, 'end': 52, 'label': ['ALTI'], 'text': '300 m asl'}\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that all the test sentences are extracted correctly\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_altitude = extract_altitude(sentence)\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Found: {extracted_altitude}\\n\")\n",
    "    assert extracted_altitude == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Addresses - EMAIL\n",
    "\n",
    "There are existing regex patterns developed to identify emails. The one used below was sourced from this StackoverFlow thread: \n",
    "- https://stackoverflow.com/questions/201323/how-can-i-validate-an-email-address-using-a-regular-expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"ty.elgin.andrews@gmail.com\",\n",
    "    \"john.smith@aol.com\",\n",
    "    \"ty.andrews@student.ubc.ca\",\n",
    "    # from GGD 54b4324ae138239d8684a37b segment 0\n",
    "    \"E-mail addresses : carina.hoorn@milne.cc (C. Hoorn -) mauro.cremaschi@libero.it\"\n",
    "]\n",
    "\n",
    "expected_results = [\n",
    "    [{'start': 0, 'end': 26, 'label': ['EMAIL'], 'text': 'ty.elgin.andrews@gmail.com'}],\n",
    "    [{'start': 0, 'end': 18, 'label': ['EMAIL'], 'text': 'john.smith@aol.com'}],\n",
    "    [{'start': 0, 'end': 25, 'label': ['EMAIL'], 'text': 'ty.andrews@student.ubc.ca'}],\n",
    "    [\n",
    "        {'start': 19, 'end': 40, 'label': ['EMAIL'], 'text': 'carina.hoorn@milne.cc'},\n",
    "        {'start': 54, 'end': 79, 'label': ['EMAIL'], 'text': 'mauro.cremaschi@libero.it'}\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    extracted_emails = extract_email(sentence)\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\")\n",
    "    print(f\"Found: {extracted_emails}\\n\")\n",
    "    assert extracted_emails == expected_results[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Baseline Methods\n",
    "\n",
    "Multiple methods to evaluate the NER task are evaluated for a given report:\n",
    "1. Does the number instances of each category align with what is present in Neotoma?\n",
    "   1. For each article need to pull out the number of instances of each category for the given DOI/GDD id\n",
    "2. Does the number of instances of each category align with what has been labelled?\n",
    "   1. Using the team labelled reports, count the number of instances of each category for each report.\n",
    "3. Does the extracted locations align with the labelled locations?\n",
    "   1. This can be done with the Python package `seqeval` which is used to evaluate the performance of a sequence labeling tasks. ([`seqeval` Github](https://github.com/chakki-works/seqeval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Raw & Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "labelled_file_path = os.path.join(os.getcwd(), os.pardir, \"data\", \"labelled\", \"labelling-labelled\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Neotoma Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Entity Counts to Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_files = os.listdir(labelled_file_path)\n",
    "\n",
    "# count the number of occurences of each label\n",
    "annotated_label_counts = defaultdict(int)\n",
    "baseline_label_counts = defaultdict(int)\n",
    "\n",
    "for file in labelled_files:\n",
    "    \n",
    "    with open(os.path.join(labelled_file_path, file), \"r\") as f:\n",
    "        task = json.load(f)\n",
    "\n",
    "    raw_text = task['task']['data']['text']\n",
    "    \n",
    "    # get the baseline annotations\n",
    "    baseline_result = baseline_extract_all(raw_text)\n",
    "    for baseline in baseline_result:\n",
    "        baseline_label_counts[baseline['label'][0]] += 1\n",
    "\n",
    "    annotation_result = task['result']\n",
    "    \n",
    "    for annotation in annotation_result:\n",
    "        annotated_label_counts[annotation['value']['labels'][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the percentage of entities extracted by baseline vs annotated\n",
    "annotated_labels = list(annotated_label_counts.keys())\n",
    "annotated_counts = list(annotated_label_counts.values())\n",
    "\n",
    "baseline_counts = [baseline_label_counts[label] for label in annotated_labels]\n",
    "\n",
    "annotated_counts = np.array(annotated_counts)\n",
    "baseline_counts = np.array(baseline_counts)\n",
    "\n",
    "# make into a tidy dataframe with columns Label, Source, Count\n",
    "annotated_df = pd.DataFrame(\n",
    "    {\n",
    "        'Label': annotated_labels + annotated_labels,\n",
    "        'Source': ['Annotated'] * len(annotated_labels) + ['Baseline'] * len(annotated_labels),\n",
    "        'Count': np.concatenate([annotated_counts, baseline_counts])\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    annotated_df,\n",
    "    x=\"Label\",\n",
    "    y=\"Count\",\n",
    "    color=\"Source\",\n",
    "    barmode='group',\n",
    "    # labels={'x': 'Labels', 'value': 'No. of Entities'},\n",
    "    title='Counts of Labels in Annotated and Baseline Results',\n",
    "    width=800,\n",
    ").update_layout(\n",
    "    xaxis={'categoryorder': 'total descending'}, \n",
    "    margin={'l': 0, 'r': 0, 't': 50, 'b': 0}, \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_extracted = baseline_counts / annotated_counts\n",
    "\n",
    "fig = px.bar(\n",
    "    x=annotated_labels, \n",
    "    y=percentage_extracted, \n",
    "    color=annotated_labels,\n",
    "    labels={'x': 'Labels', 'y': 'Percentage Extracted'},\n",
    "    title='Percentage of Entities Extracted by Baseline vs Annotated',\n",
    "    width=800,\n",
    "    text=np.round(percentage_extracted, 3)*100,\n",
    "    hover_name=annotated_labels,\n",
    ").update_layout(\n",
    "    xaxis={'categoryorder': 'total descending'},\n",
    "    margin={'l': 0, 'r': 0, 't': 50, 'b': 0},\n",
    "    yaxis={'tickformat': ',.0%'},\n",
    "    showlegend=False,\n",
    "    yaxis_range=[0, 1],\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Precision, Recall and F1 Scores with Seqeval\n",
    "\n",
    "The python package `seqeval` is used to evaluate the performance of a sequence labeling tasks. ([`seqeval` Github](https://github.com/chakki-works/seqeval)).\n",
    "\n",
    "It requires the following inputs:\n",
    "- `y_true` - a list of true labels of each token\n",
    "- `y_pred` - a list of predicted labels of each token\n",
    "\n",
    "To take the labelled data from label studio and convert it into the required format, we need to:\n",
    "1. Load the labelled data\n",
    "2. Convert the labelled data into a list of lists of labels with 'O' for tokens that are not part of an entity\n",
    "3. Convert the baseline predictions into a list of lists of labels with 'O' for tokens that are not part of an entity\n",
    "4. Calculate the precision, recall and f1 scores for each entity type\n",
    "\n",
    "### Load Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_file_path = os.path.join(os.getcwd(), os.pardir, \"data\", \"labelled\", \"labelling-labelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_label_files(labelled_files: str):\n",
    "    \"\"\"\n",
    "    Load the json files containing the labelled data and combines the text\n",
    "    into a complete text string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label_files : list\n",
    "        List of json files containing the labelled data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined_text : str\n",
    "        The combined text from all the files.\n",
    "    all_labelled_entities : list\n",
    "        List of all the labelled entities re-indexed to account for the combined text.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    combined_text = \"\"\n",
    "    all_labelled_entities = []\n",
    "    for file in labelled_files:\n",
    "        \n",
    "        with open(os.path.join(labelled_file_path, file), \"r\") as f:\n",
    "            task = json.load(f)\n",
    "\n",
    "        raw_text = task['task']['data']['text']\n",
    "\n",
    "        annotation_result = task['result']\n",
    "        labelled_entities = [annotation['value'] for annotation in annotation_result]\n",
    "\n",
    "        # add the current text length to the start and end indices of labels plus one for the space\n",
    "        for entity in labelled_entities:\n",
    "            entity['start'] += len(combined_text)\n",
    "            entity['end'] += len(combined_text)\n",
    "\n",
    "        all_labelled_entities += labelled_entities\n",
    "\n",
    "        # add the current text to the combined text with space in between\n",
    "        combined_text += raw_text + \" \"\n",
    "\n",
    "    return combined_text, all_labelled_entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Test of Evaluation\n",
    "\n",
    "The following test case is used to test the evaluation of the `seqeval` package. And ensure the steps to wrangle data into the required format are correct.\n",
    "\n",
    "The expected result is precision/accuracy/recall of 1.0 for all entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"The site was 120m above sea level and 1234 BP and found Pediastrum\"\n",
    "test_labelled_entities = [\n",
    "    {'start': 13, 'end': 33, 'labels': ['ALTI'], 'text': '120m above sea level'},\n",
    "    {'start': 38, 'end': 45, 'labels': ['AGE'], 'text': '1234 BP'},\n",
    "    {'start': 56, 'end': 66, 'labels': ['TAXA'], 'text': 'Pediastrum'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the token labels and indexes match\n",
    "test_labelled_tokens = get_token_labels(test_labelled_entities, test_text)\n",
    "\n",
    "split_text = test_text.split()\n",
    "\n",
    "# check that the token labels are correct\n",
    "for i, token in enumerate(split_text):\n",
    "    print(f\"{token}: {test_labelled_tokens[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_entities = baseline_extract_all(test_text)\n",
    "test_baseline_tokens = get_token_labels(test_baseline_entities, test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics for the test case\n",
    "accuracy, f1, recall = calculate_entity_classification_metrics(\n",
    "    test_labelled_tokens, test_baseline_tokens\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_classification_report(\n",
    "    test_labelled_tokens, \n",
    "    test_baseline_tokens, \n",
    "    \"Test of Entity Extraction Classification Report\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of All Labelled Files\n",
    "\n",
    "The following code reads in all labelled files and evaluates the performance of the baseline models across all the text at once. A future improvement will be to be able to analyze individual research papers at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_files = os.listdir(labelled_file_path)\n",
    "\n",
    "# load the labelled data\n",
    "combined_text, labelled_entities = load_json_label_files(labelled_files)\n",
    "\n",
    "# extract the baseline entities\n",
    "baseline_entities = baseline_extract_all(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_tokens = get_token_labels(labelled_entities, combined_text)\n",
    "baseline_tokens = get_token_labels(baseline_entities, combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that both lists are the same length as they should be split identically\n",
    "len(labelled_tokens), len(baseline_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1, recall = calculate_entity_classification_metrics(\n",
    "    labelled_tokens, baseline_tokens\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_classification_report(\n",
    "    labelled_tokens, \n",
    "    baseline_tokens, \n",
    "    title=\"Classification Report for Baseline Entity Extraction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci531",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
