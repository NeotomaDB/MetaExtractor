---
title: |
  Final Report: \
  Finding Fossils in the Literature \  
  ![](assets/finding-fossils-logo-symbol_highres.png){width=1.5in} \
subtitle: In partnership with the Neotoma Paleoecology Database.
date: today
author:   
  - Ty Andrews  
  - Jenit Jain
  - Kelly Wu
  - Shaun Hutchinson
execute: 
  echo: false
bibliography: assets/references.bib
format:
  pdf:
    toc: false
    number-sections: true
    colorlinks: true
params: 
  output_file: "reports"
fig-cap-location: top
---

**Executive Summary**

The project Finding Fossils in Literature is sponsored by the Neotoma database (Neotoma) which houses a paleoecology database. The challenges Neotoma faces are 1) researchers have to manually enter sample data into Neotoma, 2) researchers are not aware of Neotoma or that their research fits into it, and 3) there are too many articles published for the Neotoma team to monitor new research. This project has 3 primary deliverables to solve the challenges, first is an article relevance prediction model which predicts whether newly published articles are relevant to Neotoma. Second, is an article data extraction pipeline which identifies key entities such as taxa or geographic location. Last is a data review tool for Neotoma data stewards to review the extracted data before it is submitted to Neotoma. Once the extracted data has been reviewed, the corrections will be used to retrain the first two models of the pipeline.

\newpage

```{r}
# custom table of contents location inspiration from here: https://stackoverflow.com/questions/61104292/how-can-i-separate-the-title-page-from-the-table-of-contents-in-a-rmarkdown-word
```

```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
\newpage

# Introduction

The Neotoma database (Neotoma) [@NeotomaDB] serves as a valuable resource for researchers investigating ecological transformations spanning the last 5 million years. Nevertheless, the collection of data heavily relies on manual submissions from researchers, presenting challenges in data entry and impeding collaborative endeavors. This project aims to automate the extraction of data from relevant journal articles which can be added to Neotoma. This is completed in three parts. First, relevancy to the Neotoma database is predicted. Relevant articles are then parsed using natural language processing techniques. Finally, a data review tool is used to review and correct the extracted data before it is submitted to Neotoma. The outputs of this data review tool are then used to improve the two models in the future.
\newpage

# Data Science Methods

## Article Relevance Prediction

The article relevance prediction was posed as a binary classification problem using article metadata to predict whether the article is relevant to Neotoma.

### Approach

#### Building the Training Data

To train the supervised classification model, a sample of labelled articles was compiled as shown in @tbl-relevance_sample.

| **Label** | **Sample Size** | **Description**                                                                                                                                                                                                                                      |
|:--:|:--:|:---------|
| Positive  |       911       | A randomly sampled list of articles that currently contribute to Neotoma was provided as the positive cases. These articles cover various types of fossil data in the Neotoma database.                                        |
|-----------------|--------------------|---------------------------------------------------------------------|
| Negative  |      3523       | Articles from non-paleoecology-related subjects were queried to form a representative sample of non-relevant articles.  |

: Labelled Sampled Article {#tbl-relevance_sample} {tbl-colwidths="[25,25,75]"}

The labelled articles were split into train/validation/test sets with splits of 70%/15%/15%, which correspond to 3103/665/666 articles respectively. 

#### Preprocessing & Feature Selection

The article's metadata were obtained from the CrossRef API [@crossref], which provided over 50 types of metadata. To reduce dimensionality, feature selection was conducted with results in @tbl-feature_selection.

| **Feature**                                | **Decision**                                                                                                                                                                                                                                                                                                                                                                  |
|:----|:--------|
| Title & subtitle                           | Title and subtitle (if any) are concatenated as a descriptive text feature. Text representation techniques were then applied to this feature.                                                                                                                                                                                                                                 |
|------------------------------------------|-----------------------------------------------------------------------|
| Abstract                                   | Less than 50% of articles have abstracts available due to copyright issues. To address this, the available abstracts are combined with the article title and subtitle to create a descriptive text feature. A binary feature indicates whether the article has an abstract or not, which can affect model predictions. |
|------------------------------------------|-----------------------------------------------------------------------|
| Author & Journal                           | In order to mitigate potential bias towards well-established authors and journals, the feature list was modified to exclude time-sensitive indicators to account for new Authors and Journals published in the future.                                                            |
|------------------------------------------|-----------------------------------------------------------------------|
| Subject                                    | The subject of the journal was used as a proxy of the name of the journal.                                                                                                                                                                                                                                                                      |
|------------------------------------------|-----------------------------------------------------------------------|
| Number of citation                         | Number of citations could indicate if the article contains important data and is referred to in other articles.                                                                                                                                                                                                                                                   |
|------------------------------------------|-----------------------------------------------------------------------|
| Feature related to the publication process | These features do not provide any contextual information about the article, thus were removed from the feature list.                                                                                                                                                                                                                                                     |


: Feature Selection Decisions {#tbl-feature_selection}

The features selected include a descriptive text feature (concatenation of title, subtitle and abstract), subject of the journal, number of citations and a binary indicator for having an abstract available from CrossRef or not.

#### Feature Engineering: Text representation

The descriptive text feature provides crucial contextual information for the model to predict if the topic was related to paleoecology or not. Feature engineering was conducted to represent this descriptive text feature in numeric form. The following text representations were explored and tested to identify the most effective approach.

**Bag of words representation (Baseline):** word-frequency based method. This achieved moderate results but is limited by not containing contextual information.

**Sentence embedding (Final):** Sentence embedding is the state-of-the-art approach for text representation. Three sentence embedding models were experimented and allenai/specter2 model resulted in the best overall performance as documented in @tbl-sent_embedding.

| Model                       | Result                                                                                                                                                                                   |
| --------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| bert-tiny-finetuned-squadv2 | This model was of a small scale so that the embedding process would be quick.                                                                                                              |
| --------------------------- | -------------------------------------------------------------------------------------- |
| Biobert                     | This model was pre-trained based on large-scale biomedical corpora, thus it could be better at understanding biology-related jargons that frequently appear in the field of paleoecology. |
| --------------------------- | --------------------------------------------------------------------------------------- |
| specter2                    | This model was pre-trained using over 6 million scientific articles, thus it could be better at understanding the language style and pattern in scientific articles.                      |

: Sentence Embedding Models {#tbl-sent_embedding} {tbl-colwidths="[25,75]"}

#### Model Selection

The following selection of Probability Classifiers, Analogy-based and Tree-based & Gradient-boosted models were experimented with.

**Probability classifiers:** Traditional probability classifiers provide fast training, high interpretability, and prediction time. Logistic regression and naive Bayes were experimented during model selection.

**Analogy-based models:** The article relevance prediction problem can be framed as looking for articles that are similar to the positive examples. K-nearest neighbour and Support Vector Machine with RBF kernel were experimented.

**Tree-based & Gradient-boosted models:** Tree-based classifiers use different feature subsets during the prediction, which can be suitable for high dimensional data problem. Decision Tree, Random Forest Model, LightGBM, XGBoost and CatBoost were experimented. 

The chosen model went through further hyperparameter tuning to optimise the performance.

### Model Evaluation

The main goal of article relevance prediction is to identify as many relevant articles as possible so that valuable research data can be discovered. The cost of false negatives will be high because missed articles are unlikely to be discovered by the Neotoma community where as false positives are lower cost with easy rejection in the data review tool. For this reason recall was optimized for.

Model interpretability was also important because the data likely has biases due to positive examples coming from existing articles in Neotoma. A more interpretable model would help identify model biases and make corrections in the long run.

## Article Data Extraction

```{python}
# | warning: false
import pandas as pd
import os
import json
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import Markdown

FFOSSILS_PALETTE = sns.color_palette(
    ["#677e52", "#8fbed5", "#1f3a5f", "#005a3c", "#4c2f27", "#b8864b"]
)


def load_model_evaluation_results(results_path: str, results_type: str = "val"):
    results_df = pd.DataFrame()

    for model in os.listdir(results_path):
        if os.path.isdir(os.path.join(results_path, model)):
            model_path = os.path.join(results_path, model)
            for file in os.listdir(model_path):
                if (file.endswith("classification_results.json")) & (
                    results_type in file
                ):
                    results_dict = json.load(open(os.path.join(model_path, file), "r"))

                    parsed_dict = {}

                    for key, value in results_dict.items():
                        # original entity and token based results
                        if (key == "token") | (key == "entity"):
                            for k, v in value.items():
                                # print(f"key: {key} k: {k} v: {v}")
                                if k == "classification_report":
                                    for kk, vv in v.items():
                                        # print(f"\tkk: {kk} vv: {vv}")
                                        for kkk, vvv in vv.items():
                                            # print(f"\t\tkkk: {kkk} vvv: {vvv}")
                                            parsed_dict[
                                                f"{key}_{kk}_{kkk}".replace(" ", "-")
                                            ] = [vvv]
                                else:
                                    parsed_dict[f"{key}_{k}"] = [v]
                        # overall model results with new metrics
                        elif key == "overall_results":
                            for k, v in value.items():
                                # print(f"key: {key} k: {k} v: {v}")
                                for kk, vv in v.items():
                                    # print(f"\tkk: {kk}, vv: {vv}")
                                    if kk in [
                                        "precision",
                                        "recall",
                                        "actual",
                                        "possible",
                                    ]:
                                        parsed_dict[
                                            f"overall_{k.replace('_', '-')}_{kk}"
                                        ] = [vv]
                                    else:
                                        parsed_dict[
                                            f"overall_{k.replace('_', '-')}_percent-{kk}"
                                        ] = [vv / v["possible"]]
                        # overall results by each tag group
                        elif key == "overall_results_by_tag":
                            for label, v in value.items():
                                # print(f"key: {key} label: {label} v: {v}")
                                for eval_type, vv in v.items():
                                    # print(f"\teval_type: {eval_type} vv: {vv}")
                                    for result, vvv in vv.items():
                                        # print(f"\t\tresult: {result}, vvv: {vvv}")
                                        if result in [
                                            "precision",
                                            "recall",
                                            "actual",
                                            "possible",
                                        ]:
                                            parsed_dict[
                                                f"{label}_{eval_type.replace('_', '-')}_{result}"
                                            ] = [vvv]
                                        else:
                                            parsed_dict[
                                                f"{label}_{eval_type.replace('_', '-')}_percent-{result}"
                                            ] = [vvv / vv["possible"]]
                        # add in the counts of each entity type
                        elif key == "entity_counts":
                            for k, v in value.items():
                                # print(f"key: {key} k: {k} v: {v}")
                                parsed_dict[f"{k}_count"] = [v]
                        elif key == "num_tokens":
                            parsed_dict[f"{key}"] = [value]

                    model_results = pd.DataFrame(parsed_dict)

                    model_results["model"] = model.replace("-", " ").title()
                    results_df = pd.concat([results_df, model_results])

    return results_df


```

The article data extraction was posed as a Named Entity Recognition (NER) problem. NER is done by using models to predict the correct label for each word in the text.

### Data Description

An original sample of 300+ full text articles related to Neotoma was provided. Some articles were non-english and were removed from the dataset. The data of interest to be extracted and thus labelled in the articles using NER were as follows:

-   **SITE**: the specific site name given to the items studied
-   **REGION**: general geographic regions around the site to give context to sites
-   **GEOG**: geographic coordinates of sites or samples
-   **TAXA**: the taxa uncovered and researched in the article
-   **AGE**: any ages relevant to the samples
-   **ALTI**: the altitude at which samples were studied
-   **EMAIL**: any researchers emails for further follow up

### Data Preprocessing and Labelling

For the development of the NER models, a sample of 39 articles relevant to Neotoma were preprocessed and labelled by the research team using LabelStudio[@LabelStudio]. Other text preprocessing techniques such as lemmatization were not used due to text quality issues from OCR making it difficult to identify the correct lemma. The SITE entities were difficcult to identify and label due to obscure location names or an acronym of a core sample being used. Similarly, correctly identifying where one TAXA entity ends and the next begins proved challenging.

The labelled articles were split into train/validation/test sets by full article with splits of 70%/15%/15% respectively. The data splitting was done based upon whole articles to prevent data leakage into the model learning certain articles patterns before evaluation. This resulted in the entity distributions between train/validation/test as shown in @fig-entity-dists.

```{python}
def plot_distribution_of_entities(
    results_df: pd.DataFrame, model: str, title: str = None
):
    entity_columns = [c for c in results_df if "count" in c and c != "O_count"]

    flat_df = (
        results_df.query("model == @model")
        .melt(
            # id_vars=["model"],
            value_vars=entity_columns,
            value_name="count",
            var_name="entity",
        )
        .sort_values(["count"], ascending=False)
    )

    flat_df["entity"] = flat_df.entity.str.replace("_count", "")

    fig, ax = plt.subplots(figsize=(8, 3.5))
    ax = sns.barplot(
        x="entity",
        y="count",
        data=flat_df,
        # make all columns blue
        color=FFOSSILS_PALETTE[1],
        ax=ax,
    )

    # set suptitle for plot
    fig.suptitle(
        title,
        fontsize=16,
        fontweight="bold",
    )

    # use title for token count info
    ax.set_title(
        f"Total Token Count: {results_df.query('model == @model').iloc[0].num_tokens}\nNo. of Entities: {flat_df['count'].sum()}",
        fontsize=12,
        fontweight="bold",
    )

    # fix cut off of left/right of plot
    fig.subplots_adjust(top=0.80, bottom=0.15)

    ax.bar_label(ax.containers[0], fmt="%.0f")
    ax.set_xlabel("Entity Type")
    ax.set_ylabel("Number of Occurences")
    plt.show()


```

```{python}
#| label: fig-entity-dists
#| fig-cap: "Entity Distribution"
#| fig-subcap:
#|  - "Entity Distribution for Train set"
#|  - "Entity Distribution for Validation set"
#|  - "Entity Distribution for Test set"
#| layout-ncol: 1
sns.set_style(
    "whitegrid",
    rc={
        "grid.color": ".85",
    },
)
# print(model_results_df.columns)
sns.set_context("notebook")

train_results_df = load_model_evaluation_results(
    results_path=os.path.join(os.pardir, os.pardir, "results", "ner"),
    results_type="train",
)
plot_distribution_of_entities(
    train_results_df,
    "Roberta Finetuned V3",
    title="Entity Distribution for Train Set",
)

val_results_df = load_model_evaluation_results(
    results_path=os.path.join(os.pardir, os.pardir, "results", "ner"),
    results_type="val",
)
plot_distribution_of_entities(
    val_results_df,
    "Roberta Finetuned V3",
    title="Entity Distribution for Validation Set",
)

test_results_df = load_model_evaluation_results(
    results_path=os.path.join(os.pardir, os.pardir, "results", "ner"),
    results_type="test",
)
plot_distribution_of_entities(
    test_results_df, "Roberta Finetuned V3", title="Entity Distribution for Test Set"
)
```


### Approach

NER tasks are dominated by transformer based models [@transformer-train-tips]. The best performing non-transformer based models are transition-based stack long-short term memory (S-LSTM) models used in the spaCy package [@spacy].

The approaches considered along with the rationale for their inclusion/rejection from development are outlined in @tbl-ner-approaches.

| Approach                               | Rationale                                                                                                                                                                                             |
| :------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Rule Based Models                      | This served as the baseline using regex to extract known entities but was not developed further due to the known issues with text quality due to OCR issues and infeasibility for entities like SITE. |
| <br/>                                  | <br/>                                                                                                                                                                                                 |
| Transition Based Model                 | Low computational cost, ideal for low-compute resource deployment                                                                                                                                     |
| <br/>                                  | <br/>                                                                                                                                                                                                 |
| BERT Based Models                      | Many state of the art apporaches use BERT, multiple base models pre-trained on different domains are available                                                                                        |
| <br/>                                  | <br/>                                                                                                                                                                                                 |
| Text to Text Transfer Transformer (T5) | T5 models make each problem text to text based, for NER this requires significant pre/post processing to work and is excluded for this reason.                                                        |

: NER Approaches {#tbl-ner-approaches tbl-colwidths="[30,70]"}

For the transformer based models two approaches were used for training, spaCy command line interface (CLI) [@spacy] and HuggingFace's Training application programming interface (API) [@huggingface]. Each have advantages and disadvantages which are outlined in @tbl-spacy-pros-cons.

+--------------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------+
|                          | Pro                                                                                                 | Con                                      |
+==========================+=====================================================================================================+==========================================+
| spaCy Config Training    | -   Can integrate with any transformer hosted on HuggingFace                                        | -   Knowledge of bash scripting required |
|                          | -   Prebuilt config scripts that require minimal changes                                            | -   Limited configuration options        |
|                          | -   Access to spaCy's unique transition-based NER model                                             |                                          |
|                          | -   Ability to integrate multiple ML models in an ensemble                                          |                                          |
+--------------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------+
| Hugging Face Trainer API | -   Able to use non-NER models and add classification head easily                                   | -   More code required                   |
|                          | -   Able to easily integrate custom tracking of metrics and external logging to MLflow              |                                          |
+--------------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------+

: spaCy CLI andHugging Face's Training API Advantages and Disadvantages {#tbl-spacy-pros-cons tbl-colwidths="[15,50,35]"}

Using the Hugging Face training API multiple models were trained and evaluated. Each base model along with the hypothesis behind it's selection is outlined in @tbl-hf-model-hypoth.


| Model              | Hypothesis                                                                                                                                                  |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| RoBERTa-base       | One of the typically best performing models for NER [@roberta-ner-wang]                                                                                     |
| <br /> | <br /> |
| RoBERTa-large      | A larger model than the base version with potential to learn more complex relationships with the downside of larger compute times.                          |
| <br /> | <br /> |
| BERT-multilanguage | The known OCR issues and scientific nature of the text may mean the larger vocabulary of this multi-language model may deal with issues better.             |
| <br /> | <br /> |
| XLM-RoBERTa-base   | Another cross language model (XLM) but using the RoBERTa base architecture and pre-training.                                                                |
| <br /> | <br /> |
| Specter2           | This model is BERT based and finetuned on 6M+ scientific articles with itâ€™s own scientific vocabulary making it well suited to analyzing research articles. |

: Hugging Face Model Hypotheses {#tbl-hf-model-hypoth tbl-colwidths="[25,75]"}

Final hyper parameters used to train the models are outlined in @tbl-hf-train-hyperparams.

+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Parameters              | Notes                                                                                                                                                                                                            |
+=========================+==================================================================================================================================================================================================================+
| Batch size              | -   Maximized to utilize all available GPU memory, 8 for RoBERTa based models                                                                                                                                    |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Gradient Accumulation   | -   Used to mimic larger batch sizes, this value was chosen to achieve batch sizes of \~12k tokens based on best practices \[@transformer-train-tips]                                                            |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Epochs                  | -   Initial runs with 10-20 epochs, observed evaluation loss minima occurring in first 2-8, settled on 10                                                                                     |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Learning Rate           | -   Initially 5e-5 was used and observed rapid over fitting with eval loss reaching a minimum around 2-4 epochs then increasing for the next 5-10                                                                |
|                         | -   Moved to 2e-5 as well as introducing gradient accumulation of 3 epochs to increase effective batch size                                                                                                      |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Learning Rate Scheduler | -   All training was done with a linear learning rate scheduler which linearly decreases learning rate across epochs                                                                                             |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Warmup Ratio            | -   How many steps of training to increase LR from 0 to LR, shown to improve with Adam optimizer - [@borealisai2023tutorial] Set to 10% initially                                                                |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Hugging Face Model Training Hyperparameters {#tbl-hf-train-hyperparams tbl-colwidths="[30,70]"}

Using the spaCy CLI, the two models were trained and evaluated with each models advantages and disadvanctages outlined in @tbl-spacy-model-pros-cons.

+------------------+----------------------------------------------------------------------+------------------------------------------------------+
| Model            | Advantages                                                           | Disadvantages                                        |
+==================+======================================================================+======================================================+
| RoBERTa-base     | -   State-of-the-art pretrained transformer for NLP tasks in English | -   Computationally expensive to train and inference |
|                  | -   Context rich embeddings                                          | -   Cannot fine-tune                                 |
|                  |                                                                      | -   Slow processing                                  |
+------------------+----------------------------------------------------------------------+------------------------------------------------------+
| en_core_web_md   | -   A smaller word vector model with static embeddings for words     | -   Static embeddings without context                |
|                  | -   Memory and speed efficient                                       | -   Cannot handle OOV words well                     |
|                  |                                                                      | -   Cannot fine-tune                                 |
+------------------+----------------------------------------------------------------------+------------------------------------------------------+

: spaCy CLI Model Advantages and Disadvantages {#tbl-spacy-model-pros-cons tbl-colwidths="[30,35,35]"}

Final hyper parameters used to train the spaCy models along with comments on each impact are outlined in @tbl-spacy-hyperparams.


| **Parameters**              | **Notes**                                                                                                                  |
|:----------------------------|:---------------------------------------------------------------------------------------------------------------------------|
| **Batch size**              | -   Maximized to utilize all available GPU memory, 128 for transformer based model and 512 for word vector based model     |
| <br/> | <br/> |
| **Epochs**                  | -   Initial runs with 15 epochs, observed evaluation loss minima occurring in first 7-13 depending on learning rate        |
| <br/> | <br/> |
| **Learning Rate**           | -   Initial learning rate of 5e-5                                                                                          |
| <br/> | <br/> |
| **Learning Rate Scheduler** | -   Warmup for 250 steps followed by a linear learning rate scheduler  |
| <br/> | <br/> |
| **Regularization**          | -   L2 (lambda = 0.01) with weight decay                                                                                   |
| <br/> | <br/> |
| **Optimizer**               | -   Adam (beta1 = 0.9, beta2=0.999)                                                                                        |
| <br/> | <br/> |
| **Early stopping**          | -   1600 steps                                                                                                             |

: spaCy CLI Final Hyperparameters {#tbl-spacy-hyperparams tbl-colwidths="[30,70]"}

Both the spaCy and HuggingFace models used RoBERTa as the base model. The spaCy model labels a sequence of inputs using an algorithm similar to transition-based dependency parsing using (Stack-LSTMs)[@S-LSTM]. Whereas the HuggingFace transformer model added a linear layer with softmax on top of the RoBERTa embeddings to perform the token classification.

The workflow in @fig-training_pipeline shows the primary steps for training with the Entity Extraction models with intermediate files and processes.

```{mermaid}
%%| label: fig-training-pipeline
%%| fig-cap: "The Entity Extraction model training process with intermediate files and processes."
%%| fig-height: 6
%%{init: {'theme':'base','themeVariables': {'fontFamily': 'arial','primaryColor': '#BFDFFF','primaryTextColor': '#000','primaryBorderColor': '#4C75A3','lineColor': '#000','secondaryColor': '#006100','tertiaryColor': '#fff'}, 'flowchart' : {'curve':'monotoneY'}}}%%
flowchart TD
F8(Labelled JSON files\nfrom LabelStudio) --> C2(Split into Train/Val/Test\nSets by xDD ID)
C2 --> C5(Convert to artifacts \nfor training)
C5 --> F1(test)
C5 --> F3(val artifact)
C5 --> F2(train artifact)
F3 --> C4
F2 --> C4(Run Model Training \nw/ Validation)
F1 ----> C3(Run Model Evaluation)
F3 --> C3
C4 --> F7(Log Metrics &\nCheckpoints)
C4 --> F4(Log Final\nTrained Model)
F4 --> C3
C3 --> F6(Evaluation Plots)
C3 --> F5(Evaluation results\nJSON)
```

### Model Evaluation

The target for the data extraction is to maximize recall. This was chosen as it maximizes the amount of entities extracted and minimizes the amount of time a reviewer needs to read the full article during review. Entity and token based recall are used as the primary metrics for model evaluation. Entity based recall checks all tokens of an entity matching are correct. Token based recall allows for partial matches of entities which contain multiple words. Higher token based recall is preferred as it maximizes data extracted and utilizes the data review process to correct any false positives.


## Data Review Tool

To facilitate the review and improve the efficiency of Neotoma data stewards, an interactive dashboard was developed as the final data product. This dashboard enables manual review of the Article Relevance Prediction and Article Data Extraction results. Users can compare extracted entities to sentences or access the full-text articles to make corrections. They also have the ability to delete incorrect entities and add any missed entities. The Data Review Tool generates a JSON object as output, which can be used to retrain the Article Entity Extraction model and update the Neotoma database. This process promotes enhanced information sharing and improved results, reducing the time required for data stewards to review extracted entities in articles.

### Approach

The project's objective was to develop a customizable and freely available product with custom components. Considering this, the decision was made to prioritize open-source solutions such as R Shiny [@shiny] and Plotly Dash [@dash] over paid alternatives like Tableau or Power BI. The choice between R Shiny and Plotly Dash was influenced by the project's customization requirements. During discussions with Neotoma, the decision to implement either of these options did not have any impact. Ultimately, the team concluded that Plotly Dash in Python would be the preferred choice for the ongoing development of the project dashboard as the rest of the pipeline was to be written in Python.

### Evaluation

In order to create an interactive tool that would be appropriate and efficient for the reviewers who are using this tool, the aim was to make it user-friendly and simple to use. To accomplish this success criteria, the requirements outlined in @tbl-review_metrics were developed along with the targets to make these requirements successful for the data reviewers.

| **Requirement**                                       | **Target**                                       |
| ----------------------------------------------------- | ------------------------------------------------ |
| Options for reviewing extracted data                  | Accept, Reject, Edit then Accept                 |
| ----------------------------------------------------- | ------------------------------------------------ |
| Other data made available to the user                 | Article DOI, Journal Name, Hyperlink to Article  |
| ----------------------------------------------------- | ------------------------------------------------ |
| Displaying text from where the data was extracted     | Current sentence and 1-2 sentences before/after. |
| ----------------------------------------------------- | ------------------------------------------------ |
| User skill to run                                     | Non-Technical (e.g. no code/CLI)                 |
| ----------------------------------------------------- | ------------------------------------------------ |
| Number of mouse clicks to review single piece of data | 1-2                                              |
| ----------------------------------------------------- | ------------------------------------------------ |
| Reviewing workflow                                    | Able to save/resume progress.                    |
| ----------------------------------------------------- | ------------------------------------------------ |
| Output file format                                    | JSON                                             |

: Data Review Tool Target Metrics {#tbl-review_metrics}

# Data Products and Results

## Article Relevance Prediction

![Model Performance Comparison using Cross-Validation Scores](assets/relevance-model-comparison.png){#fig-relevance_performance}

Among all the models trained, Naive Bayes had the highest recall score (96.7%), but its precision score was low (70.1%). Among all gradient boosted models, LGBM had the highest precision (93.9%), but its recall score (89.3%) did not outperform logistic regression (94.8%). Among the analogy-based models, SVM with RBF kernel achieved a recall performance (94.8%) that's comparable to logistic regression (95.0%), and both had sufficiently high precision (@fig-relevance_performance). The final decision was to use logistic regression with tuned hyperparameters for its high performance and better interpretability. On the test data set, the final model achieved a recall score of 96.5%, and a precision score of 85.2%. To put this into actual article counts, among the 666 articles in the test set, there are 5 false negatives and 24 false positives.

## Article Data Extraction

From training spaCy and Hugging Face models there are two candidate models proposed. The first is the Hugging Face finetuned RoBERTa based model which performed best of the Hugging Face models. The second is spaCy's transformer based model. The rationale for two models moving forward is the spaCy transformer model had better entity based recall along with better precision, meaning the extracted data was more often correctly extracted. Whereas the Hugging Face RoBERTa model achieves better token recall but has lower precision, meaning it detects more of the overall entities but contains more false detections which must be deleted by the reviewers. The differences in the recall and precision can be attributed to the different NER models leveraging the RoBERTa-base transformer embeddings differently.

@fig-extraction-recall-results shows the primary candidate models token and entity based recall.

```{python}
#| label: fig-extraction-recall-results
#| layout-ncol: 1
#| fig-cap: "The entity and token based recall for each model on the test set."

def plot_model_metric_comparison(
    results_df: pd.DataFrame,
    metrics: list,
    fig_height: int = 6,
    fig_width: int = 5,
    sort_ascending: bool = False,
):
    metrics_formatted = [metric.replace("_", " ").title() for metric in metrics]

    # flatten the results_df
    flat_df = results_df.melt(
        id_vars=["model"], value_vars=metrics, value_name="score", var_name="metric"
    )

    # multiply scores by 100
    flat_df.loc[:, "score"] = flat_df.score * 100

    # format the metric names
    flat_df.loc[:, "metric"] = flat_df.metric.str.replace("_", " ").str.title()

    # plot token level recall by model using seaborn
    fig, ax = plt.subplots(figsize=(fig_width, fig_height))
    # sns.set_theme(style="whitegrid")
    ax = sns.barplot(
        y="model",
        x="score",
        hue="metric",
        data=flat_df,
        order=flat_df.iloc[
            flat_df[flat_df["metric"] == metrics_formatted[0]]["score"]
            .sort_values(ascending=sort_ascending)
            .index
        ]["model"],
        palette=FFOSSILS_PALETTE,
        ax=ax,
    )
    ax.set_title(
        f"Entity Extraction Model Performance Comparison\n",  # \nMetrics: {', '.join(metrics_formatted)}",
        fontsize=12,
        fontweight="bold",
    )
    # fix cut off of left/right of plot
    fig.subplots_adjust(left=0.35, right=0.90, top=0.80)
    for i in range(0, len(metrics)):
        ax.bar_label(ax.containers[i], fmt="%.1f%%", fontsize=10, padding=2)
    ax.set_xlabel(None)
    # set x tick locations
    ax.set_xticks([0, 20, 40, 60, 80, 100])
    # format xtick labels to have % symbols
    xtick_labels = [f"{int(x)}%" for x in ax.get_xticks()]
    ax.set_xticklabels(xtick_labels)

    ax.set_ylabel(None)
    ax.set_xlim(0.0, max(100, flat_df.score.max()))
    # plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, fontsize=12) #bbox_to_anchor=(1.02, 0.5),
    ax.legend(
        loc="upper center",
        bbox_to_anchor=(0.5, 1.1),
        ncol=3,
        fancybox=True,
        shadow=True,
    )
    sns.despine(bottom=True)
    plt.show()


model_results_df = load_model_evaluation_results(
    results_path=os.path.join(os.pardir, os.pardir, "results", "ner"),
    results_type="test",
)

sns.set_style(
    "whitegrid",
    rc={
        "grid.color": ".85",
    },
)
sns.set_context("paper")

plot_model_metric_comparison(
    model_results_df,
    ["entity_recall", "token_recall"],
    fig_height=4,
    fig_width=6,
)
```


The token based recall for each model on the test set by entity type are shown in @tbl-token-recall.

```{python}
#| label: tbl-token-recall
#| tbl-cap: 'Entity extraction model results for token based recall, overall and by entity type'
#| tbl-cap-location: bottom
#| output: markdown

# render the token_recall, token_precision, and token_f1 scores for each model and each entity type
render_columns = [
    "model",
    "token_recall",
    "token_AGE_recall",
    "token_SITE_recall",
    "token_TAXA_recall",
    "token_REGION_recall",
    "token_GEOG_recall",
    "token_EMAIL_recall",
    "token_ALTI_recall",
]

render_df = test_results_df[render_columns].copy()
render_df.columns = render_df.columns.str.replace("_", " ").str.title()
render_df = render_df.melt(id_vars=["Model"], var_name="Metric", value_name="Score")
render_df["Metric"] = (
    render_df.Metric.str.replace("Token ", "")
    .str.replace(" Recall", " (%)", regex=True)
    .str.replace("Recall", "Recall (%)")
)
render_df["Model"] = (
    render_df.Model.str.replace("Finetuned", "")
    .str.replace("Spacy", "SpaCy")
    .str.replace("Finetune", "")
    .str.replace("V\d", "", regex=True)
    .str.replace("Transformer", "Transf.")
    .str.replace("Multilanguage", "Multi-lang.")
    .str.replace("Ner", "NER")
    .str.replace("bert", "BERT")
    .str.replace("Bert", "BERT")
)
render_df["Score"] = render_df.Score * 100
render_df = (
    render_df.reset_index(drop=True)
    .round(1)
    .pivot(index="Model", columns="Metric", values="Score")
    .sort_values(["Recall (%)"], ascending=False)
)

render_df = render_df[
    [
        "Recall (%)",
        "Age (%)",
        "Site (%)",
        "Taxa (%)",
        "Region (%)",
        "Geog (%)",
        "Email (%)",
        "Alti (%)",
    ]
]

Markdown(render_df.to_markdown())
```

An important observation to make here is that the top models had a lower precision score for the SITE names and REGION names. The models got confused when deciding whether an entity should be classified as a SITE or a REGION. This was partially due to quality of labeling entities as well as the fact that both these types correspond to the name of a place or a wider area. See @fig-confusion_matrix for a confusion matrix generated using the test set assets highlights the issue.

![Confusion Matrix for RoBERTa Model](../../results/ner/roberta-finetuned-v3/roberta-finetuned-v3_test_confusion_matrix.png){#fig-confusion_matrix}

## Data Review Tool

The final data review tool that was created is a multi-page Plotly Dash [@dash] application. The tool can be replicated by launching Docker containers, enabling anyone within the Neotoma community to easily utilize the tool for reviewing outputs from the pipeline. 

![Data Review Tool](assets/data_review.png){#fig-review_tool_snap}

This web application enables users to review the extracted entities from articles, make changes, add additional missed entities and remove incorrectly extracted entities. The entire review process does not require any coding knowledge, and reviewers can navigate through the review workflow in three clicks or fewer from the Article Review page @fig-review_tool_snap.

The output of this data review tool is a parquet file that stores the originally extracted entities as well as the corrected entities. The Neotoma organization can utilize these entities to add new paleoecology articles to the database. In addition, the reviewed entities can be fed back to model using our model retraining pipeline, contributing to the continuous improvement of the quality of extraction of entities as outlined in @tbl-review_results.

| **Requirement**                                         | **Results**                                   |
| ------------------------------------------------------- | --------------------------------------------- |
| Options for reviewing extracted data                    | Accept, Reject, Edit, Add then Accept         |
| ----------------------------------------------------- | ------------------------------------------------ |
| Other data made available to the user                   | Article DOI, Hyperlink to Article             |
| ----------------------------------------------------- | ------------------------------------------------ |
| Displaying text from where the data was extracted       | Current sentence and 1 sentence before/after. |
| ----------------------------------------------------- | ------------------------------------------------ |
| User skill to run                                       | Non-Technical (e.g. no code/CLI)              |
| ----------------------------------------------------- | ------------------------------------------------ |
| Number of mouse clicks to review a single piece of data | 3 clicks from launch                          |
| ----------------------------------------------------- | ------------------------------------------------ |
| Reviewing workflow                                      | Able to save/resume progress.                 |
| ----------------------------------------------------- | ------------------------------------------------ |
| Output file format                                      | JSON                                          |

: Data Review Tool Metric Results {#tbl-review_results}


## Product Deployment

The end goal of this project is to have each data product running unsupervised. The article relevance prediction pipeline was containerized using Docker. It is expected to run on a daily or a weekly basis by Neotoma to run the article relevance prediction and submit relevant articles to xDD to have their full text processed. 

The Article Data Extraction pipeline was containerized using Docker and contains the entity extraction model within it. It will be run on the xDD servers as xDD is not legally allowed to send full text articles off their servers. The container accepts full text articles, extracts the entities, and outputs a single JSON object for each article. The JSON objects are combined with the article relevance prediction results and loaded into the Data Review Tool. @fig-deployment_pipeline depicts the work flow.

```{mermaid}
%%| label: fig-deployment_pipeline
%%| fig-cap: "This is how the MetaExtractor pipeline flows between the different components."
%%| fig-height: 6
graph TD
subgraph neotoma [Neotoma Servers]
    direction TB
    L(CRON Job Starts Pipeline)
    subgraph relevance_docker [Docker Image &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp]
      A(Get New Article DOIs<br> Since Last Run)
      B(Predict Article Relevance)
    end
end
subgraph xdd [xDD Servers &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp]
    direction TB
  C(To Be Processed Stack)
    subgraph entity_docker [Docker Image &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp]
        D(xDD Inputs Full Text Articles)
        E(Entity Extraction Pipeline)
    end
end
G(Article Relevance Predictions)
H(Extracted Entities per Article)
I(Combine Article Relevance <br>& Extracted Entities)
J(Data Review Tool)
L --> A
A --> B
B -----> |Parquet File| G
B --> |API Put Request| C
C --> D
D --> E
E --> |JSON Per Article,\nLog File| H
G --> I
H --> I
I --> |Parquet| J

%% create a class for styling subgraphs
classDef subg fill:#fbfbfb,stroke:#333,stroke-width:4px,font-weight:bold;
classDef docker fill:#cef1fc,stroke:#0db7ed,stroke-width:4px,font-weight:bold,align:left;
%% create a class for styling the nodes
classDef nodes fill:#D1E5F4,stroke:#333,stroke-width:2px;
%% style each node
class A,B,C,D,E,F,G,H,I,J,K,L nodes;
%% style each subgraph
class neotoma,xdd subg;
class entity_docker,relevance_docker docker;
```

# Conclusion and Recommendations

## Conclusions

The Neotoma database plays a crucial role in paleoecological research by providing data on ecological changes over the past 5 million years. However, the reliance on manual submissions by researchers has led to difficulties in data entry and possibly hindered collaborative efforts to fully understand these ecological changes. The implementation of our pipeline brings significant benefits to the Neotoma community and paleoecological research. Firstly, by automating the search for relevant articles through the Article Relevance Prediction this process will grow the Neotoma community, attracting more researchers and fostering collaboration in ecological research. Secondly, researchers will experience reduced time and effort required for data uploads, as some of the manual submission process is replaced by the Article Entity Extraction and Data Review Tool. Finally, with the addition of more data, researchers in the Neotoma community may be able to answer questions about ecological changes that were previously unknown.

## Recommendations

Due to time constraints, improvements were identified but could not be implemented within the project's timeframe. Three essential features have been identified that would enhance the overall pipeline.

1.  **Relevance Review Page:** incorporating an article review page would allow for a thorough assessment of article relevance, reducing both false positives and false negatives in the predictions. It will also reduce the bias in the training dataset that currently exists due to our data collection strategy.

2.  **Adding new entities:** There are additional entities that were considered beneficial but not mandatory by the Neotoma team. Adding additional entities can be done in LabelStudio and could benefit the Neotoma community at large.

3.  **Correction Model** There are likely many corrections that will be performed frequently on the data review tool. Therefore, a model could be implemented to learn these corrections and preprocess these corrections to reduce time spent reviewing.

\newpage

# Acknowledgements

Data were obtained from the Neotoma Paleoecology Database (http://www.neotomadb.org) and its constituent database(s). The work of data contributors, data stewards, and the Neotoma community is gratefully acknowledged.

A huge thanks to Simon Goring & Socorro Dominguez from the Neotoma Database team for their support on this project.

\newpage

# References
