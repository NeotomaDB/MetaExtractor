---
title: |
  Final Report: \
  Finding Fossils in the Literature \  
  ![](assets/finding-fossils-logo-symbol_highres.png){width=1.5in} \
subtitle: In partnership with the Neotoma Paleoecology Database.
date: today
author:   
  - Ty Andrews  
  - Jenit Jain
  - Kelly Wu
  - Shaun Hutchinson
execute: 
  echo: false
bibliography: assets/references.bib
format:
  pdf:
    toc: false
    number-sections: true
    colorlinks: true
params: 
  output_file: "reports"
fig-cap-location: top
---

**Executive Summary**

The project Finding Fossils in Literature is sponsored by the Neotoma database (Neotoma) which houses a paleoecology database. The challenges Neotoma faces are 1) researchers have to manually enter sample data into Neotoma, 2) researchers are not aware of Neotoma or that their research fits into it, and 3) there are too many articles published for the Neotoma team to monitor new research. This project has 3 primary deliverables to solve the challenges, first is an article relevance prediction model which predicts whether newly published articles are relevant to Neotoma. Second, is an article data extraction pipeline which identifies key entities such as taxa or geographic location. Last is a data review tool for Neotoma data stewards to review the extracted data before it is submitted to Neotoma. Once the extracted data has been reviewed, the corrections will be used to retrain the first two models of the pipeline.

\newpage

```{r}
# custom table of contents location inspiration from here: https://stackoverflow.com/questions/61104292/how-can-i-separate-the-title-page-from-the-table-of-contents-in-a-rmarkdown-word
```

```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
\newpage

# Introduction

The Neotoma database (Neotoma) [@NeotomaDB] serves as a valuable resource for researchers investigating ecological transformations spanning the last 5 million years. Nevertheless, the collection of data heavily relies on manual submissions from researchers, presenting challenges in data entry and impeding collaborative endeavors aimed at achieving a comprehensive understanding of ecological changes.This project aims to automate the extraction of data from relevant journal articles which can be added to Neotoma. This can be completed in three parts. First, article relevancy for the Neotoma database is predicted. Relevant articles are then parsed using natural language processing techniques. Finally, an interactive data review tool is built to review and correct the extracted data before it is submitted to Neotoma. The outputs of this data review tool are then used to retrain the two data models in the future.
\newpage

# Data Science Methods

## Article Relevance Prediction

The article relevance prediction was posed as a binary classification problem. Based on the available article metadata, the model was able to predict how likely the article was relevant to the Neotoma paleoecology/paleoenvironment database. If the article was deemed likely relevant, the article will proceed to the next stage for article data extraction.

### Approach

#### Building the Training Data

To train the supervised classification model, a sample of labelled articles has been compiled as shown in @tbl-relevance_sample.

| **Label** | **Sample Size** | **Description**                                                                                                                                                                                                                                      |
|:--:|:--:|:---------|
| Positive  |       911       | A randomly sampled list of articles that currently contribute to Neotoma was provided as the positive cases. These articles cover various types of fossil data in the Neotoma database.                                        |
|-----------------|--------------------|---------------------------------------------------------------------|
| Negative  |      3523       | Articles from non-paleoecology-related subjects were queried to form a representative sample of non-relevant articles.  |

: Labelled Sampled Article {#tbl-relevance_sample} {tbl-colwidths="[25,25,75]"}

The labelled articles were split into train/validation/test sets with splits of 0.7/0.15/0.15, which correspond to 3103/665/666 articles. Due to the relatively small total sample size, 70% of the articles were included in the training set so that there are more examples provided during training.

#### Preprocessing & Feature Selection

Article's metadata were retrieved from the CrossRef API [@crossref]. There were over 50 types of metadata provided by the CrossRef API. Since many of them are related to the publication aspect of the article, feature selection was performed to reduce dimensionality. @tbl-feature_selection explains the key decisions made during the feature selection step.

| **Feature**                                | **Decision**                                                                                                                                                                                                                                                                                                                                                                  |
|:----|:--------|
| Title & subtitle                           | Title and subtitle (if any) are concatenated as a descriptive text feature. Text representation techniques were then applied to this feature.                                                                                                                                                                                                                                 |
|------------------------------------------|-----------------------------------------------------------------------|
| Abstract                                   | Less than 50% of articles have abstracts available due to copyright issues. To address this, the available abstracts are combined with the article title and subtitle to create a descriptive text feature. A binary feature indicates whether the article has an abstract or not, which can affect model predictions. |
|------------------------------------------|-----------------------------------------------------------------------|
| Author & Journal                           | In order to mitigate potential bias towards well-established authors and journals, the feature list was modified to exclude time-sensitive indicators to account for new Authors and Journals published in the future.                                                            |
|------------------------------------------|-----------------------------------------------------------------------|
| Subject                                    | The subject of the journal was used as a proxy of the name of the journal.                                                                                                                                                                                                                                                                      |
|------------------------------------------|-----------------------------------------------------------------------|
| Number of citation                         | Number of citations could indicate if the article contains important data and is referred to in other articles.                                                                                                                                                                                                                                                   |
|------------------------------------------|-----------------------------------------------------------------------|
| Feature related to the publication process | These features do not provide any contextual information about the article, thus were removed from the feature list.                                                                                                                                                                                                                                                     |


: Feature Selection Decisions {#tbl-feature_selection}

The features selected include a descriptive text feature (concatenation of title, subtitle and abstract), subject of the journal, number of citations and a binary indicator for having an abstract available from CrossRef or not.

#### Feature Engineering: Text representation

The descriptive text feature provides crucial contextual information for the model to predict if the topic was related to paleoecology or not. Feature engineering was conducted to represent this descriptive text feature in numeric forms. The following text representations were explored and tested to identify the most effective approach.

**Bag of words representation (Baseline):** word-frequency based method. While the model with bag of words feature achieved a decent performance, the limitation is that the semantic aspect of the text could not be well represented. This method was chosen to be used in the baseline model.

**Sentence embedding (Final):** Sentence embedding is the state-of-the-art approach for text representation. Three sentence embedding models were experimented and allenai/specter2 model resulted in the best overall performance as documented in @tbl-sent_embedding.

| Model                       | Result                                                                                                                                                                                   |
| --------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| bert-tiny-finetuned-squadv2 | This model was of a small scale so that the embedding process would be quick.                                                                                                              |
| --------------------------- | -------------------------------------------------------------------------------------- |
| Biobert                     | This model was pre-trained based on large-scale biomedical corpora, thus it could be better at understanding biology-related jargons that frequently appear in the field of paleoecology. |
| --------------------------- | --------------------------------------------------------------------------------------- |
| specter2                    | This model was pre-trained using over 6 million scientific articles, thus it could be better at understanding the language style and pattern in scientific articles.                      |

: Sentence Embedding Models {#tbl-sent_embedding} {tbl-colwidths="[25,75]"}

#### Model Selection

The following selection of Probability Classifiers, Analogy-based and Tree-based & Gradient-boosted models were experimented with.

**Probability classifiers:** Traditional probability classifiers provide fast training and prediction time, which could be beneficial when processing and retraining large numbers of new articles. Its simplicity also could provide favorable model transparency and interpretability for model evaluation. Logistic regression and naive Bayes were experimented during model selection.

**Analogy-based models:** The nature of the article relevance prediction problem could be framed as looking for articles that were similar to the positive examples. Analogy-based models could be suitable for this problem. K-nearest neighbour and Support Vector Machine with RBF kernel were experimented.

**Tree-based & Gradient-boosted models:** Tree-based classifiers could use different feature subsets at various steps during the prediction, which can be suitable for our relatively high dimensional data problem. Decision Tree, Random Forest Model, LightGBM, XGBoost and CatBoost were experimented. 

The choice of the final model was primarily based on model performance on recall score. The chosen model went through further hyperparameter tuning to optimise the performance.

### Model Evaluation

The main goal of article relevance prediction is to identify potentially as many useful articles as possible from the article repository so that valuable research data can be discovered. The cost of false negatives is high because if the model does not catch a relevant article, this article will unlikely be discovered by the Neotoma community and its data will be missed. On the other hand, the cost of false positives is more manageable. Hence, recall was the metric that was optimized. During data validation, Data Stewards are able to easily mark an article as not relevant using our data review tool.

Another key consideration was model interpretation. The positive examples were gathered using articles that currently exist in the Neotoma database, and it was difficult to tell if there are existing biases in the training data. A more interpretable model would help identify model biases and make corrections in the long run.

## Article Data Extraction

```{python}
# | warning: false
import pandas as pd
import os
import json
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import Markdown

FFOSSILS_PALETTE = sns.color_palette(
    ["#677e52", "#8fbed5", "#1f3a5f", "#005a3c", "#4c2f27", "#b8864b"]
)


def load_model_evaluation_results(results_path: str, results_type: str = "val"):
    results_df = pd.DataFrame()

    for model in os.listdir(results_path):
        if os.path.isdir(os.path.join(results_path, model)):
            model_path = os.path.join(results_path, model)
            for file in os.listdir(model_path):
                if (file.endswith("classification_results.json")) & (
                    results_type in file
                ):
                    results_dict = json.load(open(os.path.join(model_path, file), "r"))

                    parsed_dict = {}

                    for key, value in results_dict.items():
                        # original entity and token based results
                        if (key == "token") | (key == "entity"):
                            for k, v in value.items():
                                # print(f"key: {key} k: {k} v: {v}")
                                if k == "classification_report":
                                    for kk, vv in v.items():
                                        # print(f"\tkk: {kk} vv: {vv}")
                                        for kkk, vvv in vv.items():
                                            # print(f"\t\tkkk: {kkk} vvv: {vvv}")
                                            parsed_dict[
                                                f"{key}_{kk}_{kkk}".replace(" ", "-")
                                            ] = [vvv]
                                else:
                                    parsed_dict[f"{key}_{k}"] = [v]
                        # overall model results with new metrics
                        elif key == "overall_results":
                            for k, v in value.items():
                                # print(f"key: {key} k: {k} v: {v}")
                                for kk, vv in v.items():
                                    # print(f"\tkk: {kk}, vv: {vv}")
                                    if kk in [
                                        "precision",
                                        "recall",
                                        "actual",
                                        "possible",
                                    ]:
                                        parsed_dict[
                                            f"overall_{k.replace('_', '-')}_{kk}"
                                        ] = [vv]
                                    else:
                                        parsed_dict[
                                            f"overall_{k.replace('_', '-')}_percent-{kk}"
                                        ] = [vv / v["possible"]]
                        # overall results by each tag group
                        elif key == "overall_results_by_tag":
                            for label, v in value.items():
                                # print(f"key: {key} label: {label} v: {v}")
                                for eval_type, vv in v.items():
                                    # print(f"\teval_type: {eval_type} vv: {vv}")
                                    for result, vvv in vv.items():
                                        # print(f"\t\tresult: {result}, vvv: {vvv}")
                                        if result in [
                                            "precision",
                                            "recall",
                                            "actual",
                                            "possible",
                                        ]:
                                            parsed_dict[
                                                f"{label}_{eval_type.replace('_', '-')}_{result}"
                                            ] = [vvv]
                                        else:
                                            parsed_dict[
                                                f"{label}_{eval_type.replace('_', '-')}_percent-{result}"
                                            ] = [vvv / vv["possible"]]
                        # add in the counts of each entity type
                        elif key == "entity_counts":
                            for k, v in value.items():
                                # print(f"key: {key} k: {k} v: {v}")
                                parsed_dict[f"{k}_count"] = [v]
                        elif key == "num_tokens":
                            parsed_dict[f"{key}"] = [value]

                    model_results = pd.DataFrame(parsed_dict)

                    model_results["model"] = model.replace("-", " ").title()
                    results_df = pd.concat([results_df, model_results])

    return results_df


```

The article data extraction was posed as a Named Entity Recognition (NER) problem. The NER tasks teaches the model to predict the correct label for each word where each word is assigned a label.

### Data Description

An original sample of 300+ full text articles related to Neotoma was provided. Some articles were non-english and were removed from the dataset. The data of interest to be extracted and thus labelled in the articles using NER are as follows:

-   **SITE**: the specific site name given to the items studied
-   **REGION**: general geographic regions around the site to give general context to locations
-   **GEOG**: geographic coordinates of sites or samples
-   **TAXA**: the taxa uncovered and researched in the article
-   **AGE**: any ages relevant to the samples
-   **ALTI**: the altitude at which samples were studied
-   **EMAIL**: any researchers emails for further follow up

### Data Preprocessing and Labelling

For the development of the NER models, a sample of 39 articles relevant to Neotoma were preprocessed and labelled by the research team using LabelStudio. Preprocessing steps involved chunking the articles into bite-sized pieces and replacing special tokens with their English equivalent symbol. Other text preprocessing techniques were not used for this project as the aim was to preserve the original text and ensure that the model understands contextual information. The Label Studio interface hosted on HuggingFace was used for labeling purposes as it provides an out-of-the-box collaborative environment for several ML tagging tasks, including named entity recognition. The SITE entity proved difficult to label as differentiating between obscure location names or an acronym of a core sample was difficult for the team. Similarly, correctly identifying TAXA and where on TAXA label began and another ends proved challenging. A significant improvement in results is expected by improving the quality of labeling with the help of data stewards with subject matter expertise.

The labelled articles were split into train/validation/test sets by full article with splits of 70%/15%/15% respectively. The data splitting was done based upon whole articles to prevent data leakage into the model learning certain articles patterns before evaluation. This resulted in the entity distributions between train/validation/test as shown in @fig-entity-dists.

```{python}
def plot_distribution_of_entities(
    results_df: pd.DataFrame, model: str, title: str = None
):
    entity_columns = [c for c in results_df if "count" in c and c != "O_count"]

    flat_df = (
        results_df.query("model == @model")
        .melt(
            # id_vars=["model"],
            value_vars=entity_columns,
            value_name="count",
            var_name="entity",
        )
        .sort_values(["count"], ascending=False)
    )

    flat_df["entity"] = flat_df.entity.str.replace("_count", "")

    fig, ax = plt.subplots(figsize=(8, 3.5))
    ax = sns.barplot(
        x="entity",
        y="count",
        data=flat_df,
        # make all columns blue
        color=FFOSSILS_PALETTE[1],
        ax=ax,
    )

    # set suptitle for plot
    fig.suptitle(
        title,
        fontsize=16,
        fontweight="bold",
    )

    # use title for token count info
    ax.set_title(
        f"Total Token Count: {results_df.query('model == @model').iloc[0].num_tokens}\nNo. of Entities: {flat_df['count'].sum()}",
        fontsize=12,
        fontweight="bold",
    )

    # fix cut off of left/right of plot
    fig.subplots_adjust(top=0.80, bottom=0.15)

    ax.bar_label(ax.containers[0], fmt="%.0f")
    ax.set_xlabel("Entity Type")
    ax.set_ylabel("Number of Occurences")
    plt.show()


```

```{python}
#| label: fig-entity-dists
#| fig-cap: "Entity Distribution"
#| fig-subcap:
#|  - "Entity Distribution for Train set"
#|  - "Entity Distribution for Validation set"
#|  - "Entity Distribution for Test set"
#| layout-ncol: 1
sns.set_style(
    "whitegrid",
    rc={
        "grid.color": ".85",
    },
)
# print(model_results_df.columns)
sns.set_context("notebook")

train_results_df = load_model_evaluation_results(
    results_path=os.path.join(os.pardir, os.pardir, "results", "ner"),
    results_type="train",
)
plot_distribution_of_entities(
    train_results_df,
    "Roberta Finetuned V3",
    title="Entity Distribution for Train Set",
)

val_results_df = load_model_evaluation_results(
    results_path=os.path.join(os.pardir, os.pardir, "results", "ner"),
    results_type="val",
)
plot_distribution_of_entities(
    val_results_df,
    "Roberta Finetuned V3",
    title="Entity Distribution for Validation Set",
)

test_results_df = load_model_evaluation_results(
    results_path=os.path.join(os.pardir, os.pardir, "results", "ner"),
    results_type="test",
)
plot_distribution_of_entities(
    test_results_df, "Roberta Finetuned V3", title="Entity Distribution for Test Set"
)
```


### Approach

NER tasks are dominated by transformer based models. The best performing non-transformer based models are a transition-based stack long-short term memory (S-LSTM) model used in the spaCy package.

The approaches considered along with the rationale for their inclusion/rejection from development are outlined in @tbl-ner-approaches.

| Approach                               | Rationale                                                                                                                                                                                             |
| :------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Rule Based Models                      | This served as the baseline using regex to extract known entities but was not developed further due to the known issues with text quality due to OCR issues and infeasibility for entities like SITE. |
| <br/>                                  | <br/>                                                                                                                                                                                                 |
| Transition Based Model                 | Has lower computational cost ideal for low-compute resource deployment                                                                                                                                |
| <br/>                                  | <br/>                                                                                                                                                                                                 |
| BERT Based Models                      | These are achieving state of the art results and have multiple base models pre-trained on different domains                                                                                           |
| <br/>                                  | <br/>                                                                                                                                                                                                 |
| Text to Text Transfer Transformer (T5) | T5 models turn every problem into a text to text problem which for NER requires significant pre/post processing to work and was excluded for this reason.                                             |

: NER Approaches {#tbl-ner-approaches tbl-colwidths="[30,70]"}

For the transformer based models two approaches were used for training, spaCy command line interface (CLI) [@spacy] and HuggingFace's Training application programming interface (API) [@huggingface]. Each have advantages and disadvantages outlined in @tbl-spacy-pros-cons.

+--------------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------+
|                          | Pro                                                                                                 | Con                                      |
+==========================+=====================================================================================================+==========================================+
| spaCy Config Training    | -   Can integrate with any transformer hosted on HuggingFace                                        | -   Knowledge of bash scripting required |
|                          | -   Prebuilt config scripts that require minimal changes                                            | -   Limited configuration options        |
|                          | -   Access to spaCy's unique transition-based NER model                                             |                                          |
|                          | -   Ability to integrate multiple ML models in an ensemble -Integrates with visualization libraries |                                          |
+--------------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------+
| Hugging Face Trainer API | -   Able to use non-NER models and add classification head easily                                   | -   More code required                   |
|                          | -   Able to easily integrate custom tracking of metrics and external logging to MLflow              |                                          |
+--------------------------+-----------------------------------------------------------------------------------------------------+------------------------------------------+

: spaCy CLI andHugging Face's Training API Advantages and Disadvantages {#tbl-spacy-pros-cons tbl-colwidths="[15,50,35]"}

Using the Hugging Face training API multiple models were trained and evaluated. The model along with the hypothesis behind it's selection is outlined in @tbl-hf-model-hypoth.


| Model              | Hypothesis                                                                                                                                                  |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| RoBERTa-base       | One of the typically best performing models for NER [@roberta-ner-wang]                                                                                     |
| <br /> | <br /> |
| RoBERTa-large      | A larger model than the base version with potential to learn more complex relationships with the downside of larger compute times.                          |
| <br /> | <br /> |
| BERT-multilanguage | The known OCR issues and scientific nature of the text may mean the larger vocabulary of this multi-language model may deal with issues better.             |
| <br /> | <br /> |
| XLM-RoBERTa-base   | Another cross language model (XLM) but using the RoBERTa base architecture and pre-training.                                                                |
| <br /> | <br /> |
| Specter2           | This model is BERT based and finetuned on 6M+ scientific articles with itâ€™s own scientific vocabulary making it well suited to analyzing research articles. |

: Hugging Face Model Hypotheses {#tbl-hf-model-hypoth tbl-colwidths="[25,75]"}

Final hyper parameters used to train the models are outlined in @tbl-hf-train-hyperparams.

+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Parameters              | Notes                                                                                                                                                                                                            |
+=========================+==================================================================================================================================================================================================================+
| Batch size              | -   Maximized to utilize all available GPU memory, 8 for RoBERTa based models, 4 for large models                                                                                                                |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Gradient Accumulation   | -   Used to mimic larger batch sizes, this value was chosen to achieve batch sizes of \~12k tokens based on existing research \[SOURCE\]                                                                         |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Epochs                  | -   Initial runs with 10-20 epochs, observed evaluation loss minima occurring in first 2-8 depending on learning rate below                                                                                      |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Learning Rate           | -   Initially 5e-5 was used and observed rapid over fitting with eval loss reaching a minimum around 2-4 epochs then increasing for the next 5-10                                                                |
|                         | -   Moved to 2e-5 as well as introducing gradient accumulation of 3 epochs to increase effective batch size, the eval loss didn't reach a minimum for a bit longer while recall continued to improve             |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Learning Rate Scheduler | -   All initial training has been done with a linear learning rate scheduler which linearly decreases learning rate across epochs                                                                                |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Warmup Ratio            | -   How many steps of training to increase LR from 0 to LR, shown to improve with Adam optimizer - [@borealisai2023tutorial] Set to 10% initially                                                                |
+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Hugging Face Model Training Hyperparameters {#tbl-hf-train-hyperparams tbl-colwidths="[30,70]"}

Using the spaCy CLI, the two models were trained and evaluated with each models advantages and disadvanctages outlined in @tbl-spacy-model-pros-cons.

+------------------+----------------------------------------------------------------------+------------------------------------------------------+
| Model            | Advantages                                                           | Disadvantages                                        |
+==================+======================================================================+======================================================+
| RoBERTa-base     | -   State-of-the-art pretrained transformer for NLP tasks in English | -   Computationally expensive to train and inference |
|                  | -   Context rich embeddings                                          | -   Cannot fine-tune                                 |
|                  |                                                                      | -   Slow processing                                  |
+------------------+----------------------------------------------------------------------+------------------------------------------------------+
| en_core_web_md   | -   A smaller word vector model with static embeddings for words     | -   Static embeddings without context                |
|                  | -   Memory and speed efficient                                       | -   Cannot handle OOV words well                     |
|                  |                                                                      | -   Cannot fine-tune                                 |
+------------------+----------------------------------------------------------------------+------------------------------------------------------+

: spaCy CLI Model Advantages and Disadvantages {#tbl-spacy-model-pros-cons tbl-colwidths="[30,35,35]"}

Final hyper parameters used to train the spaCy models along with comments on each impact are outlined in @tbl-spacy-hyperparams.


| **Parameters**              | **Notes**                                                                                                                  |
|:----------------------------|:---------------------------------------------------------------------------------------------------------------------------|
| **Batch size**              | -   Maximized to utilize all available GPU memory, 128 for transformer based model and 512 for word vector based model     |
| <br/> | <br/> |
| **Epochs**                  | -   Initial runs with 15 epochs, observed evaluation loss minima occurring in first 7-13 depending on learning rate        |
| <br/> | <br/> |
| **Learning Rate**           | -   Initial learning rate of 5e-5                                                                                          |
| <br/> | <br/> |
| **Learning Rate Scheduler** | -   Warmup for 250 steps followed by a linear learning rate scheduler which linearly decreases learning rate across epochs |
| <br/> | <br/> |
| **Regularization**          | -   L2 (lambda = 0.01) with weight decay                                                                                   |
| <br/> | <br/> |
| **Optimizer**               | -   Adam (beta1 = 0.9, beta2=0.999)                                                                                        |
| <br/> | <br/> |
| **Early stopping**          | -   1600 steps                                                                                                             |

: spaCy CLI Final Hyperparameters {#tbl-spacy-hyperparams tbl-colwidths="[30,70]"}

An interesting difference were the architectures of the NER models. Both the models use a RoBERTa-base model to tokenize the input sentence and generate word embeddings, but the HuggingFace NER model fine-tunes a token classification head, which consists of a set of linear layer and soft max, whereas, the spaCy NER model chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing using (Stack-LSTMs)[@S-LSTM].

The workflow in @fig-training_pipeline shows the primary steps for training with the Entity Extraction models with intermediate files and processes.

```{mermaid}
%%| label: fig-training-pipeline
%%| fig-cap: "The Entity Extraction model training process with intermediate files and processes."
%%| fig-height: 6
%%{init: {'theme':'base','themeVariables': {'fontFamily': 'arial','primaryColor': '#BFDFFF','primaryTextColor': '#000','primaryBorderColor': '#4C75A3','lineColor': '#000','secondaryColor': '#006100','tertiaryColor': '#fff'}, 'flowchart' : {'curve':'monotoneY'}}}%%
flowchart TD
F8(Labelled JSON files\nfrom LabelStudio) --> C2(Split into Train/Val/Test\nSets by xDD ID)
C2 --> C5(Convert to artifacts \nfor training)
C5 --> F1(test)
C5 --> F3(val artifact)
C5 --> F2(train artifact)
F3 --> C4
F2 --> C4(Run Model Training \nw/ Validation)
F1 ----> C3(Run Model Evaluation)
F3 --> C3
C4 --> F7(Log Metrics &\nCheckpoints)
C4 --> F4(Log Final\nTrained Model)
F4 --> C3
C3 --> F6(Evaluation Plots)
C3 --> F5(Evaluation results\nJSON)
```

### Model Evaluation

The target for the data extraction is to maximize extracted data recall as the cost of a data reviewer deleting a piece of data is much lower than having to go and read the article to add missed entities. Similarly, token based recall is chosen as the target metric which measures partial matches of data extracted. Entity based recall which measures exact matches of all parts of a word is considered as it reduces data reviewers time to review articles but the emphasis is placed on token recall for evaluating models.

For more in depth analysis of results four primary methods are used to evaluate each model as outlined in @tbl-ner-model-eval-methods.

| **Method**  | **Description**                                                                                      |
|:------------|:-----------------------------------------------------------------------------------------------------|
| **Strict**  | Exact boundary of extracted string and entity type matches the annotation.                           |
| <br/> | <br/> |
| **Exact**   | Exact boundary of extracted string matches but does not discriminate by correct entity label         |
| <br/> | <br/> |
| **Partial** | A partial boundary overlap of the extracted string and does not discriminate by correct entity label |
| <br/> | <br/> |
| **Type**    | Any overlap of the correct entity type is considered correct.                                        |

: Model Evaluation Methods {#tbl-ner-model-eval-methods tbl-colwidths="[25,75]"} 

For each method in @tbl-ner-model-eval-methods a set of detailed metrics are used during evaluation as outlined in @tbl-ner-specific-metrics.

| **Metric**          | **Description**                                                       |
|:--------------------|:----------------------------------------------------------------------|
| **Correct (COR)**   | Both the labelled entity and predicted entity are the same            |
| <br/> | <br/> |
| **Incorrect (INC)** | The labelled entity and predicted entity do not match                 |
| <br/> | <br/> |
| **Partial (PAR)**   | The labelled entity and predicted entity are similar but not the same |
| <br/> | <br/> |
| **Missing (MIS)**   | A labelled entity is not captured by the model                        |
| <br/> | <br/> |
| **Spurius (SPU)**   | The model predicts an entity where there is none in the labelled text |

: Model Evaluation Metrics {#tbl-ner-specific-metrics tbl-colwidths="[25,75]"}


## Data Review Tool

To facilitate the review and improve the efficiency of Neotoma data stewards, an interactive dashboard was developed as the final data product. This dashboard enables manual review of the Article Relevance Prediction and Article Data Extraction results. Users can compare extracted entities to sentences or access the full-text articles to make corrections. They also have the ability to delete incorrect entities and add any missed entities. The Data Review Tool generates a JSON object as output, which can be used to retrain the Article Entity Extraction model and update the Neotoma database. This process promotes enhanced information sharing and improved results, reducing the time required for data stewards to review extracted entities in articles.

### Approach

The project's objective was to develop a customizable and freely available product with custom components. Considering this, the decision was made to prioritize open-source solutions such as R Shiny [@shiny] and Plotly Dash [@dash] over paid alternatives like Tableau or Power BI. The choice between R Shiny and Plotly Dash was influenced by the project's customization requirements. During discussions with Neotoma, the decision to implement either of these options did not have any impact. Ultimately, the team concluded that Plotly Dash in Python would be the preferred choice for the ongoing development of the project dashboard as the rest of the pipeline was to be written in Python.

### Evaluation

In order to create an interactive tool that would be appropriate and efficient for the reviewers who are using this tool, the aim was to make it user-friendly and simple to use. To accomplish this success criteria, the requirements outlined in @tbl-review_metrics were developed along with the targets to make these requirements successful for the data reviewers.

| **Requirement**                                       | **Target**                                       |
| ----------------------------------------------------- | ------------------------------------------------ |
| Options for reviewing extracted data                  | Accept, Reject, Edit then Accept                 |
| ----------------------------------------------------- | ------------------------------------------------ |
| Other data made available to the user                 | Article DOI, Journal Name, Hyperlink to Article  |
| ----------------------------------------------------- | ------------------------------------------------ |
| Displaying text from where the data was extracted     | Current sentence and 1-2 sentences before/after. |
| ----------------------------------------------------- | ------------------------------------------------ |
| User skill to run                                     | Non-Technical (e.g. no code/CLI)                 |
| ----------------------------------------------------- | ------------------------------------------------ |
| Number of mouse clicks to review single piece of data | 1-2                                              |
| ----------------------------------------------------- | ------------------------------------------------ |
| Reviewing workflow                                    | Able to save/resume progress.                    |
| ----------------------------------------------------- | ------------------------------------------------ |
| Output file format                                    | JSON                                             |

: Data Review Tool Target Metrics {#tbl-review_metrics}

# Data Products and Results

## Article Relevance Prediction

![Model Performance Comparison using Cross-Validation Scores](assets/relevance-model-comparison.png){#fig-relevance_performance}

Among all the models trained, Naive Bayes had the highest recall score (96.7%), but its precision score was low (70.1%) and would introduce false positives. Among all gradient boosted models, LGBM had the highest precision (93.9%), but its recall score (89.3%) did not outperform logistic regression (94.8%). Among the analogy-based models, SVM with RBF kernel achieve a recall performance (94.8%) that's comparable to logistic regression (95.0%), and both had sufficiently high precision (@fig-relevance_performance). The final decision was to use logistic regression with tuned hyperparameters for its good performance and better interpretability. On the test data set, the final model achieved a recall score of 96.5%, and a precision score of 85.2%. To put this into actual article counts, among the 666 articles in the test set, there are 5 false negatives and 24 false positives.

## Article Data Extraction

Of the two approaches developed using the spaCy and Hugging Face model training systems there were two candidate models proposed to be used. The first was the Hugging Face finetuned RoBERTa based model which performed best of the Hugging Face models. The second is spaCy's transformer based model. The reason for proposing two models moving forward is the spaCy transformer model had better entity based recall along with better precision, meaning the extracted data was more often correctly extracted. Whereas the Hugging Face RoBERTa model achieves better token recall but has lower precision, meaning it detects more of the overall entities but contains more false detections which must be deleted by the reviewers. The slight differences in the accuracies and varying strengths can be attributed to the different NER models leveraging the RoBERTa-base transformer embeddings differently.

@fig-extraction-recall-results shows the primary candidate models token and entity based recall.

```{python}
#| label: fig-extraction-recall-results
#| layout-ncol: 1
#| fig-cap: "The entity and token based recall for each model on the test set."

def plot_model_metric_comparison(
    results_df: pd.DataFrame,
    metrics: list,
    fig_height: int = 6,
    fig_width: int = 5,
    sort_ascending: bool = False,
):
    metrics_formatted = [metric.replace("_", " ").title() for metric in metrics]

    # flatten the results_df
    flat_df = results_df.melt(
        id_vars=["model"], value_vars=metrics, value_name="score", var_name="metric"
    )

    # multiply scores by 100
    flat_df.loc[:, "score"] = flat_df.score * 100

    # format the metric names
    flat_df.loc[:, "metric"] = flat_df.metric.str.replace("_", " ").str.title()

    # plot token level recall by model using seaborn
    fig, ax = plt.subplots(figsize=(fig_width, fig_height))
    # sns.set_theme(style="whitegrid")
    ax = sns.barplot(
        y="model",
        x="score",
        hue="metric",
        data=flat_df,
        order=flat_df.iloc[
            flat_df[flat_df["metric"] == metrics_formatted[0]]["score"]
            .sort_values(ascending=sort_ascending)
            .index
        ]["model"],
        palette=FFOSSILS_PALETTE,
        ax=ax,
    )
    ax.set_title(
        f"Entity Extraction Model Performance Comparison\n",  # \nMetrics: {', '.join(metrics_formatted)}",
        fontsize=12,
        fontweight="bold",
    )
    # fix cut off of left/right of plot
    fig.subplots_adjust(left=0.35, right=0.90, top=0.80)
    for i in range(0, len(metrics)):
        ax.bar_label(ax.containers[i], fmt="%.1f%%", fontsize=10, padding=2)
    ax.set_xlabel(None)
    # set x tick locations
    ax.set_xticks([0, 20, 40, 60, 80, 100])
    # format xtick labels to have % symbols
    xtick_labels = [f"{int(x)}%" for x in ax.get_xticks()]
    ax.set_xticklabels(xtick_labels)

    ax.set_ylabel(None)
    ax.set_xlim(0.0, max(100, flat_df.score.max()))
    # plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, fontsize=12) #bbox_to_anchor=(1.02, 0.5),
    ax.legend(
        loc="upper center",
        bbox_to_anchor=(0.5, 1.1),
        ncol=3,
        fancybox=True,
        shadow=True,
    )
    sns.despine(bottom=True)
    plt.show()


model_results_df = load_model_evaluation_results(
    results_path=os.path.join(os.pardir, os.pardir, "results", "ner"),
    results_type="test",
)

sns.set_style(
    "whitegrid",
    rc={
        "grid.color": ".85",
    },
)
sns.set_context("paper")

plot_model_metric_comparison(
    model_results_df,
    ["entity_recall", "token_recall"],
    fig_height=4,
    fig_width=6,
)
```


The token based recall for each model on the test set by entity type are shown in @tbl-token-recall.

```{python}
#| label: tbl-token-recall
#| tbl-cap: 'Entity extraction model results for token based recall, overall and by entity type'
#| tbl-cap-location: bottom
#| output: markdown

# render the token_recall, token_precision, and token_f1 scores for each model and each entity type
render_columns = [
    "model",
    "token_recall",
    "token_AGE_recall",
    "token_SITE_recall",
    "token_TAXA_recall",
    "token_REGION_recall",
    "token_GEOG_recall",
    "token_EMAIL_recall",
    "token_ALTI_recall",
]

render_df = test_results_df[render_columns].copy()
render_df.columns = render_df.columns.str.replace("_", " ").str.title()
render_df = render_df.melt(id_vars=["Model"], var_name="Metric", value_name="Score")
render_df["Metric"] = (
    render_df.Metric.str.replace("Token ", "")
    .str.replace(" Recall", " (%)", regex=True)
    .str.replace("Recall", "Recall (%)")
)
render_df["Model"] = (
    render_df.Model.str.replace("Finetuned", "")
    .str.replace("Spacy", "SpaCy")
    .str.replace("Finetune", "")
    .str.replace("V\d", "", regex=True)
    .str.replace("Transformer", "Transf.")
    .str.replace("Multilanguage", "Multi-lang.")
    .str.replace("Ner", "NER")
    .str.replace("bert", "BERT")
    .str.replace("Bert", "BERT")
)
render_df["Score"] = render_df.Score * 100
render_df = (
    render_df.reset_index(drop=True)
    .round(1)
    .pivot(index="Model", columns="Metric", values="Score")
    .sort_values(["Recall (%)"], ascending=False)
)

render_df = render_df[
    [
        "Recall (%)",
        "Age (%)",
        "Site (%)",
        "Taxa (%)",
        "Region (%)",
        "Geog (%)",
        "Email (%)",
        "Alti (%)",
    ]
]

Markdown(render_df.to_markdown())
```

An important observation to make here is that even the top models had a lower precision score for the SITE names and REGION names. The models got confused when deciding whether an entity should be classified as a SITE or a REGION. This was partially due to quality of labeling entities as well as the fact that both these types correspond to the name of a place or a wider area. See @fig-confusion_matrix for a confusion matrix generated using the test set assets highlights the issue.

![Confusion Matrix for RoBERTa Model](../../results/ner/roberta-finetuned-v3/roberta-finetuned-v3_test_confusion_matrix.png){#fig-confusion_matrix}

## Data Review Tool

The final data review tool that was created was a multi-page Plotly Dash application. The tool can be replicated by launching Docker containers, enabling anyone within the Neotoma community to easily utilize the tool for reviewing outputs from the pipeline. 

![Data Review Tool](assets/data_review.png){#fig-review_tool_snap}

This web application enables users to review the extracted entities from articles, make changes, add additional missed entities and remove incorrectly extracted entities. The application includes a button to launch the article in an external browser tab so that the reviewer can verify beyond the current sentence and sentence preceding/following that was provided as output. The entire review process does not require any coding knowledge, and reviewers can navigate through the review workflow in three clicks or fewer from the Article Review page @fig-review_tool_snap.

The output of this data review tool is a JSON object that stores the originally extracted entities as well as the corrected entities. The Neotoma organization can utilize these entities to add new paleoecology articles to the database. In addition, the reviewed entities can be fed back to model using our model retraining pipeline, contributing to the continuous improvement of the quality of extraction of entities.

| **Requirement**                                         | **Results**                                   |
| ------------------------------------------------------- | --------------------------------------------- |
| Options for reviewing extracted data                    | Accept, Reject, Edit, Add then Accept         |
| ----------------------------------------------------- | ------------------------------------------------ |
| Other data made available to the user                   | Article DOI, Hyperlink to Article             |
| ----------------------------------------------------- | ------------------------------------------------ |
| Displaying text from where the data was extracted       | Current sentence and 1 sentence before/after. |
| ----------------------------------------------------- | ------------------------------------------------ |
| User skill to run                                       | Non-Technical (e.g. no code/CLI)              |
| ----------------------------------------------------- | ------------------------------------------------ |
| Number of mouse clicks to review a single piece of data | 3 clicks from launch                          |
| ----------------------------------------------------- | ------------------------------------------------ |
| Reviewing workflow                                      | Able to save/resume progress.                 |
| ----------------------------------------------------- | ------------------------------------------------ |
| Output file format                                      | JSON                                          |

: Data Review Tool Metric Results {#tbl-review_results}

The implemented Dash application fulfills all the requirements stated in the proposal (as shown by @tbl-review_results). However, with more time available, there are potential improvements that can be made. One crucial enhancement would be to add an extra page to the dashboard, allowing users to review the results of the Article Relevance Prediction model. This page would display both positive predictions (relevant articles to Neotoma) and negative predictions (articles deemed irrelevant). By enabling Neotoma data stewards to review and correct these predictions, the Article Relevance model's accuracy can be enhanced through retraining. Currently, the data review tool includes a button to mark an article as irrelevant, facilitating model retraining by labeling it as not relevant to Neotoma. However, there is no existing mechanism to make corrections for negative predictions. Addressing this issue in the future would minimize bias from the training data sample, resulting in improved model performance.

## Product Deployment

The end goal of this project is to have each data product running in a semi/un-supervised fashion. The article relevance prediction pipeline is containerized using Docker. It is expected to run on a daily or a weekly basis by the Neotoma and to fetch the latest published articles from the public xDD API, run the article relevance prediction, and finally submits relevant articles to xDD to have their full text processed. 

The Article Data Extraction pipeline is containerized using Docker and contains the entity extraction model within it. It is run on the xDD servers as xDD is not legally allowed to send full text articles off their servers. The container accepts full text articles, extracts the entities, and outputs a single JSON object for each article which is then exported by xDD back to the Neotoma team. The extracted entity JSON objects were combined with the article relevance prediction results and loaded into the Data Review Tool. @fig-deployment_pipeline depicts the work flow.

```{mermaid}
%%| label: fig-deployment_pipeline
%%| fig-cap: "This is how the MetaExtractor pipeline flows between the different components."
%%| fig-height: 6
graph TD
subgraph neotoma [Neotoma Servers]
    direction TB
    L(CRON Job Starts Pipeline)
    subgraph relevance_docker [Docker Image &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp]
      A(Get New Article DOIs<br> Since Last Run)
      B(Predict Article Relevance)
    end
end
subgraph xdd [xDD Servers &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp]
    direction TB
  C(To Be Processed Stack)
    subgraph entity_docker [Docker Image &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp]
        D(xDD Inputs Full Text Articles)
        E(Entity Extraction Pipeline)
    end
end
G(Article Relevance Predictions)
H(Extracted Entities per Article)
I(Combine Article Relevance <br>& Extracted Entities)
J(Data Review Tool)
L --> A
A --> B
B -----> |Parquet File| G
B --> |API Put Request| C
C --> D
D --> E
E --> |JSON Per Article,\nLog File| H
G --> I
H --> I
I --> |Parquet| J

%% create a class for styling subgraphs
classDef subg fill:#fbfbfb,stroke:#333,stroke-width:4px,font-weight:bold;
classDef docker fill:#cef1fc,stroke:#0db7ed,stroke-width:4px,font-weight:bold,align:left;
%% create a class for styling the nodes
classDef nodes fill:#D1E5F4,stroke:#333,stroke-width:2px;
%% style each node
class A,B,C,D,E,F,G,H,I,J,K,L nodes;
%% style each subgraph
class neotoma,xdd subg;
class entity_docker,relevance_docker docker;
```

# Conclusion and Recommendations

## Conclusions

The Neotoma database plays a crucial role in paleoecological research by providing data on ecological changes over the past 5 million years. However, the reliance on manual submissions by researchers has led to difficulties in data entry and possibly hindered collaborative efforts to fully understand these ecological changes. The implementation of our pipeline brings significant benefits to the Neotoma community and paleoecological research. Firstly, by automating the search for submissions of new research articles that are relevant to the Neotoma database through the Article Relevance Prediction this process will growth of the Neotoma community, attracting more researchers and fostering collaboration in ecological research. Secondly, researchers will experience reduced time and effort required for data uploads, as the manual submission process is replaced with automated data extraction and entry through both the Article Entity Extraction and Data Review Tool. Finally, with the addition of access to the data, researchers in the Neotoma community may be able to answer questions about ecological changes that were previously unknown

## Recommendations

Due to time constraints, improvements were identified but could not be implemented within the project's timeframe. To provide a concise summary for future project contributors, three essential features have been identified that would significantly enhance the overall pipeline.

1.  **Relevance Review Page:** incorporating an article review page would allow for a thorough assessment of article relevance, reducing both false positives and false negatives in the predictions. It will also reduce the bias in the training dataset that currently exists due to our data collection strategy.

2.  **Adding new entities:** Although the focus was primarily on accurately extracting the required entities, there are additional entities that are considered beneficial but not mandatory by the Neotoma team. To address this, the Label Studio setup that was created through the project enables future contributors to easily add additional entities to benefit the Neotoma community at large.

3.  **Correction Modal** There are likely many corrections that will be performed frequently on the data review tool by the Neotoma Data Stewards. Therefore, a model could be implemented to learn these corrections and preprocess these corrections in future improving efficiency and reducing time spent reviewing.

4.  **Computer Vision:** Using PDF images and OCR technology to extract figures and table data for richer information extraction and inclusion of higher quality data into the Neotoma database.

\newpage

# Acknowledgements

Data were obtained from the Neotoma Paleoecology Database (http://www.neotomadb.org) and its constituent database(s). The work of data contributors, data stewards, and the Neotoma community is gratefully acknowledged.

A huge thanks to Simon Goring & Socorro Dominguez from the Neotoma Database team for their support on this project.

\newpage

# References
