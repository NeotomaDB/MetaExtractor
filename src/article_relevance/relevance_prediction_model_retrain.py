# Author: Kelly Wu
# 2023-06-22

# Using corrected article relevance data to retrain the logistic regressio model
# Input: directory with the JSON files of new relevant articles & the parquet file with article info of the batch
# Output: Model object to be used for running the prediction pipeline

# Process Overview:
# - Gather the list of new data for training
# - Retried these article's feature from parquet file, convert to format that's suitable for model training
# - Spilt the new data into training split and test split. Default is 80%/20%, but user could modify the parameter in the train_test_split function.
# - Merge the new training split with old training split. Merge the test split with old test split.
# - Model retraining
# - Model evaluation

# Assumption:
# - All parquet files generated by the article relevance pipeline is stored in one folder
# - All new articles's reviewed data outputted from data review tool are stored in one folder, with subfolders for each batch.
# - Each parquet file contains doi, metadata, sentence embeddings, i.e. all info required for retraining

import os
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import (
    cross_validate,
    cross_val_score,
    train_test_split,
)
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression 
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from logs import get_logger
logger = get_logger(__name__) # this gets the object with the current modules name


def train_data_load_split(train_raw_csv_path):
    '''
    Load the old sample used in the original model training.
    Return the train, validation, and test split as three dataframes.

    Args:
        train_raw_csv_path (str)    The path to the original training data file metadata_processed.csv.

    Return:
        Three pandas Data frames: train_df, valid_df, test_df

    Example:
        train_data_load_split(train_raw_csv_path = "../../data/article-relevance/processed/metadata_processed.csv")
    '''

    # load original training sample
    metadata_df = pd.read_csv(train_raw_csv_path, index_col=0)
    training_keepcol = [
       'is-referenced-by-count', 
       'has_abstract', 
       'subject_clean',
       'text_with_abstract', 
       'target']
    metadata_df_cleaned = metadata_df.loc[:, training_keepcol]

    # empty strings being imported as NaN's, so filling with empty strings
    metadata_df_cleaned['subject_clean'] = metadata_df_cleaned['subject_clean'].fillna(value='')

    # add specter2 embeddings
    logger.info(f'Loading - Original sample has {metadata_df_cleaned.shape[0]} examples. ')
    logger.info(f'Embedding - Adding embeddings to these examples.')

    model = SentenceTransformer('allenai/specter2')
    embeddings = metadata_df_cleaned['text_with_abstract'].apply(model.encode)
    embeddings_df = pd.DataFrame(embeddings.tolist())
    
    # Join embeddings with metadata
    embeddings_df.columns = embeddings_df.columns.astype(str) # require column names as strings for modelling
    metadata_with_embedding = pd.concat([metadata_df_cleaned, embeddings_df], axis = 1)
    logger.info(f'Embedding - Embedding merged with metadata.')

    # Split into train/valid/test sets
    train_df, val_test_df = train_test_split(metadata_with_embedding, test_size=0.8, random_state=123)
    valid_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=123)
    logger.info(f'Result - Original sample has {train_df.shape[0]}/{valid_df.shape[0]}/{test_df.shape[0]} in train/valid/test splits.')

    return train_df, valid_df, test_df


def retrain_data_load_split(reviewed_parquet_folder_path):
    '''
    Get a DOI list of reviewed articles (i.e. status is completed/irrelevant).
    Retrieve their metadata and split into train/valid/test sets.
    Return the train, validation, and test splits as three dataframes.

    Args:
        reviewed_parquet_folder_path (str)  The path to the folder storing reviewed articles parquet files.

    Return:
        Three pandas Data frames: train_df, valid_df, test_df
    '''
    result_df = []

    # Get a list of parquet files in the folder
    parquet_list = os.listdir(reviewed_parquet_folder_path)

    # Loop thorugh all parquet files and extract the rows with status "Non-relevant" or "Completed"
    for file_name in parquet_list:
        if file_name.endswith('.parquet'):
            # Construct the file path
            file_path = os.path.join(reviewed_parquet_folder_path, file_name)

            # Read the parquet file into a dataframe
            onefile_df = pd.read_parquet(file_path)

            # Filter rows based on the "status" column
            filtered_df = onefile_df[onefile_df['status'].isin(['non-relevant', 'completed'])]

            # Append the filtered dataframe to the list
            result_df.append(filtered_df)

    # Concatenate all dataframes into a single dataframe
    return_df = pd.concat(result_df, ignore_index=True)

    # Reindex
    
    # Create reviewed_target column using status column


    return return_df


def retrain_data_merge(old_train, new_train, old_valid, new_valid,old_test, new_test):
    '''
    
    '''

    pass



def model_train(train_df, model_c = 0.01563028103558011):
    '''
    
    '''

    pass

def model_export(model, output_dir):
    '''
    
    '''

    pass

def model_eval(model, valid_df, test_df, report_dir):
    '''
    
    '''

    pass

def main():
    pass



