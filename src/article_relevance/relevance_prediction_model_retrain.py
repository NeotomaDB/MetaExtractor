# Author: Kelly Wu
# 2023-06-22

# Using corrected article relevance data to retrain the logistic regressio model
# Input: directory with the JSON files of new relevant articles & the parquet file with article info of the batch
# Output: Model object to be used for running the prediction pipeline

# Process Overview:
# - Gather the list of new data for training
# - Retried these article's feature from parquet file, convert to format that's suitable for model training
# - Spilt the new data into training split and test split. Default is 80%/20%, but user could modify the parameter in the train_test_split function.
# - Merge the new training split with old training split. Merge the test split with old test split.
# - Model retraining
# - Model evaluation

# Assumption:
# - All parquet files generated by the article relevance pipeline is stored in one folder
# - All new articles's reviewed data outputted from data review tool are stored in one folder, with subfolders for each batch.
# - Each parquet file contains doi, metadata, sentence embeddings, i.e. all info required for retraining

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import (
    cross_validate,
    cross_val_score,
    train_test_split,
)
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression 
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score



def train_data_load_split(train_raw_path):
    '''

    '''
    # load original training sample
    metadata_df = pd.read_csv("../../data/article-relevance/processed/metadata_processed.csv", index_col=0)
    training_keepcol = [
       'is-referenced-by-count', 
       'has_abstract', 
       'subject_clean',
       'text_with_abstract', 
       'target']
    metadata_df_cleaned = metadata_df.loc[:, training_keepcol]

    # empty strings being imported as NaN's, so filling with empty strings
    metadata_df_cleaned['subject_clean'] = metadata_df_cleaned['subject_clean'].fillna(value='')

    # add specter2 embeddings
    model = SentenceTransformer('allenai/specter2')
    embeddings = metadata_df_cleaned['text_with_abstract'].apply(model.encode)
    embeddings_df = pd.DataFrame(embeddings.tolist())
    
    # Join embeddings with metadata
    embeddings_df.columns = embeddings_df.columns.astype(str) # require column names as strings for modelling
    metadata_with_embedding = pd.concat([metadata_df_cleaned, embeddings_df], axis = 1)

    # Split into train/valid/test sets
    train_df, val_test_df = train_test_split(metadata_with_embedding, test_size=0.8, random_state=123)
    valid_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=123)

    return train_df, valid_df, test_df

    

def retrain_data_load_split(reviewed_data_path):
    '''
    
    '''

    pass

def retrain_data_merge(old_train):
    '''
    
    '''

    pass



def model_train(train_df, model_c = 0.01563028103558011):
    '''
    
    '''

    pass

def model_export(model, output_dir):
    '''
    
    '''

    pass

def main():
    pass



