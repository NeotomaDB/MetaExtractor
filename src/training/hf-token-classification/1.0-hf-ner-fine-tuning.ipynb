{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Fine Tuning with Hugging Face\n",
    "\n",
    "This notebook evaluates using Hugging Face's NER pipeline for fine tuning on the labelled articles. The Specter2 model is intended to be used as a BERT based encoder model with a token classification head.\n",
    "\n",
    "Specter2 model card on HF hub: https://huggingface.co/allenai/specter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# use autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wnut_17 (C:/Users/tyand/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n",
      "100%|██████████| 3/3 [00:00<00:00, 332.88it/s]\n"
     ]
    }
   ],
   "source": [
    "wnut = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure src is in path\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.entity_extraction.entity_extraction_evaluation import get_token_labels, load_json_label_files\n",
    "\n",
    "labelled_file_path = os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir, \"data\", \"labelled\", \"labelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text, labelled_entities = load_json_label_files(labelled_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, token_labels = get_token_labels(labelled_entities, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-GEOG\",\n",
    "    2: \"I-GEOG\",\n",
    "    3: \"B-SITE\",\n",
    "    4: \"I-SITE\",\n",
    "    5: \"B-EMAIL\",\n",
    "    6: \"I-EMAIL\",\n",
    "    7: \"B-ALTI\",\n",
    "    8: \"I-ALTI\",\n",
    "    9: \"B-TAXA\",\n",
    "    10: \"I-TAXA\",\n",
    "    11: \"B-REGION\",\n",
    "    12: \"I-REGION\",\n",
    "    13: \"B-AGE\",\n",
    "    14: \"I-AGE\",\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-GEOG\": 1,\n",
    "    \"I-GEOG\": 2,\n",
    "    \"B-SITE\": 3,\n",
    "    \"I-SITE\": 4,\n",
    "    \"B-EMAIL\": 5,\n",
    "    \"I-EMAIL\": 6,\n",
    "    \"B-ALTI\": 7,\n",
    "    \"I-ALTI\": 8,\n",
    "    \"B-TAXA\": 9,\n",
    "    \"I-TAXA\": 10,\n",
    "    \"B-REGION\": 11,\n",
    "    \"I-REGION\": 12,\n",
    "    \"B-AGE\": 13,\n",
    "    \"I-AGE\": 14,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  convert the labels to ids\n",
    "token_label_ids = [label2id[label] for label in token_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into chunks of 128 tokens and labels\n",
    "chunked_tokens = [tokens[i : i + 128] for i in range(0, len(tokens), 128)]\n",
    "chunked_token_label_ids = [token_label_ids[i : i + 128] for i in range(0, len(token_label_ids), 128)]\n",
    "chunked_labels = [token_labels[i : i + 128] for i in range(0, len(token_labels), 128)]\n",
    "\n",
    "# make each chunk a dict with keys ner_tags and tokens\n",
    "chunked_data = [\n",
    "    {\n",
    "        \"ner_tags\": chunked_token_label_ids[i], \n",
    "        \"tokens\": chunked_tokens[i],\n",
    "        \"labels\": chunked_labels[i]\n",
    "     } for i in range(len(chunked_tokens))\n",
    "]\n",
    "# make the data into a huggingface dataset\n",
    "dataset = pd.DataFrame(chunked_data)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "dataset = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed as tokenizing adds CLS and SEP tokens, doesn't match labels\n",
    "# see here for more detail: https://huggingface.co/docs/transformers/tasks/token_classification\n",
    "# It does:\n",
    "# 1. Mapping all tokens to their corresponding word with the word_ids method.\n",
    "# 2. Assigning the label -100 to the special tokens [CLS] and [SEP] so they’re ignored by the PyTorch loss function.\n",
    "# 3. Only labeling the first token of a given word. Assign -100 to other subtokens from the same word.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/833 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter2\")\n",
    "\n",
    "tokenized = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized[\"labels\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "train, test = train_test_split(tokenized, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new huggingface daatset with train/test\n",
    "train_dataset = Dataset.from_dict(train)\n",
    "test_dataset = Dataset.from_dict(test)\n",
    "\n",
    "dataset_dict = DatasetDict({\"train\":train_dataset,\"test\":test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ner_tags', 'tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 666\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ner_tags', 'tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 167\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_list = label2id.keys()\n",
    "\n",
    "# labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/specter2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"allenai/specter2\", num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downgraded to transformers==4.28.0 due to PartialState import error, referenced here and was recommended solution: \n",
    "# https://github.com/huggingface/transformers/issues/22816 \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    no_cuda=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffossils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
